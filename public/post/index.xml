<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Yet another enthusiast blog!</title>
    <link>http://blog.yadutaf.fr/post/index.xml</link>
    <description>Recent content in Posts on Yet another enthusiast blog!</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Jean-Tiare Le Bigot</copyright>
    <lastBuildDate>Tue, 11 Jul 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://blog.yadutaf.fr/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Tracing a packet journey thanks to eBPF</title>
      <link>http://blog.yadutaf.fr/2017/07/11/tracing-a-packet-journey-thanks-to-ebpf/</link>
      <pubDate>Tue, 11 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>http://blog.yadutaf.fr/2017/07/11/tracing-a-packet-journey-thanks-to-ebpf/</guid>
      <description>

&lt;p&gt;If you do networking, you are used to long debugging sessions, firing up a couple of &lt;code&gt;tcpdump&lt;/code&gt;, &lt;code&gt;mtr&lt;/code&gt; and &lt;code&gt;*ping&lt;/code&gt; along the expected/actual packet journey. At least, when there is routing involved, &lt;code&gt;mtr&lt;/code&gt; (or &lt;code&gt;traceroute&lt;/code&gt; if you have no choice) does a good at showing what&amp;rsquo;s going on. But not everything is L3. Since containers became popular on Linux, people found new &amp;lsquo;exciting&amp;rsquo; ways to torture the networking subsystem with (powerful) virtual interfaces like veth, macvlan bridge, V(x)LAN, &amp;hellip; Add a bunch of network namespaces [&lt;strong&gt;TODO LINK TO POST&lt;/strong&gt;] in the equation and debugging starts to get&amp;hellip; interesting. If not excitingly frustrating.&lt;/p&gt;

&lt;p&gt;When I was working for OVH, I designed a significant part of the inner networking of the next generation of Load Balancers. Without revealing trade secrets / announcing not-yet-if-ever-released products, I can tell that it involved a fair amount of network namespaces, virtual interfaces and truly nasty routing tricks (like routing via an interface with no IP and using &lt;code&gt;dummy&lt;/code&gt; interfaces). At least, it did not involve multiple routing tables (at that time). For all this time, I whished I had a tool to trace a packet journey across the interfaces and network namespaces.&lt;/p&gt;

&lt;p&gt;Latter, I was struggling with a network issue in a trivial 2 nodes Docker Swarm setup I use to play around &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:play-around&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:play-around&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Exactly half the connections timed out. It turned out to be the kind of bugs you end up fixing by rebooting&amp;hellip; the other node. Frustrating. I whished I had a tool to trace a packet journey across the interfaces and network namespaces.&lt;/p&gt;

&lt;p&gt;In such situation, one usually ends up checking manually each possible ip route, ip table, iptables, iptables tables (!) in each possible network namespace / vrf, possibly firing a couple of tcpdumps as an attempt to make sense of the problem. Feels like a maze. At least, it does to me. You have to guess a path, check it, rinse, repeat. At least, in an actual maze, you can cheat by looking from above. &lt;code&gt;tcpdump&lt;/code&gt; would be a good tool even though tracing is not the area where it excels. To get the &amp;ldquo;view from above&amp;rdquo;, you could make it listen on the special &amp;ldquo;any&amp;rdquo; interface. But that won&amp;rsquo;t help when using multiple network namespaces. You&amp;rsquo;d need 1 tcpdump instance per network namespace. Doable. But cumbersome.&lt;/p&gt;

&lt;h3 id=&#34;the-solution-enter-ebpf&#34;&gt;The solution: Enter eBPF&lt;/h3&gt;

&lt;p&gt;Then, it hit me. When I wrote the solisten.py [&lt;strong&gt;TODO LINK TO POST&lt;/strong&gt;] tool to notify whenever a programs starts to listen on any network interface, in any network namespace, I used eBPF to hook on the main listen function in the kernel (inet_listen [&lt;strong&gt;TODO LINK TO KERNEL SOURCE&lt;/strong&gt;]) and send events. Maybe we can do the same to trace a packet? Sounds reasonable!&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s define the problem. We want to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Trace a packet journey in the kernel, on a single node&lt;/li&gt;
&lt;li&gt;List all crossed interface&lt;/li&gt;
&lt;li&gt;List all crossed network namespace&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ok, now that the problem is defined, this looks more like a trackable problem. Let&amp;rsquo;s translate these goals to needs. We need to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Use bcc [&lt;strong&gt;TODO LINK TO BCC PROJECT&lt;/strong&gt;]. It comes with a C to eBPF compiler and a Python API. Seems reasonable for prototyping.&lt;/li&gt;
&lt;li&gt;Trace ping packets. They are well known, have no side effect, contain an &amp;ldquo;identifier&amp;rdquo; and &amp;ldquo;sequence&amp;rdquo; field.&lt;/li&gt;
&lt;li&gt;Find a function that is (almost) always called when a packet is queued on an interface. That would solve the &amp;ldquo;all crossed interface and network namespace&amp;rdquo;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To keep things simple, distributed, TCP and UDP tracing are out of the scope. That would be perfectly feasible, but that would also be over-engineering in that specific case. Additionally, even though I did my proof of concept with IPv6 and IPv4 support, I&amp;rsquo;ll focus exclusively on IPv4 in the post for the sake of readability and link to the final code at the end of this post for curious people [&lt;strong&gt;TODO LINK TO THE FULL CODE&lt;/strong&gt;].&lt;/p&gt;

&lt;h3 id=&#34;find-a-function-to-trace&#34;&gt;Find a function to trace&lt;/h3&gt;

&lt;p&gt;We need a good function to trace. Long story short, we&amp;rsquo;ll use &lt;code&gt;dev_hard_start_xmit&lt;/code&gt; on Linux 4.10 [&lt;strong&gt;TODO LINK&lt;/strong&gt;] for the sake of this post. This may not be the best / perfect pick. But it does the job.&lt;/p&gt;

&lt;p&gt;As digging in the kernel source may seem intimidating, I&amp;rsquo;d like to broadly re-trace the process of finding one, in the hope it can help you find one next time you need to.&lt;/p&gt;

&lt;p&gt;There are some constraints. We need a function that is never inlined. If it is inlined, it is potentially present in all the calling site and maybe optimized away. Moreover, it can not be a &amp;ldquo;static&amp;rdquo; function. Static functions are basically the C equivalent of private function in most languages. All in all, if the candidate is not present in &lt;code&gt;/proc/kallsyms&lt;/code&gt;, it won&amp;rsquo;t work. Some of them are special and can&amp;rsquo;t be traced either, but we&amp;rsquo;ll leave it aside.&lt;/p&gt;

&lt;p&gt;Additionally, if possible, we&amp;rsquo;d prefer a function that is common to all or most of the devices and code path to limit the number of functions to trace. This is not a hard constraint, but that would surely help!&lt;/p&gt;

&lt;p&gt;As I add no clue behind looking into &amp;ldquo;/net&amp;rdquo;, I started from the veth driver. It looked like a reasonable candidate to start. This is a reasonably simple virtual driver, there should not be too much noise in it. Use &amp;lsquo;find&amp;rsquo; to locate it and there you are &lt;a href=&#34;http://elixir.free-electrons.com/linux/v4.10.17/source/drivers/net/veth.c&#34;&gt;http://elixir.free-electrons.com/linux/v4.10.17/source/drivers/net/veth.c&lt;/a&gt;. Around 500 lines of code. For the kernel, that&amp;rsquo;s pretty small. Nice. Now, we need to check the functions in this file, looking for some good looking name. A good looking name would suggest that this function queues or forwards a packet on this interface. Good news, there is a single candidate, &amp;ldquo;vethxmit&amp;rdquo; &lt;a href=&#34;http://elixir.free-electrons.com/linux/v4.10.17/source/drivers/net/veth.c#L106&#34;&gt;http://elixir.free-electrons.com/linux/v4.10.17/source/drivers/net/veth.c#L106&lt;/a&gt;. As this is a driver, if this function is actually the one used by to enqueue packets, it will be registered somewhere and called via a function pointer. This the kernel&amp;rsquo;s way to interfaces. A few lines below, it is registered under the name &amp;lsquo;ndo_start_xmit&amp;rsquo; &lt;a href=&#34;http://elixir.free-electrons.com/linux/v4.10.17/source/drivers/net/veth.c#L291&#34;&gt;http://elixir.free-electrons.com/linux/v4.10.17/source/drivers/net/veth.c#L291&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Some &amp;lsquo;grep&amp;rsquo; later, we learn that &lt;code&gt;ndo_start_xmit()&lt;/code&gt; is mostly called from &lt;code&gt;__netdev_start_xmit&lt;/code&gt; in &lt;a href=&#34;http://elixir.free-electrons.com/linux/v4.10.17/source/include/linux/netdevice.h&#34;&gt;http://elixir.free-electrons.com/linux/v4.10.17/source/include/linux/netdevice.h&lt;/a&gt;. There are 2 other call sites but they are related to usb and infiniband. We&amp;rsquo;ll ignore them for now. Bad news is, this is a static inline function. We&amp;rsquo;ll need another candidate.&lt;/p&gt;

&lt;p&gt;This function is called from a couple of places [&lt;a href=&#34;http://elixir.free-electrons.com/linux/v4.10.17/ident/netdev_start_xmit&#34;&gt;http://elixir.free-electrons.com/linux/v4.10.17/ident/netdev_start_xmit&lt;/a&gt;]. There are multiple good candidates here. I guess we&amp;rsquo;d need to trace at least a couple of them. But we&amp;rsquo;ll start with the most likely candidate and maybe add more later. We&amp;rsquo;ll go for &lt;code&gt;xmit_one&lt;/code&gt; in &lt;a href=&#34;http://elixir.free-electrons.com/linux/v4.10.17/source/net/core/dev.c#L2905&#34;&gt;http://elixir.free-electrons.com/linux/v4.10.17/source/net/core/dev.c#L2905&lt;/a&gt;. Unfortunately, this function is static, hence private, hence untraceable. On the bright side, it&amp;rsquo;s static so we know all call sites must be in the same module. In our case, there is a single call place in &lt;code&gt;dev_hard_start_xmit&lt;/code&gt; &lt;a href=&#34;http://elixir.free-electrons.com/linux/v4.10.17/source/net/core/dev.c#L2922&#34;&gt;http://elixir.free-electrons.com/linux/v4.10.17/source/net/core/dev.c#L2922&lt;/a&gt;. a quick grep in &lt;code&gt;/proc/kallsyms&lt;/code&gt; confirms this simple is exported: we have our candidate!&lt;/p&gt;

&lt;h3 id=&#34;install-bcc&#34;&gt;Install bcc&lt;/h3&gt;

&lt;p&gt;If you already have it installed or are not intending to experiment (yet) on this PoC, you can safely skip this section. I should probably just skip it entirely anyway but install instruction are not exactly up to date. Hence, here is an updated, quick and dirty &amp;trade; procedure for Ubuntu 17.04 (Zesty).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install dependencies
sudo apt install bison build-essential cmake flex git libedit-dev python zlib1g-dev libelf-dev libllvm4.0 llvm-dev libclang-dev luajit luajit-5.1-dev

# Grab the sources
git clone https://github.com/iovisor/bcc.git

# Build and install
mkdir bcc/build
cd bcc/build
cmake .. -DCMAKE_INSTALL_PREFIX=/usr
make
sudo make install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On Ubuntu 16.10 (Xenial) and older, install llvm3.7 instead.&lt;/p&gt;

&lt;h3 id=&#34;trace-dev-hard-start-xmit-using-bcc&#34;&gt;Trace &lt;code&gt;dev_hard_start_xmit&lt;/code&gt; using bcc&lt;/h3&gt;

&lt;p&gt;That&amp;rsquo;s the fun part. We&amp;rsquo;ll attach a kernel probe (kprobe, you guessed it :)) to &lt;code&gt;dev_hard_start_xmit&lt;/code&gt; and start building some events from the probe. Then we&amp;rsquo;ll use a piece of simple Python code to parse these events and print some info.&lt;/p&gt;

&lt;p&gt;Even though probes look like regular C and is indeed build using a LLVM C compiler, this is only a restricted subset of C that compiles to eBPF. That is, a simple VM inside the kernel designed to allow safe observers (ie: not actors / modifiers) to look around, provided the meet some criterions. First, most loops won&amp;rsquo;t be allowed, the kernels needs to prove the program will always exit before starting it. Second, this is an observing program. You can not call arbitrary kernel functions for obvious security reasons. Some inline accessors defined in &amp;ldquo;.h&amp;rdquo; files are OK though. Third, you can not access memory outside the probe stack. If you need to do so, you need to be explicit about it and use bpf accessors. More on this later.&lt;/p&gt;

&lt;p&gt;With this in place, we can proceed with a hello world probe. We&amp;rsquo;ll simply emit an event for each packet sent (don&amp;rsquo;t run it on a production system!). This event will only contain the owning programm name.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#include &amp;lt;bcc/proto.h&amp;gt;
#include &amp;lt;linux/sched.h&amp;gt;

// Event structure
struct route_evt_t {
    char comm[TASK_COMM_LEN];
};
BPF_PERF_OUTPUT(route_evt);

int kprobe__dev_hard_start_xmit(struct pt_regs *ctx, struct sk_buff *first, struct net_device *dev, struct netdev_queue *txq, int *ret)
{
    // Built event for userland
    struct route_evt_t evt = {};
    bpf_get_current_comm(evt.comm, TASK_COMM_LEN);

    // Send event to userland
    route_evt.perf_submit(ctx, &amp;amp;evt, sizeof(evt));

    return 0;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s look at what it does.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Grab some definitions, just like a regular C program&lt;/li&gt;
&lt;li&gt;Declare out event structure (&lt;code&gt;struct route_evt_t&lt;/code&gt;) and channel name (&lt;code&gt;BPF_PERF_OUTPUT(route_evt)&lt;/code&gt;)
3/ Declare our probe (&lt;code&gt;kprobe__dev_hard_start_xmit&lt;/code&gt;). Notice the &lt;code&gt;kprobe__&lt;/code&gt; prefix and &lt;code&gt;struct pt_regs *ctx&lt;/code&gt; argument. bcc automatically detects the prefix and attaches it to the proper kernel function and will pass the context as first argument.
4/ Use a special bpf helper function (&lt;code&gt;bpf_get_current_comm&lt;/code&gt;) to load the program name into the event structure. This is one of the rare function that can be called from eBPF!
5/ Send the event (&lt;code&gt;route_evt.perf_submit()&lt;/code&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We can now integrate it in a simple Python program:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/env python
# coding: utf-8

from socket import inet_ntop
from bcc import BPF
import ctypes as ct

bpf_text = &#39;&#39;&#39;&amp;lt;SEE CODE SNIPPET ABOVE&amp;gt;&#39;&#39;&#39;

TASK_COMM_LEN = 16 # linux/sched.h

class RouteEvt(ct.Structure):
    _fields_ = [
        (&amp;quot;comm&amp;quot;,    ct.c_char * TASK_COMM_LEN),
    ]

def event_printer(cpu, data, size):
    # Decode event
    event = ct.cast(data, ct.POINTER(RouteEvt)).contents

    # Print event
    print &amp;quot;Just go a packet from %s&amp;quot; % (event.comm)

if __name__ == &amp;quot;__main__&amp;quot;:
    b = BPF(text=bpf_text)
    b[&amp;quot;route_evt&amp;quot;].open_perf_buffer(event_printer)

    while True:
        b.kprobe_poll()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I won&amp;rsquo;t go too much into the details, here, this is mostly self-explanatory. This is basically te mirror of the setup on the eBPF side. Declare the event structure using ctypes to decode it, declare the probe, listen for events and print them.&lt;/p&gt;

&lt;p&gt;If you run this program (as root), you&amp;rsquo;ll probably see something like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Just go a packet from ping
Just go a packet from ping
Just go a packet from Socket Thread
Just go a packet from irq/46-iwlwifi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You&amp;rsquo;ll notice that I have a ping running in the background and using WiFi. So you can rightfully guess I&amp;rsquo;m typing from this from my laptop. Indeed, eBPF is not reserved to datacenters :)&lt;/p&gt;

&lt;h3 id=&#34;load-interface-name-netns-id-from-ipv4-packets&#34;&gt;Load interface name, netns id from IPv4 packets&lt;/h3&gt;

&lt;p&gt;Once the plumbing is in place this is straightforward. If we can call straightforward to parse network packets from kernel structures :p. In this section, I&amp;rsquo;ll focus on &lt;code&gt;kprobe__dev_hard_start_xmit&lt;/code&gt; in the C/eBPF part of the probe. The structure and Python part can be easily extended. I&amp;rsquo;ll put a link to a complete version at the end of the post.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll start by keeping only IPv4 packets:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// Cast types. Intermediate cast not needed, kept for readability
struct sock *sk = first-&amp;gt;sk;

// Filter IPv4 packets
if (sk-&amp;gt;sk_family != AF_INET) {
	return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here, there is no choice, you need to read the structure definitions in the kernel. Fortunately, this is one of the most active and documented subsystem of Linux.&lt;/p&gt;

&lt;p&gt;While dealing with &lt;code&gt;sk&lt;/code&gt;, we can grab the network namespace internal identifier. This will not give you a pretty name like &lt;code&gt;ip netns&lt;/code&gt; does, that&amp;rsquo;s only a magic trick. Rather, it will return the numerical part of what you can see when using &lt;code&gt;readlink&lt;/code&gt; on some &lt;code&gt;/proc/[PID]/ns/net&lt;/code&gt; pseudo file.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// Get netns id
evt.netns = sk-&amp;gt;sk_net.net-&amp;gt;ns.inum;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Granted, this may require quite a bit of detective work to find! While we are at the low hanging fruits, we can load the interface name into the event structure as well, from the device:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// Get interface name
bpf_probe_read(&amp;amp;evt.ifname, IFNAMSIZ, dev-&amp;gt;name);
__builtin_memcpy(&amp;amp;evt.ifname, dev-&amp;gt;name, IFNAMSIZ);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice the &lt;code&gt;bpf_probe_read&lt;/code&gt; function. This is the helper to use when reading data. It will handle the memory safety checks for you so that the kernel can trust your code, even though it may access memory outside the eBPF stack. Alternatively, you could use &lt;code&gt;__builtin_memcpy&lt;/code&gt; as well. Actually, this is what I used initially. But it feels hackish. Failing to do so may result in strange build errors like &amp;ldquo;error: extraneous closing brace (&amp;lsquo;}&amp;rsquo;)&amp;rdquo; if you used the more familiar &amp;ldquo;memcpy&amp;rdquo; or &amp;ldquo;Permission Denied&amp;rdquo; from the kernel if it failed to prove safe memory access at validation time, before your code even get a chance to run.&lt;/p&gt;

&lt;h3 id=&#34;keep-only-icmp-echo-request-ping-and-echo-replies&#34;&gt;Keep only ICMP echo request (ping) and echo replies&lt;/h3&gt;

&lt;p&gt;With these gratifying low hanging fruits in hands, we can focus on the packet itself. Depending on you point of view when reading this post, it may seem trivial or&amp;hellip; disheartening, wondering how one can find the necessary fields and offsets. I generally fall into the second category. That&amp;rsquo;s OK. You may use kernel structures for IP and ICMP to guess the fields and Wikipedia has really high quality pages about these foundation protocols.&lt;/p&gt;

&lt;p&gt;Enough talking. Let&amp;rsquo;s get started. First, step, some grounding work. Let&amp;rsquo;s compute the IPv4 and ICMP headers addresses:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// Pre-Compute header addresses
char* ip_header_address   = first-&amp;gt;head + first-&amp;gt;network_header;
char* icmp_header_address = first-&amp;gt;head + first-&amp;gt;transport_header;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice the manual inlining of &lt;code&gt;skb_network_header()&lt;/code&gt; and &lt;code&gt;skb_transport_header()&lt;/code&gt;. Although both are static inline, we can unfortunately not use them here. If we do, the kernel will complain about &amp;ldquo;R1 invalid mem access &amp;lsquo;inv&amp;rsquo;&amp;rdquo;. I suspect this is a compiler glitch as the code are strictly equivalent. Keep in mind eBPF and it&amp;rsquo;s bcc frontends are quite recent additions.&lt;/p&gt;

&lt;p&gt;On the a side note, I accidentally lost quite some time using &lt;code&gt;first-&amp;gt;data&lt;/code&gt; instead of &lt;code&gt;first-&amp;gt;head&lt;/code&gt;. The former is the head of the buffer from which all offsets are computed, while the later is the offset of the first &amp;ldquo;meaningful&amp;rdquo; byte in the buffer.&lt;/p&gt;

&lt;p&gt;Then, load the IP header and filter ICMP packets only, using the next &lt;code&gt;protocol&lt;/code&gt; field from the IPv4 header. We&amp;rsquo;ll use the kernel&amp;rsquo;s &lt;code&gt;struct iphdr&lt;/code&gt; definition to get the offsets right:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// Filter ICMP packets
struct iphdr* iphdr = (struct iphdr*)ip_header_address;
if (iphdr-&amp;gt;protocol != IPPROTO_ICMP) {
	return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You may wonder why I did not write the more compact &lt;code&gt;struct iphdr* iphdr = (struct iphdr*)(first-&amp;gt;head + first-&amp;gt;network_header)&lt;/code&gt;. This is the same reason as above. The generated code would fail to pass the kernel validation phase. As it generally helps with bcc, I split the statements into smaller ones to help the verifier.&lt;/p&gt;

&lt;p&gt;We can now do the same with the ICMP header and keep only ICMP echo request and ICMP echo replies:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// Filter ICMP echo request and echo reply
struct icmphdr* icmphdr = (struct icmphdr*)icmp_header_address;
if (icmphdr-&amp;gt;type != ICMP_ECHO &amp;amp;&amp;amp; icmphdr-&amp;gt;type != ICMP_ECHOREPLY) {
	return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Last but not least: load relevant data into the event.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// Get address and icmp info
evt.saddr    = iphdr-&amp;gt;saddr;
evt.daddr    = iphdr-&amp;gt;daddr;
evt.icmptype = icmphdr-&amp;gt;type;
evt.icmpid   = icmphdr-&amp;gt;un.echo.id;
evt.icmpseq  = icmphdr-&amp;gt;un.echo.sequence;

// Fix endian
evt.icmpid  = be16_to_cpu(evt.icmpid);
evt.icmpseq = be16_to_cpu(evt.icmpseq);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we&amp;rsquo;re done! We now have an event ready to send to user space with all relevant information.&lt;/p&gt;

&lt;p&gt;Note: If you intend to add IPv6 support, please note that the ICMP protocol number is NOT the same as with IPv4 and echo request/replay have different op codes, even though the general packet structure looks similar. I lost quite some time on this&amp;hellip;&lt;/p&gt;

&lt;h3 id=&#34;show-time&#34;&gt;Show time&lt;/h3&gt;

&lt;p&gt;With some trivial Python to handle the event, we can test it in a couple of scenarios. Start the program as root, launch some &amp;ldquo;ping&amp;rdquo; in another terminal and observe:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# ping -4 localhost
[  4026531957]               lo request #32693.001 127.0.0.1 -&amp;gt; 127.0.0.1
[  4026531957]               lo   reply #32693.001 127.0.0.1 -&amp;gt; 127.0.0.1
[  4026531957]               lo request #32693.002 127.0.0.1 -&amp;gt; 127.0.0.1
[  4026531957]               lo   reply #32693.002 127.0.0.1 -&amp;gt; 127.0.0.1
[  4026531957]               lo request #32693.003 127.0.0.1 -&amp;gt; 127.0.0.1
[  4026531957]               lo   reply #32693.003 127.0.0.1 -&amp;gt; 127.0.0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We clearly see the first 3 ping sent the process 32693 (the ICMP id on Linux&amp;rsquo;s ping) on the loopback interface as well as the generated reply. Mission accomplished!&lt;/p&gt;

&lt;p&gt;What about some external &amp;ldquo;random&amp;rdquo; target IP?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# ping -4 google.com
[  4026531957]           wlp2s0 request #31348.001 192.168.1.11 -&amp;gt; 216.58.198.206
[  4026531957]           wlp2s0 request #31348.002 192.168.1.11 -&amp;gt; 216.58.198.206
[  4026531957]           wlp2s0 request #31348.003 192.168.1.11 -&amp;gt; 216.58.198.206
[  4026531957]           wlp2s0 request #31348.004 192.168.1.11 -&amp;gt; 216.58.198.206
[  4026531957]           wlp2s0 request #31348.005 192.168.1.11 -&amp;gt; 216.58.198.206
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We clearly see the 5 first pings sent via my WiFi interface from my home network to Google. Interestingly, we don&amp;rsquo;t see the reply here. This is probably due the hypothesis we did above when choosing a function to trace. We certainly should add some tracing points to be exhaustive, but the general principle says the same. The point is proven !&lt;/p&gt;

&lt;p&gt;And my personal favorite: let&amp;rsquo;s ping a Docker container:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# ping -4 172.17.0.2
[  4026531957]          docker0 request #01952.001 172.17.0.1 -&amp;gt; 172.17.0.2
[  4026531957]      veth0e65931 request #01952.001 172.17.0.1 -&amp;gt; 172.17.0.2
[  4026532395]             eth0   reply #01952.001 172.17.0.2 -&amp;gt; 172.17.0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Like the Google example, this is not perfect BUT we do see the change of network namespace and can reasonably guess that the packet journey goes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;       Host netns           | Container netns
+---------------------------+-----------------+
| docker0 ---&amp;gt; veth0e65931 ---&amp;gt; eth0          |
+---------------------------+-----------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;final-word&#34;&gt;Final word&lt;/h3&gt;

&lt;p&gt;eBPF can be used to instrument the kernel and trace the journey of an arbitrary bit of information in the kernel. I would not pretend this is a &amp;ldquo;quick way&amp;rdquo; to instrument the kernel. It&amp;rsquo;s not. And the C-like language limitations can feel frustrating at first. But once this initial frustration step is over, this is an extremely powerful tool. I hope this post gave you a good taste of it and helped ease the initial journey in eBPF. I intentionally pasted full error messages in this post in the hope they may be indexed and help you if you meet them (and you will ^^).&lt;/p&gt;

&lt;p&gt;As far as this demo is concerned, it would benefit from additional tracing point. Some packets are clearly missing. It would also be interesting to instrument the routing and filtering phases to better help troubleshoot common routing issues.&lt;/p&gt;

&lt;p&gt;I did not measure the performance impact. I know there is one. Kernel probes work by inserting jumps in the kernel on tracing points. This tracing point ends up being called for each packet. On production system that would mean a &lt;em&gt;LOT&lt;/em&gt;. It should not matter. I would not install LLVM and a build toolchain a production system for the sole sake of debugging anyway!&lt;/p&gt;

&lt;p&gt;As promised, the full code for this post is available on Github, with IPv4 and IPv6 support: &lt;a href=&#34;https://github.com/yadutaf/tracepkt&#34;&gt;https://github.com/yadutaf/tracepkt&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:play-around&#34;&gt;That is, if by &amp;ldquo;play around&amp;rdquo; you mean &amp;ldquo;host a hobby application with literally 10s of thousands clients syncing on a daily basis&amp;rdquo;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:play-around&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Docker for your users - Introducing user namespace</title>
      <link>http://blog.yadutaf.fr/2016/04/14/docker-for-your-users-introducing-user-namespace/</link>
      <pubDate>Thu, 14 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>http://blog.yadutaf.fr/2016/04/14/docker-for-your-users-introducing-user-namespace/</guid>
      <description>

&lt;p&gt;A few years ago, back when I was a student, my school had rooms full of counters running Linux that any student could use at any time. We all had a personal account on the machines and the machine management was done by a dedicated team.&lt;/p&gt;

&lt;p&gt;Every once in a while, we found ourselves needing a specific tool like &lt;code&gt;valgrind&lt;/code&gt; which was not readily available or a more recent version of another tool. Like &lt;code&gt;gcc&lt;/code&gt;. Replace &amp;ldquo;valgring&amp;rdquo; and &amp;ldquo;gcc&amp;rdquo; with &amp;ldquo;Node&amp;rdquo;, &amp;ldquo;Rust&amp;rdquo; or &amp;ldquo;Go&amp;rdquo;. You get the idea.&lt;/p&gt;

&lt;p&gt;At that point, we basically had 2 options. Either the tool was vital to our study, and it was possible to get it installed for everybody. Or it was not, we were just experimenting on our own as part of a random project.&lt;/p&gt;

&lt;p&gt;In the later case, the only solution was to build it from scratch, put it in our &lt;code&gt;$HOME&lt;/code&gt;, mess up with^W^W^W tweak the &lt;code&gt;$PATH&lt;/code&gt; and &lt;code&gt;$LD_LIBRARY_PATH&lt;/code&gt; environment variables and sometime get some voodoo involved.&lt;/p&gt;

&lt;p&gt;It &lt;em&gt;usually&lt;/em&gt; worked.&lt;/p&gt;

&lt;p&gt;A year ago, I was giving a talk to introduce Docker and, in the question section, I was asked whether I believed Docker could be a solution to this kind of problem. I answered that it was a dangerous idea. Giving docker access to user was basically like giving him the &lt;code&gt;root&lt;/code&gt; password. He would be better with traditional VMs&lt;/p&gt;

&lt;p&gt;Recently, the question came back to me.&lt;/p&gt;

&lt;p&gt;As it turns out, the response has changed and this is a good occasion to talk about the 6th namespace. The one I never blogged about in my &lt;a href=&#34;https://blog.yadutaf.fr/2013/12/22/introduction-to-linux-namespaces-part-1-uts/&#34;&gt;introduction to Linux Namespaces&lt;/a&gt;&amp;hellip; But I will soon #teaser.&lt;/p&gt;

&lt;h3 id=&#34;user-namespaces-and-docker&#34;&gt;User Namespaces and Docker&lt;/h3&gt;

&lt;p&gt;(If you don&amp;rsquo;t like or care about the technical background, you can safely skip this part)&lt;/p&gt;

&lt;p&gt;In a nutshell, a user namespace is a special Linux kernel mechanism allowing Docker container&amp;rsquo;s to have a &amp;ldquo;faked&amp;rdquo; root user. For example, the root user in a container would be able to manage it&amp;rsquo;s root owned files in the container, act as any user in the container, manage his own network interfaces and some of his mountpoints (restrictions apply) and at the same time being &amp;ldquo;mapped&amp;rdquo; or &amp;ldquo;translated&amp;rdquo; to, say, user &amp;ldquo;ubuntu&amp;rdquo; with uid 1000 on the host system.&lt;/p&gt;

&lt;p&gt;User namespaces are have been introduced as early as Linux 3.5 and are considered as stable &lt;a href=&#34;https://lwn.net/Articles/657432/&#34;&gt;starting with Linux 4.3&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I won&amp;rsquo;t dive too much in the details of user namespaces here, I&amp;rsquo;d really love too, low level bits are by far my favorite topic, but that would be far out of the scope of this post. But stay tuned. While writing this post, I started a more technical one on this very subject ;)&lt;/p&gt;

&lt;p&gt;As far as docker is concerned, starting with Docker 1.10 (the current stable version at the time of writing), it supports a new &lt;code&gt;daemon&lt;/code&gt; option &lt;code&gt;--userns-remap=[USERNAME]&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Neat.&lt;/p&gt;

&lt;p&gt;Wait, what does this &lt;code&gt;--userns-remap&lt;/code&gt; and &lt;code&gt;[USERNAME]&lt;/code&gt; stuff stand for exactly?&lt;/p&gt;

&lt;p&gt;As suggested just earlier, user namespace works by mapping some virtual user ids like root to other user ids on the host. Hence the option name &lt;code&gt;--userns-remap&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Regarding, &amp;ldquo;[USERNAME]&amp;ldquo;, refers to &lt;a href=&#34;http://man7.org/linux/man-pages/man5/subuid.5.html&#34;&gt;&lt;code&gt;/etc/subuid&lt;/code&gt; and &lt;code&gt;/etc/subgid&lt;/code&gt;&lt;/a&gt; files. In a word, these files define the user and group ids a given user can use, beyond his own user id. Just like root can impersonate any user id. If you wonder where this file come from, it&amp;rsquo;s from &lt;a href=&#34;https://github.com/shadow-maint/shadow/blob/ef45bb2496182b5df90ad0323bef75d1a5d69887/src/useradd.c#L2188&#34;&gt;stock &lt;code&gt;useradd&lt;/code&gt; command&lt;/a&gt;. Every time a real user (not a system user) is created on the system, a range of 65536 sub-ids is allocated.&lt;/p&gt;

&lt;p&gt;Does is sound new? Well, not that much. It was &lt;a href=&#34;https://github.com/shadow-maint/shadow/commit/f28ad4b251a42a35c29685850d1686a083cac725&#34;&gt;introduced in early August&amp;hellip; 2013&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Anyway, it maintains simple flat text files looking like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yadutaf:100000:65536
somuser:165536:65536
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It reads as: &amp;ldquo;Let user &amp;lsquo;yadutaf&amp;rsquo; use 65536 uids, starting at 100000&amp;rdquo; and &amp;ldquo;Let user &amp;lsquo;someuser&amp;rsquo; use 65536 uids, starting at 165536&amp;rdquo;. Which is basically the next adjacent range.&lt;/p&gt;

&lt;p&gt;The rule is not set in stone, but the start sub-uid can be guessed as&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FIRST_SUB_UID = 100000 + (UID - 1000) * 65536
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Again, this is only a convention. We can do something slightly different like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yadutaf:1000:1
yadutaf:100000:65535
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It reads as &amp;ldquo;let user yadutaf use his own uid as well as 65535 uids, starting at 100000 and making the total of uids to 65536&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;And this won&amp;rsquo;t break anything.&lt;/p&gt;

&lt;p&gt;Well, actually, this is where is starts to get interesting.&lt;/p&gt;

&lt;p&gt;When starting docker with &lt;code&gt;docker daemon --userns-remap=yadutaf&lt;/code&gt;, docker will parse the subuid and subgid files for &lt;code&gt;yadutaf&lt;/code&gt;, sort all read entries by growing start id and generate kernel userns mapping rules. Without diving too much into the details, this will generate the following rules in &lt;code&gt;/proc/[PID]/uid_map&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;         0       1000          1
         1     100000      65535
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which should look familiar. This structures looks like the one above, but the meaning it slightly different. This time, it reads as:&lt;/p&gt;

&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;Let uid 1000 &lt;em&gt;outside&lt;/em&gt; the container act as &lt;code&gt;root&lt;/code&gt; &lt;em&gt;inside&lt;/em&gt; the container&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Let the 65535 uids starting with 100000 &lt;em&gt;outside&lt;/em&gt; the container act the 65535 uids starting with 1 &lt;em&gt;inside&lt;/em&gt;&amp;ldquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;In other words, 1000 will be 1 and 100002 will be 3.&lt;/p&gt;

&lt;p&gt;This is extremely powerful as this is key to share files between your main host system and your container without loosing access to them. You need a common uid. This common uid will be root in the container while being yours in the real system context.&lt;/p&gt;

&lt;h3 id=&#34;give-power-back-to-the-user-no-security-compromise&#34;&gt;Give power back to the user, no (security) compromise&lt;/h3&gt;

&lt;p&gt;With all this in mind, we can put the pieces together and let the magic happen. We need to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;get latest Docker release (&amp;gt;=1.10.0)&lt;/li&gt;
&lt;li&gt;configure the subids so that our user will act as root in the container&lt;/li&gt;
&lt;li&gt;configure docker so that it used our ranges&lt;/li&gt;
&lt;li&gt;use real-world applications&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Of course, as the name is passed on the command line of the docker daemon, this will only work for a single user. But keep in mind that Docker 1.10 is the first version to support this feature. It may evolve in the future and get more flexible&lt;/p&gt;

&lt;p&gt;OK, let&amp;rsquo;s start. Assuming our user is &amp;ldquo;yadutaf&amp;rdquo; (that&amp;rsquo;s me) with uid 1000, we&amp;rsquo;ll want &lt;code&gt;/etc/subuid&lt;/code&gt; and &lt;code&gt;/etc/subgid&lt;/code&gt; to contain:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yadutaf:1000:1
yadutaf:100000:65535
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we want docker daemon to use it, without messing with systemd&amp;rsquo;s unit files (trust me, you don&amp;rsquo;t want to), so we&amp;rsquo;ll use the docker configuration file &lt;code&gt;/etc/docker/daemon.json&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
        &amp;quot;userns-remap&amp;quot;: &amp;quot;yadutaf&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All we have to do is restart the daemon, run an innocent, random, test container and see the result:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo systemctl restart docker
$ docker run -d --name redis-userns redis
$ cat /proc/$(docker inspect -f &#39;{{ .State.Pid }}&#39; redis-userns)/uid_map
         0       1000          1
         1     100000      65535
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hooray!&lt;/p&gt;

&lt;p&gt;What about graphical? What about sound? You promised read applications didn&amp;rsquo;t you? Sure I did. Here is a working Firefox:&lt;/p&gt;

&lt;p&gt;First, the Dockerfile:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM ubuntu
MAINTAINER Jean-Tiare Le Bigot &amp;lt;jt AT yadutaf DOT fr&amp;gt;

# Get PulseAudio for the sound, Firefox for, well, you know...
RUN apt-get update &amp;amp;&amp;amp; apt-get -y install firefox pulseaudio

ENTRYPOINT [&amp;quot;firefox&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Build and run it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker build -t firefox .
$ docker run --rm -it \
    -v /tmp/.X11-unix:/tmp/.X11-unix \
    -v /run/user/$UID/pulse/native:/run/pulse \
    -e DISPLAY=unix$DISPLAY \
    -e PULSE_SERVER=unix:/run/pulse \
    --name firefox \
    firefox --new-instance &amp;quot;https://www.youtube.com/watch?v=k1-TrAvp_xs&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What it does is:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;share the X11 socket&lt;/li&gt;
&lt;li&gt;share the user&amp;rsquo;s pulseaudio socket as root&amp;rsquo;s&lt;/li&gt;
&lt;li&gt;expose them via environment variables&lt;/li&gt;
&lt;li&gt;start it!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As a (desirable) side-effect, setting docker daemon with user namespaces effectively disables a variety of security sensitive options like starting privileged containers or sharing the host&amp;rsquo;s network. This extra-security comes with the kernel&amp;rsquo;s implementation and we&amp;rsquo;ll certainly not refuse it!&lt;/p&gt;

&lt;p&gt;Of course, this has limitations. For example, if you try with chrome, you&amp;rsquo;ll be disappointed to realize there is no sound. This is because chrome requires the older Alsa sound system which are only accessible to the &amp;ldquo;audio&amp;rdquo; group. But this group is not and can&amp;rsquo;t be mapped in Docker just yet. This is supported by the kernel though. Just not Docker. By the way, if you want to test out chrome, make sure to add the &lt;code&gt;--disable-setuid-sandbox&lt;/code&gt; flag&lt;/p&gt;

&lt;p&gt;This limitation aside, this is fairly interesting. Using similar setups, you can have docker on your host, exploit most of it power, without ever taking the risk to compromise your security or integrity.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to turn any syscall into an event: Introducing eBPF Kernel probes</title>
      <link>http://blog.yadutaf.fr/2016/03/30/turn-any-syscall-into-event-introducing-ebpf-kernel-probes/</link>
      <pubDate>Wed, 30 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>http://blog.yadutaf.fr/2016/03/30/turn-any-syscall-into-event-introducing-ebpf-kernel-probes/</guid>
      <description>

&lt;p&gt;TL;DR: Using eBPF in recent (&amp;gt;=4.4) Linux kernel, you can turn any kernel function call into a user land event with arbitrary data. This is made easy by bcc. The probe is written in C while the data is handled by python.&lt;/p&gt;

&lt;p&gt;If you are not familiar with eBPF or linux tracing, you really should read the full post. It tries to progressively go through the pitfalls I stumbled unpon while playing around with bcc / eBPF while saving you a lot of the time I spent searching and digging.&lt;/p&gt;

&lt;h3 id=&#34;a-note-on-push-vs-pull-in-a-linux-world&#34;&gt;A note on push vs pull in a Linux world&lt;/h3&gt;

&lt;p&gt;When I started to work on containers, I was wondering how we could update a load balancer configuration dynamically based on actual system state. A common strategy, which works, it to let the container orchestrator trigger a load balancer configuration update whenever it starts a container and then let the load balancer poll the container until some health check passes. It may be a simple &amp;ldquo;SYN&amp;rdquo; test.&lt;/p&gt;

&lt;p&gt;While this configuration works, it has the downside of making your load balancer waiting for some system to be available while it should be&amp;hellip; load balancing.&lt;/p&gt;

&lt;p&gt;Can we do better?&lt;/p&gt;

&lt;p&gt;When you want a program to react to some change in a system there are 2 possible strategies. The program may &lt;em&gt;poll&lt;/em&gt; the system to detect changes or, if the system supports it, the system may &lt;em&gt;push&lt;/em&gt; events and let the program react to them. Wether you want to use push or poll depends on the context. A good rule of the thumb is to use push events when the event rate is low with respect to the processing time and switch to polling when the events are coming fast or the system may become unusable. For example, typical network driver will wait for events from the network card while frameworks like dpdk will actively poll the card for events to achieve the highest throughput and lowest latency.&lt;/p&gt;

&lt;p&gt;In an ideal world, we&amp;rsquo;d have some kernel interface telling us:&lt;/p&gt;

&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;Hey Mr. ContainerManager, I&amp;rsquo;ve just created a socket for the Nginx-ware of container &lt;em&gt;servestaticfiles&lt;/em&gt;, maybe you want to update your state?&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Sure Mr. OS, Thanks for letting me know&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;While Linux has a wide range of interfaces to deal with events, up to 3 for file events, there is no dedicated interface to get socket event notifications. You can get routing table events, neighbor table events, conntrack events, interface change events. Just, not socket events. Or maybe there is, deep hidden in a Netlink interface.&lt;/p&gt;

&lt;p&gt;Ideally, we&amp;rsquo;d need a generic way to do it. How?&lt;/p&gt;

&lt;h3 id=&#34;kernel-tracing-and-ebpf-a-bit-of-history&#34;&gt;Kernel tracing and eBPF, a bit of history&lt;/h3&gt;

&lt;p&gt;Until recently the only way was to patch the kernel or resort on SystemTap. &lt;a href=&#34;https://en.wikipedia.org/wiki/SystemTap&#34;&gt;SytemTap&lt;/a&gt; is a tracing Linux system. In a nutshell, it provides a DSL which is then compiled into a kernel module which is then live-loaded into the running kernel. Except that some production system disable dynamic module loading for security reasons. Including the one I was working on at that time. The other way would be to patch the kernel to trigger some events, probably based on netlink. This is not really convenient. Kernel hacking come with downsides including &amp;ldquo;interesting&amp;rdquo; new &amp;ldquo;features&amp;rdquo; and increased maintenance burden.&lt;/p&gt;

&lt;p&gt;Hopefully, starting with Linux 3.15 the ground was laid to safely transform any traceable kernel function into userland events. &amp;ldquo;Safely&amp;rdquo; is common computer science expression referring to &amp;ldquo;some virtual machine&amp;rdquo;. This case is no exception. Linux has had one for years. Since Linux 2.1.75 released in 1997 actually. It&amp;rsquo;s called Berkeley Packet Filter of BPF for short. As its name suggests, it was originally developed for the BSD firewalls. It had only 2 registers and only allowed forward jumps meaning that you could not write loops with it (Well, you can, if you know the maximum iterations and you manually unroll them). The point was to guarantee the program would always terminate and hence never hang the system. Still not sure if it has any use while you have iptables? It serves as the &lt;a href=&#34;https://blog.cloudflare.com/bpf-the-forgotten-bytecode/&#34;&gt;foundation of CloudFlare&amp;rsquo;s AntiDDos protection&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;OK, so, with Linux the 3.15, &lt;a href=&#34;TODO&#34;&gt;BPF was extended&lt;/a&gt; turning it into eBPF. For &amp;ldquo;extended&amp;rdquo; BPF. It upgrades from 2 32 bits registers to 10 64 bits 64 registers and adds backward jumping among others. It has then been &lt;a href=&#34;https://lwn.net/Articles/604043/&#34;&gt;further extended in Linux 3.18&lt;/a&gt; moving it out of the networking subsystem, and adding tools like maps. To preserve the safety guarantees, it &lt;a href=&#34;http://lxr.free-electrons.com/source/kernel/bpf/verifier.c#L21&#34;&gt;introduces a checker&lt;/a&gt; which validates all memory accesses and possible code path. If the checker can&amp;rsquo;t guarantee the code will terminate within fixed boundaries, it will deny the initial insertion of the program.&lt;/p&gt;

&lt;p&gt;For more history, there is &lt;a href=&#34;http://events.linuxfoundation.org/sites/events/files/slides/tracing-linux-ezannoni-linuxcon-ja-2015_0.pdf&#34;&gt;an excellent Oracle presentation on eBPF&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s get started.&lt;/p&gt;

&lt;h3 id=&#34;hello-from-from-inet-listen&#34;&gt;Hello from from &lt;code&gt;inet_listen&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;As writing assembly is not the most convenient task, even for the best of us, we&amp;rsquo;ll use &lt;a href=&#34;https://github.com/iovisor/bcc&#34;&gt;bcc&lt;/a&gt;. bcc is a collection of tools based on LLVM and Python abstracting the underlying machinery. Probes are written in C and the results can be exploited from python allowing to easily write non trivial applications.&lt;/p&gt;

&lt;p&gt;Start by install bcc. For some of these examples, you may require a recent (read &amp;gt;= 4.4) version of the kernel. If you are willing to actually try these examples, I highly recommend that you setup a VM. &lt;em&gt;NOT&lt;/em&gt; a docker container. You can&amp;rsquo;t change the kernel in a container. As this is a young and dynamic projects, install instructions are highly platform/version dependant. You can find up to date instructions on &lt;a href=&#34;https://github.com/iovisor/bcc/blob/master/INSTALL.md&#34;&gt;https://github.com/iovisor/bcc/blob/master/INSTALL.md&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So, we want to get an event whenever a program starts to listen on TCP socket. When calling the &lt;code&gt;listen()&lt;/code&gt; syscall on a &lt;code&gt;AF_INET&lt;/code&gt; + &lt;code&gt;SOCK_STREAM&lt;/code&gt; socket, the underlying kernel function is &lt;a href=&#34;http://lxr.free-electrons.com/source/net/ipv4/af_inet.c#L194&#34;&gt;&lt;code&gt;inet_listen&lt;/code&gt;&lt;/a&gt;. We&amp;rsquo;ll start by hooking a &amp;ldquo;Hello World&amp;rdquo; &lt;code&gt;kprobe&lt;/code&gt; on it&amp;rsquo;s entrypoint.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from bcc import BPF

# Hello BPF Program
bpf_text = &amp;quot;&amp;quot;&amp;quot; 
#include &amp;lt;net/inet_sock.h&amp;gt;
#include &amp;lt;bcc/proto.h&amp;gt;

// 1. Attach kprobe to &amp;quot;inet_listen&amp;quot;
int kprobe__inet_listen(struct pt_regs *ctx, struct socket *sock, int backlog)
{
    bpf_trace_printk(&amp;quot;Hello World!\\n&amp;quot;);
    return 0;
};
&amp;quot;&amp;quot;&amp;quot;

# 2. Build and Inject program
b = BPF(text=bpf_text)

# 3. Print debug output
while True:
    print b.trace_readline()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This program does 3 things:
1. It attaches a kernel probe to &amp;ldquo;inet_listen&amp;rdquo; using a naming convention. If the function was called, say, &amp;ldquo;my_probe&amp;rdquo;, it could be explicitly attached with &lt;code&gt;b.attach_kprobe(&amp;quot;inet_listen&amp;quot;, &amp;quot;my_probe&amp;quot;&lt;/code&gt;.
2. It builds the program using LLVM new BPF backend, inject the resulting bytecode using the (new) &lt;code&gt;bpf()&lt;/code&gt; syscall and automatically attaches the probes matching the naming convention.
3. It reads the raw output from the kernel pipe.&lt;/p&gt;

&lt;p&gt;Note: eBPF backend of LLVM is still young. If you think you&amp;rsquo;ve hit a bug, you may want to upgrade.&lt;/p&gt;

&lt;p&gt;Noticed the &lt;code&gt;bpf_trace_printk&lt;/code&gt; call? This is a stripped down version of the kernel&amp;rsquo;s &lt;code&gt;printk()&lt;/code&gt; debug function. When used, it produces tracing informations to a special kernel pipe in &lt;code&gt;/sys/kernel/debug/tracing/trace_pipe&lt;/code&gt;. As the name implies, this is a pipe. If multiple readers are consuming it, only 1 will get a given line. This makes it unsuitable for production.&lt;/p&gt;

&lt;p&gt;Fortunately, Linux 3.19 introduced maps for message passing and Linux 4.4 brings arbitrary perf events support. I&amp;rsquo;ll demo the perf event based approach later in this post.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# From a first console
ubuntu@bcc:~/dev/listen-evts$ sudo /python tcv4listen.py 
              nc-4940  [000] d... 22666.991714: : Hello World!
 
# From a second console
ubuntu@bcc:~$ nc -l 0 4242
^C
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Yay!&lt;/p&gt;

&lt;h3 id=&#34;grab-the-backlog&#34;&gt;Grab the backlog&lt;/h3&gt;

&lt;p&gt;Now, let&amp;rsquo;s print some easily accessible data. Say the &amp;ldquo;backlog&amp;rdquo;. The backlog is the number of pending established TCP connections, pending to be &lt;code&gt;accept()&lt;/code&gt;ed.&lt;/p&gt;

&lt;p&gt;Just tweak a bit the &lt;code&gt;bpf_trace_printk&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;bpf_trace_printk(&amp;quot;Listening with with up to %d pending connections!\\n&amp;quot;, backlog);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you re-run the example with this world-changing improvement, you should see something like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(bcc)ubuntu@bcc:~/dev/listen-evts$ sudo python tcv4listen.py 
              nc-5020  [000] d... 25497.154070: : Listening with with up to 1 pending connections!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;nc&lt;/code&gt; is a single connection program, hence the backlog of 1. Nginx or Redis would output 128 here. But that&amp;rsquo;s another story.&lt;/p&gt;

&lt;p&gt;Easy hue? Now let&amp;rsquo;s get the port.&lt;/p&gt;

&lt;h3 id=&#34;grab-the-port-and-ip&#34;&gt;Grab the port and IP&lt;/h3&gt;

&lt;p&gt;Studying &lt;code&gt;inet_listen&lt;/code&gt; source from the kernel, we know that we need to get the &lt;code&gt;inet_sock&lt;/code&gt; from the &lt;code&gt;socket&lt;/code&gt; object. Just copy from the sources, and insert at the beginning of the tracer:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// cast types. Intermediate cast not needed, kept for readability
struct sock *sk = sock-&amp;gt;sk;
struct inet_sock *inet = inet_sk(sk);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The port can now be accessed from &lt;code&gt;inet-&amp;gt;inet_sport&lt;/code&gt; in network byte order (aka: Big Endian). Easy! So, we could just replace the &lt;code&gt;bpf_trace_printk&lt;/code&gt; with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;bpf_trace_printk(&amp;quot;Listening on port %d!\\n&amp;quot;, inet-&amp;gt;inet_sport);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then run:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ubuntu@bcc:~/dev/listen-evts$ sudo /python tcv4listen.py 
...
R1 invalid mem access &#39;inv&#39;
...
Exception: Failed to load BPF program kprobe__inet_listen
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Except that it&amp;rsquo;s not (yet) so simple. Bcc is improving a &lt;em&gt;lot&lt;/em&gt; currently. While writing this post, a couple of pitfalls had already been addressed. But not yet all. This Error means the in-kernel checker could prove the memory accesses in program are correct. See the explicit cast. We need to help is a little by making the accesses more explicit. We&amp;rsquo;ll use &lt;code&gt;bpf_probe_read&lt;/code&gt; trusted function to read an arbitrary memory location while guaranteeing all necessary checks are done with something like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// Explicit initialization. The &amp;quot;=0&amp;quot; part is needed to &amp;quot;give life&amp;quot; to the variable on the stack
u16 lport = 0;

// Explicit arbitrary memory access. Read it:
//    Read into &#39;lport&#39;, &#39;sizeof(lport)&#39; bytes from &#39;inet-&amp;gt;inet_sport&#39; memory location
bpf_probe_read(&amp;amp;lport, sizeof(lport), &amp;amp;(inet-&amp;gt;inet_sport));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Reading the bound address for IPv4 is basically the same, using &lt;code&gt;inet-&amp;gt;inet_rcv_saddr&lt;/code&gt;. If we put is all together, we should get the backlog, the port and the bound IP:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from bcc import BPF  
  
# BPF Program  
bpf_text = &amp;quot;&amp;quot;&amp;quot;   
#include &amp;lt;net/sock.h&amp;gt;  
#include &amp;lt;net/inet_sock.h&amp;gt;  
#include &amp;lt;bcc/proto.h&amp;gt;  
  
// Send an event for each IPv4 listen with PID, bound address and port  
int kprobe__inet_listen(struct pt_regs *ctx, struct socket *sock, int backlog)  
{  
    // Cast types. Intermediate cast not needed, kept for readability  
    struct sock *sk = sock-&amp;gt;sk;  
    struct inet_sock *inet = inet_sk(sk);  

    // Working values. You *need* to initialize them to give them &amp;quot;life&amp;quot; on the stack and use them afterward  
    u32 laddr = 0;  
    u16 lport = 0;  

    // Pull in details. As &#39;inet_sk&#39; is internally a type cast, we need to use &#39;bpf_probe_read&#39;  
    // read: load into &#39;laddr&#39; &#39;sizeof(laddr)&#39; bytes from address &#39;inet-&amp;gt;inet_rcv_saddr&#39;  
    bpf_probe_read(&amp;amp;laddr, sizeof(laddr), &amp;amp;(inet-&amp;gt;inet_rcv_saddr));  
    bpf_probe_read(&amp;amp;lport, sizeof(lport), &amp;amp;(inet-&amp;gt;inet_sport));  

    // Push event
    bpf_trace_printk(&amp;quot;Listening on %x %d with %d pending connections\\n&amp;quot;, ntohl(laddr), ntohs(lport), backlog);  
    return 0;
};  
&amp;quot;&amp;quot;&amp;quot;  
  
# Build and Inject BPF  
b = BPF(text=bpf_text)  
  
# Print debug output  
while True:  
  print b.trace_readline()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A test run should output something like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;(bcc)ubuntu@bcc:~/dev/listen-evts$ sudo python tcv4listen.py 
              nc-5024  [000] d... 25821.166286: : Listening on 7f000001 4242 with 1 pending connections
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Provided that you listen on localhost. The address is displayed as hex here to avoid dealing with the IP pretty printing but that&amp;rsquo;s all wired. And that&amp;rsquo;s cool.&lt;/p&gt;

&lt;p&gt;Note: you may wonder why &lt;code&gt;ntohs&lt;/code&gt; and &lt;code&gt;ntohl&lt;/code&gt; can be called from BPF while they are not trusted. This is because they are macros and inline functions from &amp;ldquo;.h&amp;rdquo; files and a small bug was &lt;a href=&#34;https://github.com/iovisor/bcc/pull/453&#34;&gt;fixed&lt;/a&gt; while writing this post.&lt;/p&gt;

&lt;p&gt;All done, one more piece: We want to get the related container. In the context of networking, that&amp;rsquo;s means we want the network namespace. The network namespace being the building block of containers allowing them to have isolated networks.&lt;/p&gt;

&lt;h3 id=&#34;grab-the-network-namespace-a-forced-introduction-to-perf-events&#34;&gt;Grab the network namespace: a forced introduction to perf events&lt;/h3&gt;

&lt;p&gt;On the userland, the network namespace can be determined by checking the target of &lt;code&gt;/proc/PID/ns/net&lt;/code&gt;. It should look like &lt;code&gt;net:[4026531957]&lt;/code&gt;. The number between brackets is the inode number of the network namespace. This said, we could grab it by scrapping &amp;lsquo;/proc&amp;rsquo; but this is racy, we may be dealing with short-lived processes. And races are never good. We&amp;rsquo;ll grab the inode number directly from the kernel. Fortunately, that&amp;rsquo;s an easy one:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// Create an populate the variable
u32 netns = 0;

// Read the netns inode number, like /proc does
netns = sk-&amp;gt;__sk_common.skc_net.net-&amp;gt;ns.inum;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Easy. And it works.&lt;/p&gt;

&lt;p&gt;But if you&amp;rsquo;ve read so far, you may guess there is something wrong somewhere. And there is:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;bpf_trace_printk(&amp;quot;Listening on %x %d with %d pending connections in container %d\\n&amp;quot;, ntohl(laddr), ntohs(lport), backlog, netns);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you try to run it, you&amp;rsquo;ll get some cryptic error message:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;(bcc)ubuntu@bcc:~/dev/listen-evts$ sudo python tcv4listen.py
error: in function kprobe__inet_listen i32 (%struct.pt_regs*, %struct.socket*, i32)
too many args to 0x1ba9108: i64 = Constant&amp;lt;6&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What clang is trying to tell you is &amp;ldquo;Hey pal, &lt;code&gt;bpf_trace_printk&lt;/code&gt; can only take 4 arguments, you&amp;rsquo;ve just used 5.&amp;ldquo;. I won&amp;rsquo;t dive into the details here, but that&amp;rsquo;s a BPF limitation. If you want to dig it, &lt;a href=&#34;http://lxr.free-electrons.com/source/kernel/trace/bpf_trace.c#L86&#34;&gt;here is a good starting point&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The only way to fix it is to&amp;hellip; stop debugging and make it production ready. So let&amp;rsquo;s get started (and make sure run at least Linux 4.4). We&amp;rsquo;ll use perf events which supports passing arbitrary sized structures to userland. Additionally, only our reader will get it so that multiple unrelated eBPF programs can produce data concurrently without issues.&lt;/p&gt;

&lt;p&gt;To use it, we need to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;define a structure&lt;/li&gt;
&lt;li&gt;declare the event&lt;/li&gt;
&lt;li&gt;push the event&lt;/li&gt;
&lt;li&gt;re-declare the event on Python&amp;rsquo;s side (This step should go away in the future)&lt;/li&gt;
&lt;li&gt;consume and format the event&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This may seem like a lot, but it ain&amp;rsquo;t. See:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// At the begining of the C program, declare our event
struct listen_evt_t {
    u64 laddr;
    u64 lport;
    u64 netns;
    u64 backlog;
};
BPF_PERF_OUTPUT(listen_evt);

// In kprobe__inet_listen, replace the printk with
struct listen_evt_t evt = {
    .laddr = ntohl(laddr),
    .lport = ntohs(lport),
    .netns = netns,
    .backlog = backlog,
};
listen_evt.perf_submit(ctx, &amp;amp;evt, sizeof(evt));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Python side will require a little more work, though:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# We need ctypes to parse the event structure
import ctypes

# Declare data format
class ListenEvt(ctypes.Structure):
    _fields_ = [
        (&amp;quot;laddr&amp;quot;,   ctypes.c_ulonglong),
        (&amp;quot;lport&amp;quot;,   ctypes.c_ulonglong),
        (&amp;quot;netns&amp;quot;,   ctypes.c_ulonglong),
        (&amp;quot;backlog&amp;quot;, ctypes.c_ulonglong),
    ]

# Declare event printer
def print_event(cpu, data, size):
    event = ctypes.cast(data, ctypes.POINTER(ListenEvt)).contents
    print(&amp;quot;Listening on %x %d with %d pending connections in container %d&amp;quot; % (
        event.laddr,
        event.lport,
        event.backlog,
        event.netns,
    ))

# Replace the event loop
b[&amp;quot;listen_evt&amp;quot;].open_perf_buffer(print_event)
while True:
    b.kprobe_poll()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Give it a try. In this example, I have a redis running in a docker container and nc on the host:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;(bcc)ubuntu@bcc:~/dev/listen-evts$ sudo python tcv4listen.py
Listening on 0 6379 with 128 pending connections in container 4026532165
Listening on 0 6379 with 128 pending connections in container 4026532165
Listening on 7f000001 6588 with 1 pending connections in container 4026531957
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;last-word&#34;&gt;Last word&lt;/h3&gt;

&lt;p&gt;Absolutely everything is now setup to use trigger events from arbitrary function calls in the kernel using eBPF, and you should have seen most of the common pitfalls I hit while learning eBPF. If you want to see the full version of this tool, along with some more tricks like IPv6 support, have a look at &lt;a href=&#34;https://github.com/iovisor/bcc/blob/master/tools/solisten.py&#34;&gt;https://github.com/iovisor/bcc/blob/master/tools/solisten.py&lt;/a&gt;. It&amp;rsquo;s now an official tool, thanks to the support of the bcc team.&lt;/p&gt;

&lt;p&gt;To go further, you may want to checkout Brendan Gregg&amp;rsquo;s blog, in particular &lt;a href=&#34;http://www.brendangregg.com/blog/2015-05-15/ebpf-one-small-step.html&#34;&gt;the post about eBPF maps and statistics&lt;/a&gt;. He his one of the project&amp;rsquo;s main contributor.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Security as a commercial strategy</title>
      <link>http://blog.yadutaf.fr/2016/02/26/security-as-a-commercial-strategy/</link>
      <pubDate>Fri, 26 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>http://blog.yadutaf.fr/2016/02/26/security-as-a-commercial-strategy/</guid>
      <description>&lt;p&gt;This post is an essay. I am not a business expert. I am not working for Cloudflare. Actually, I&amp;rsquo;m working for a &lt;a href=&#34;https://www.ovh.com/&#34;&gt;competitor&lt;/a&gt;. This is an attempt to understand Cloudflare&amp;rsquo;s strategy, based on my own (rather short) experience.&lt;/p&gt;

&lt;p&gt;2 days ago, Cloudflare &lt;a href=&#34;https://blog.cloudflare.com/introducing-cloudflare-registrar/&#34;&gt;announced&lt;/a&gt; they created a new Registrar, but not one like all the quadrillions other registrars. A registrar for security concerned companies. Just in case, a &lt;a href=&#34;https://en.wikipedia.org/wiki/Domain_name_registrar&#34;&gt;registrar&lt;/a&gt; is the guy who lends you a domain name.&lt;/p&gt;

&lt;p&gt;Long story short, their reasoning is pretty straight-forward:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;domain names are critical to an organization&lt;/li&gt;
&lt;li&gt;most registrar authenticate organization as any individual&lt;/li&gt;
&lt;li&gt;people knowing the authentication secret eventually leave&lt;/li&gt;
&lt;li&gt;people knowing the authentication secret eventually &lt;em&gt;might&lt;/em&gt; be abused&lt;/li&gt;
&lt;li&gt;in turn, domain names &lt;em&gt;might&lt;/em&gt; eventually be abused&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So they told themselves, why not fixing this, like they did for content distribution?&lt;/p&gt;

&lt;p&gt;But here is the catch: unlike content distribution which requires a pretty big initial investment, anybody can bootstrap a registrar with only a handful of cheap servers from any cloud provider. The costs stay low. If you have no customer, you pay close to nothing. You can even bootstrap your activity &lt;a href=&#34;https://partners.ovh.com/&#34;&gt;as a reseller&lt;/a&gt; for a couple of bucks. This is cheap, this is easy. The landscape is &lt;a href=&#34;https://www.icann.org/registrar-reports/accredited-list.html&#34;&gt;crowded&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So, you want to be a registrar? Great. How different are you?&lt;/p&gt;

&lt;p&gt;With this in mind, CloudFlare&amp;rsquo;s move is a clever one. By focusing on security, they bootstrap their registrar activity with a limited number of high level, technically skilled, big paying customers with easy to implement things on their side (ACLs). All this while capitalizing on their existing image. Remember, the AntiDDos thing.&lt;/p&gt;

&lt;p&gt;Then, in a second time, they&amp;rsquo;ll most probably generalize (if not yet done) this security to all accounts and services AND most probably open their registrar offers to any one. And maybe even offer free domains with a custom &lt;a href=&#34;https://en.wikipedia.org/wiki/Generic_top-level_domain&#34;&gt;gTLD&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;One step at a time.&lt;/p&gt;

&lt;p&gt;More generally, their strategy to attract bigger customers seems to be security. The most obvious way is their &lt;a href=&#34;https://www.cloudflare.com/ddos/&#34;&gt;L7 attack mitigation&lt;/a&gt;. On the same trend, they developed a way to offer SSL termination on their side without giving them the secret key (basically, &lt;a href=&#34;https://blog.cloudflare.com/keyless-ssl-the-nitty-gritty-technical-details/&#34;&gt;a patch in OpenSSL&lt;/a&gt; to implement the Oracle pattern). They did this to target banks (in this case, a BitCoin exchange) by offering them scalability without compromising on the security. Again, security.&lt;/p&gt;

&lt;p&gt;Should security be your main concern for your business? No idea. Seriously, I don&amp;rsquo;t know it, you do. It &lt;em&gt;is&lt;/em&gt; important, for sure. What I can tell you is CloudFlare was clever. They turned what most see as a constraint not only as a commercial argument but as the core of their commercial strategy. A constraint turned into a strength.&lt;/p&gt;

&lt;p&gt;What is your future strength?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Of being hacked, found guilty of spam</title>
      <link>http://blog.yadutaf.fr/2015/08/25/of-being-hacked-found-guilty-of-spam/</link>
      <pubDate>Tue, 25 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>http://blog.yadutaf.fr/2015/08/25/of-being-hacked-found-guilty-of-spam/</guid>
      <description>&lt;p&gt;A few days ago, my hosting company sent me an automated email notifying me that port 25 had been blocked on my personal server. Cause: It had been found guilty of sending spam. As I&amp;rsquo;m not (at least officially) in the spam business, this could only mean one thing: I got hacked.&lt;/p&gt;

&lt;p&gt;I was shocked. If felt to me as though I was having a car accident.&lt;/p&gt;

&lt;p&gt;The first think to do in such situations is to restrict to the bare minimum connections from the outside world to regain control of the machine. In my case, I rebooted the server to rescue mode with only SSH access. This means mail server downtime BUT SMTP protocol is reliable by design. Actually, it has been developed when Internet barely existed and mails where directly hosted on terminals with intermittent connexions. Hence, not an issue.&lt;/p&gt;

&lt;p&gt;Next, inspect postfix queue to get an overview:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;postqueue -p | head
&lt;/pre&gt;

&lt;p&gt;Dumping a random email from the queue is also a good idea:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;postcat -qv POSTFIX_QUEUE_ID
&lt;/pre&gt;

&lt;p&gt;This gives a good idea of where the bulk of the emails came from. In this specific scenario, most mail (~55K) were coming from &amp;#8220;@blog.jtlebi.fr&amp;#8221;. Which is a pretty good news since NO legitimate mail is ever sent from this domain. Anyway, at this point, you should be able to infer basic patterns.&lt;/p&gt;

&lt;p&gt;Time to filter out the spam. The film-hacker way: with a shiny progress bar. Actully, this is not about hype but truly about getting feedback. Filtering 10s of thousands of mails using postfix tools takes a &lt;em&gt;very&lt;/em&gt; long time. You need to have an ETA. Here is the command I used:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;CANDIDATES=&#34;grep -rlP &#39;(MAILER-DAEMON|@blog\.jtlebi\.fr)&#39; /var/spool/postfix/deferred/&#34;; (
    eval $CANDIDATES   # get the list of mails, directly from the pool
    | tee deleting     # track actions
    | grep -o &#39;[^/]*$&#39; # extract POSTFIX_QUEUE_ID
    | pv -lns $(eval $CANDIDATES | wc -l) -i0.1 # compute progress based on processed lines (mails) vs matching files (mails) in the spool.
    | postadmin -d -   # delete mail by  POSTFIX_QUEUE_ID (1 per line)

) 2&amp;gt;&amp;1
| dialog --no-lines --no-shadow  --gauge &#34;Delicately filtering away da F*cking spam... &#34; 7 70 # The hype thing
&lt;/pre&gt;

&lt;p&gt;After that, before re-opening accesses, do not forget to close the holes the hacker came in through. Temporary fix was to upgrade all, disable most plugins. Long term fix ? &lt;strong&gt;KILL WORDPRESS&lt;/strong&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Force a remote host to reboot via VNC</title>
      <link>http://blog.yadutaf.fr/2015/05/04/force-a-remote-host-to-reboot-via-vnc/</link>
      <pubDate>Mon, 04 May 2015 00:00:00 +0000</pubDate>
      
      <guid>http://blog.yadutaf.fr/2015/05/04/force-a-remote-host-to-reboot-via-vnc/</guid>
      <description>&lt;p&gt;Yesterday, dealt with a machine in a pretty bad state:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;SSH was Down&lt;/li&gt;
&lt;li&gt;Memory was exhausted (OOM)&lt;/li&gt;
&lt;li&gt;Ctrl + Alt + Del from VNC was not responding&lt;/li&gt;
&lt;li&gt;A background operation on the OpenStack API was preventing any &lt;code&gt;nova reboot --hard zombie-essential-instance.my-infra.net&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In such situations, the last resort is &lt;code&gt;Alt+SysRQ+b&lt;/code&gt; to force the host into immediate reboot, possible loosing or corrupting data data in the way.&lt;/p&gt;

&lt;p&gt;The trick is that, obviously, you can not type this sequence on your laptop as usual, or the machine that will reboot will not be the one you expect&amp;#8230; Hence to goal is to feed the relevant keycodes directly to VNC. &lt;a href=&#34;http://www.realvnc.com/docs/rfbproto.pdf&#34;&gt;As VNC has originally been built specifically for X11&lt;/a&gt;, the keycodes you need to send are the one X11 itself uses internally. Which are found &lt;a href=&#34;http://www.cl.cam.ac.uk/~mgk25/ucs/keysymdef.h&#34;&gt;in the source code&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Long story short, the codes you are looking for are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;0xffe9&lt;/code&gt;: Alt&lt;/li&gt;
&lt;li&gt;&lt;code&gt;0xff15&lt;/code&gt;: SySRq&lt;/li&gt;
&lt;li&gt;&lt;code&gt;0x0062&lt;/code&gt;: b&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you are viewing the console through NoVNC, you may feed theses codes to the guest by opening a console in your browser (&lt;code&gt;F12&lt;/code&gt; in most browser) and typing:&lt;/p&gt;

&lt;pre class=&#34;brush: jscript; title: ; notranslate&#34; title=&#34;&#34;&gt;rfb.sendKey(0xffe9, 1);
rfb.sendKey(0xff15, 1);
rfb.sendKey(0x0062, 1);
rfb.sendKey(0x0062, 0);
rfb.sendKey(0xff15, 0);
rfb.sendKey(0xffe9, 0);
&lt;/pre&gt;

&lt;p&gt;This will send the relevant key down events then the key up in reverse order. This is roughly how the &amp;#8220;Send CtrlAltDel&amp;#8221; button works.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How I shrunk a Docker image by 98.8% – featuring fanotify</title>
      <link>http://blog.yadutaf.fr/2015/04/25/how-i-shrunk-a-docker-image-by-98-8-featuring-fanotify/</link>
      <pubDate>Sat, 25 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://blog.yadutaf.fr/2015/04/25/how-i-shrunk-a-docker-image-by-98-8-featuring-fanotify/</guid>
      <description>&lt;p&gt;Some weeks ago, I did an internal presentation on Docker. During the presentation, one of the ops asked an seemingly trivial question: Is there anything like a &amp;#8220;diet program for Docker Images&amp;#8221; ?&lt;/p&gt;

&lt;p&gt;You can find a couple of pretty decent common-sense powered approach &lt;a href=&#34;https://intercityup.com/blog/downsizing-docker-containers.html&#34;&gt;on the web&lt;/a&gt; like removing well known cache folders, temporary files, installing all superfluous packages and flatten layers if not the full image. There is also the &lt;code&gt;-slim&lt;/code&gt; declination of the official language images.&lt;/p&gt;

&lt;p&gt;But, thinking at it, do we &lt;em&gt;really&lt;/em&gt; need a full consistent base Linux install? Which files do we &lt;em&gt;really&lt;/em&gt; need in a given image? I found a radical and pretty efficient approaches with a go binary. It was statically build, almost no external dependency. &lt;a href=&#34;http://blog.codeship.com/building-minimal-docker-containers-for-go-applications/&#34;&gt;Resulting image&lt;/a&gt;: 6.12MB.&lt;/p&gt;

&lt;p&gt;Whaou! Is there any chance to do something comparable, deterministic with any random application?&lt;/p&gt;

&lt;p&gt;It turns out there could be one. The idea is simple: We could profile the image at run time one way or another to determine which files are ever accessed/opened/&amp;#8230;, then remove all the remaining files. Hmm, sounds promising. Let&amp;rsquo;s PoC it.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Target definition&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Start image&lt;/strong&gt;: Ubuntu (~200MB)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Application that MUST run&lt;/strong&gt;: &lt;code&gt;/bin/ls&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Build the smallest possible image&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;/bin/ls&lt;/code&gt; is a good target: It is simple enough for a PoC with no nasty behavior but still not trivial, it uses dynamic linking.&lt;/p&gt;

&lt;p&gt;Now that we have a target, let&amp;rsquo;s pick a tool. As this is a proof of concept, using dynamites where a hole puncher would  be enough &lt;em&gt;IS&lt;/em&gt; an option, as long as it does the job.&lt;/p&gt;

&lt;p&gt;The base idea it to record all file accesses. Be it a stat or a open. There are a couple of good candidates to help with the task. We could use &lt;a href=&#34;http://linux.die.net/man/7/inotify&#34; title=&#34;Man Inotify&#34;&gt;inotify&lt;/a&gt; but it is a pain to setup and watches needs to be attached on every single files, which potentially mean a *lot* of watches. We could use LD_PRELOAD but 1/ it&amp;rsquo;s no fun to use, 2/ it won&amp;rsquo;t catch direct syscalls 3/ it won&amp;rsquo;t work with statically linked programs (who said golang&amp;rsquo;s?). A solution that would work well even for statically linked program would be to use &lt;a href=&#34;http://linux.die.net/man/2/ptrace&#34; title=&#34;Man ptrace&#34;&gt;ptrace&lt;/a&gt; to trace all syscalls, in realtime. It is also a pain to setup but, it would be a reliable and flexible option. A lesser known linux syscall is &lt;a href=&#34;http://man7.org/linux/man-pages/man7/fanotify.7.html&#34;&gt;fanotify&lt;/a&gt;. As the title suggests, This is the one we&amp;rsquo;ll go with&lt;sup&gt;1&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;fanotify&lt;/code&gt; syscall has originally been implemented as &amp;#8220;decent&amp;#8221; mechanism for anti-virus vendors to intercept file access events, potentially on a whole mountpoint at once. Sounds familiar? While it may be used to deny file accesses, it may also just report file access events in a non-blocking fashion, potentially dropping&lt;sup&gt;2&lt;/sup&gt; events if the kernel queue overflows. In this last case, a special message will be generated to notify user-land listener about the message loss. This is perfectly what I needed. Non intrusive, a whole mountpoint at once, simple setup (well, provided that you find the documentation, no comment&amp;#8230;). This may seem anecdotal but it has its importance, as a learned after.&lt;/p&gt;

&lt;p&gt;Using it is fairly simple:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1/ Init &lt;code&gt;fanotify&lt;/code&gt; in &lt;code&gt;FAN_CLASS_NOTIF&lt;/code&gt;ication mode using &lt;a href=&#34;http://man7.org/linux/man-pages/man2/fanotify_init.2.html&#34;&gt;&lt;code&gt;fanotify_init&lt;/code&gt; syscall&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre class=&#34;brush: cpp; title: ; notranslate&#34; title=&#34;&#34;&gt;// Open ``fan`` fd for fanotify notifications. Messages will embed a 
// filedescriptor on accessed file. Expect it to be read-only
fan = fanotify_init(FAN_CLASS_NOTIF, O_RDONLY);
&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;2/ Subscribe to &lt;code&gt;FAN_ACCESS&lt;/code&gt; and &lt;code&gt;FAN_OPEN&lt;/code&gt; events on &amp;#8220;/&amp;#8221; &lt;code&gt;FAN_MARK_MOUNT&lt;/code&gt;point using &lt;a href=&#34;http://man7.org/linux/man-pages/man2/fanotify_mark.2.html&#34;&gt;&lt;code&gt;fanotify_mark&lt;/code&gt; syscall&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre class=&#34;brush: cpp; title: ; notranslate&#34; title=&#34;&#34;&gt;// Watch open/access events on root mountpoint
fanotify_mark(
    fan, 
    FAN_MARK_ADD | FAN_MARK_MOUNT, // Add mountpoint mark to fan
    FAN_ACCESS | FAN_OPEN,         // Report open and access events, non blocking
    -1, &#34;/&#34;                        // Watch root mountpoint (-1 is ignored for FAN_MARK_MOUNT type calls)
);
&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;3/ read&lt;/code&gt; pending event messages from the filedescriptor returned by &lt;code&gt;fanotify_init&lt;/code&gt; and iterate using &lt;code&gt;FAN_EVENT_NEXT&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre class=&#34;brush: cpp; title: ; notranslate&#34; title=&#34;&#34;&gt;// Read pending events from ``fan`` into ``buf``
buflen = read(fan, buf, sizeof(buf));

// Position cursor on first message
metadata = (struct fanotify_event_metadata*)&amp;buf;

// Loop until we reached the last event
while(FAN_EVENT_OK(metadata, buflen)) {
    // Do something interesting with the notification
    // ``metadata-&amp;gt;fd`` will contain a valid, RO fd to accessed file.

    // Close opened fd, otherwise we&#39;ll quickly exhaust the fd pool.
    close(metadata-&amp;gt;fd);

    // Move to next event in buffer
    metadata = FAN_EVENT_NEXT(metadata, buflen);
}
&lt;/pre&gt;

&lt;p&gt;Putting it all together, we&amp;rsquo;ll print the full name of all accessed files and add queue overflow detection. This should be plain enough for us (comments and error checks stripped for the purpose of this illustration):&lt;/p&gt;

&lt;pre class=&#34;brush: cpp; title: ; notranslate&#34; title=&#34;&#34;&gt;#include &amp;lt;fcntl.h&amp;gt;
#include &amp;lt;limits.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;sys/fanotify.h&amp;gt;

int main(int argc, char** argv) {
    int fan;
    char buf[4096];
    char fdpath[32];
    char path[PATH_MAX + 1];
    ssize_t buflen, linklen;
    struct fanotify_event_metadata *metadata;

    // Init fanotify structure
    fan = fanotify_init(FAN_CLASS_NOTIF, O_RDONLY);

    // Watch open/access events on root mountpoint
    fanotify_mark(
        fan,
        FAN_MARK_ADD | FAN_MARK_MOUNT,
        FAN_ACCESS | FAN_OPEN,
        -1, &#34;/&#34;
    );

    while(1) {
        buflen = read(fan, buf, sizeof(buf));
        metadata = (struct fanotify_event_metadata*)&amp;buf;

        while(FAN_EVENT_OK(metadata, buflen)) {
            if (metadata-&amp;gt;mask &amp; FAN_Q_OVERFLOW) {
                printf(&#34;Queue overflow!\n&#34;);
                continue;
            }

            // Resolve path, using automatically opened fd
            sprintf(fdpath, &#34;/proc/self/fd/%d&#34;, metadata-&amp;gt;fd);
            linklen = readlink(fdpath, path, sizeof(path) - 1);
            path[linklen] = &#39;&amp;#92;&amp;#48;&#39;;
            printf(&#34;%s\n&#34;, path);

            close(metadata-&amp;gt;fd);
            metadata = FAN_EVENT_NEXT(metadata, buflen);
        }
    }
}
&lt;/pre&gt;

&lt;p&gt;To build it, use:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;gcc main.c --static -o fanotify-profiler
&lt;/pre&gt;

&lt;p&gt;We basically now have a tool to report any file access on the active &amp;#8216;/&amp;rsquo; mountpoint in real time. Good.&lt;/p&gt;

&lt;p&gt;What now? Let&amp;rsquo;s create an Ubuntu container, start the recorder and run &lt;code&gt;/bin/ls&lt;/code&gt;. &lt;code&gt;fanotify&lt;/code&gt; requires require the &amp;#8220;&lt;code&gt;CAP_SYS_ADMIN&lt;/code&gt;&amp;#8221; capability. This is basically the &amp;#8220;catch-all&amp;#8221; root &lt;a href=&#34;http://linux.die.net/man/7/capabilities&#34;&gt;capability&lt;/a&gt;. Still better than running in &lt;code&gt;--privileged&lt;/code&gt; mode though.&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;# Run image
docker run --name profiler_ls \
           --volume $PWD:/src \
           --cap-add SYS_ADMIN \
           -it ubuntu /src/fanotify-profiler

# Run the command to profile, from another shell
docker exec -it profiler_ls ls

# Interrupt Running image using
docker kill profiler_ls # You know, the &#34;dynamite&#34;
&lt;/pre&gt;

&lt;p&gt;This should produce an output like:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;/etc/passwd
/etc/group
/etc/passwd
/etc/group
/bin/ls
/bin/ls
/bin/ls
/lib/x86_64-linux-gnu/ld-2.19.so
/lib/x86_64-linux-gnu/ld-2.19.so
/etc/ld.so.cache
/lib/x86_64-linux-gnu/libselinux.so.1
/lib/x86_64-linux-gnu/libacl.so.1.1.0
/lib/x86_64-linux-gnu/libc-2.19.so
/lib/x86_64-linux-gnu/libc-2.19.so
/lib/x86_64-linux-gnu/libpcre.so.3.13.1
/lib/x86_64-linux-gnu/libdl-2.19.so
/lib/x86_64-linux-gnu/libdl-2.19.so
/lib/x86_64-linux-gnu/libattr.so.1.1.0
&lt;/pre&gt;

&lt;p&gt;Awesome! It worked. We now know for sure what &lt;code&gt;/bin/ls&lt;/code&gt; ultimately needs to run.&lt;/p&gt;

&lt;p&gt;So we&amp;rsquo;ll just copy-paste-import all this in a &amp;#8220;&lt;code&gt;FROM scratch&lt;/code&gt;&amp;#8221; Docker Image and we&amp;rsquo;ll be done. Easy. Well, not so. But let&amp;rsquo;s do it to see by ourselves.&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;# Export base docker image
mkdir ubuntu_base
docker export profiler_ls | sudo tar -x -C ubuntu_base

# Create new image
mkdir ubuntu_lean

# Get the linker (trust me)
sudo mkdir -p ubuntu_lean/lib64
sudo cp -a ubuntu_base/lib64/ld-linux-x86-64.so.2 ubuntu_lean/lib64/

# Copy the files
sudo mkdir -p ubuntu_lean/etc
sudo mkdir -p ubuntu_lean/bin
sudo mkdir -p ubuntu_lean/lib/x86_64-linux-gnu/

sudo cp -a ubuntu_base/bin/ls ubuntu_lean/bin/ls
sudo cp -a ubuntu_base/etc/group ubuntu_lean/etc/group
sudo cp -a ubuntu_base/etc/passwd ubuntu_lean/etc/passwd
sudo cp -a ubuntu_base/etc/ld.so.cache ubuntu_lean/etc/ld.so.cache
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/ld-2.19.so ubuntu_lean/lib/x86_64-linux-gnu/ld-2.19.so
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/ld-2.19.so ubuntu_lean/lib/x86_64-linux-gnu/ld-2.19.so
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/libselinux.so.1 ubuntu_lean/lib/x86_64-linux-gnu/libselinux.so.1
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/libacl.so.1.1.0 ubuntu_lean/lib/x86_64-linux-gnu/libacl.so.1.1.0
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/libc-2.19.so ubuntu_lean/lib/x86_64-linux-gnu/libc-2.19.so
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/libpcre.so.3.13.1 ubuntu_lean/lib/x86_64-linux-gnu/libpcre.so.3.13.1
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/libdl-2.19.so ubuntu_lean/lib/x86_64-linux-gnu/libdl-2.19.so
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/libattr.so.1.1.0 ubuntu_lean/lib/x86_64-linux-gnu/libattr.so.1.1.0

# Import it back to Docker
cd ubuntu_lean
sudo tar -c . | docker import - ubuntu_lean
&lt;/pre&gt;

&lt;p&gt;Run the resulting image:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;docker run --rm -it ubuntu_lean /bin/ls
&lt;/pre&gt;

&lt;p&gt;And, Tadaaaaa:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;# If you did not trust me with the linker (as it was already loaded when the profiler started, it does not show in the ouput)
no such file or directoryFATA[0000] Error response from daemon: Cannot start container f318adb174a9e381500431370a245275196a2948828919205524edc107626d78: no such file or directory

# Otherwise
/bin/ls: error while loading shared libraries: libacl.so.1: cannot open shared object file: No such file or directory
&lt;/pre&gt;

&lt;p&gt;Well, not so&amp;#8230; What went wrong? Remember when I said this syscall was primarily designed with antivirus in mind? The real-time part of the antivirus is supposed to detect that a file is being accessed, run some checks, take a decision. What matters here is the actual, real content of the file. In particular, filesystem races MUST be avoided at all costs. This is the reason why &lt;code&gt;fanotify&lt;/code&gt; yields filedescriptors instead of accesses path. Determining the underlying physical file is done by probing &lt;code&gt;/proc/self/fd/[fd]&lt;/code&gt;. It does not tell you through which symlink the file being accessed was accessed, only what file it is.&lt;/p&gt;

&lt;p&gt;To make this work, we need to find all links to reported files and install them in the filtered image as well. A &lt;code&gt;find&lt;/code&gt; command like this will do the job:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;# Find all files refering to a given one
find -L -samefile &#34;./lib/x86_64-linux-gnu/libacl.so.1.1.0&#34; 2&amp;gt;/dev/null

# If you want to exclude the target itself from the results
find -L -samefile &#34;./lib/x86_64-linux-gnu/libacl.so.1.1.0&#34; -a ! -path &#34;./lib/x86_64-linux-gnu/libacl.so.1.1.0&#34; 2&amp;gt;/dev/null
&lt;/pre&gt;

&lt;p&gt;This can easily be automated with a loop like:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;for f in $(cd ubuntu_lean; find)
do 
    (
        cd ubuntu_base
        find -L -samefile &#34;$f&#34; -a ! -path &#34;$f&#34;
    ) 2&amp;gt;/dev/null
done
&lt;/pre&gt;

&lt;p&gt;Which produces the list of missing symlinks. All libs.&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;./lib/x86_64-linux-gnu/libc.so.6
./lib/x86_64-linux-gnu/ld-linux-x86-64.so.2
./lib/x86_64-linux-gnu/libattr.so.1
./lib/x86_64-linux-gnu/libdl.so.2
./lib/x86_64-linux-gnu/libpcre.so.3
./lib/x86_64-linux-gnu/libacl.so.1
&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s copy them too from the source image and re-create the destination image. (Yeah, could also have created them on the fly).&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;# Copy the links
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/libc.so.6 ubuntu_lean/lib/x86_64-linux-gnu/libc.so.6
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2 ubuntu_lean/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/libdl.so.2 ubuntu_lean/lib/x86_64-linux-gnu/libdl.so.2
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/libpcre.so.3 ubuntu_lean/lib/x86_64-linux-gnu/libpcre.so.3
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/libacl.so.1 ubuntu_lean/lib/x86_64-linux-gnu/libacl.so.1
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/libattr.so.1 ubuntu_lean/lib/x86_64-linux-gnu/libattr.so.1

# Import it back to Docker
cd ubuntu_lean
docker rmi -f ubuntu_lean; sudo tar -c . | docker import - ubuntu_lean
&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: This method is limited. For example, it won&amp;rsquo;t return links to links to files neither absolute links. The later requiring at least a chroot. Or to be run in the source container itself, provided that find or equivalent is present.&lt;/p&gt;

&lt;p&gt;Run the resulting image:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;docker run --rm -it ubuntu_lean /bin/ls
&lt;/pre&gt;

&lt;p&gt;And, Tadaaaaa:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;bin  dev  etc  lib  lib64  proc  sys
&lt;/pre&gt;

&lt;p&gt;It works! &lt;sup&gt;tm&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Time is over, let&amp;rsquo;s measure:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;ubuntu&lt;/strong&gt;: 209M&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;ubuntu_lean&lt;/strong&gt;: 2,5M&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Resulting Docker image is 83.5 &lt;em&gt;times&lt;/em&gt; smaller&lt;sup&gt;3&lt;/sup&gt;. That&amp;rsquo;s a 98.8% reduction. Looks good to me, I&amp;rsquo;ll accept it. If you agree.&lt;/p&gt;

&lt;h3 id=&#34;last-thought&#34;&gt;Last Thought&lt;/h3&gt;

&lt;p&gt;Like all profiling based method, it will only tell you about what&amp;rsquo;s actually done/used in a specific scenario. For example, try to run &lt;code&gt;/bin/ls -l&lt;/code&gt; in the resulting image and see by yourself. (spoiler: it does not work. Well it does, but not as expected).&lt;/p&gt;

&lt;p&gt;The profiling technique itself is not without flaws. It does not detect how a file was opened but only which file this is. This is a problem for symlinks, especially cross-filesytems (read: cross-volumes). With fanotify, we&amp;rsquo;ll completely miss the original symlink and break the application.&lt;/p&gt;

&lt;p&gt;If I were to build a production shrinker, I would probably go for a &lt;code&gt;ptrace&lt;/code&gt; based method.&lt;/p&gt;

&lt;h3 id=&#34;footnotes&#34;&gt;Footnotes&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Let&amp;rsquo;s face the truth: What I really wanted, was experimenting with this syscall. Docker images are more of a (good) pretext.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Actually, one could use &lt;code&gt;FAN_UNLIMITED_QUEUE&lt;/code&gt; well calling &lt;code&gt;fanotify_init&lt;/code&gt; to remove this limitation, provided that the calling process is at least &lt;code&gt;CAP_SYS_ADMIN&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;That&amp;rsquo;s also 2.4 times smaller that the 6.13MB image I mentioned at the beginning of this post. But the comparison is not fair.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    
    <item>
      <title>How to run Docker behind an Nginx reverse proxy</title>
      <link>http://blog.yadutaf.fr/2014/12/12/how-to-run-docker-behind-an-nginx-reverse-proxy/</link>
      <pubDate>Fri, 12 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>http://blog.yadutaf.fr/2014/12/12/how-to-run-docker-behind-an-nginx-reverse-proxy/</guid>
      <description>&lt;p&gt;A couple of weeks ago, I wanted to run some experiment to see how Docker could run in a cloud / shared hosting like environment. In the mean time, Docker released version 1.4 bringing additional security/authentication and Docker machine to automate the process of creating and running a remote Docker instance.&lt;/p&gt;

&lt;p&gt;Shared hosting farms are usually built around some kind of public gateway for incoming/outgoing traffic as well as management traffic including FTP and SSH. Te largest part of the farm - not unlike an iceberg - being &amp;#8220;hidden&amp;#8221; in a private network behind these gateways.&lt;/p&gt;

&lt;p&gt;So, my question was, is there any way we can imagine that could enable a similar gateway behavior with Docker, including multi-tenancy support and all features you&amp;rsquo;d expect?&lt;/p&gt;

&lt;p&gt;It turns out, there is.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Docker binary can actually play up to 3 roles:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Docker Command line -&amp;gt; the one making it shiny and plain awesome&lt;/li&gt;
&lt;li&gt;Docker Daemon -&amp;gt; the one behind the scenes doing most of the hard work&lt;/li&gt;
&lt;li&gt;Docker init -&amp;gt; the one behind the one behind the scenes doing the early container setup&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The command line and and daemon talk together using a &lt;em&gt;&lt;strong&gt;mostly&lt;/strong&gt;&lt;/em&gt; HTTP based protocol. I say &amp;#8220;mostly&amp;#8221; because the a couple of API endpoints &amp;#8216;hijack&amp;rsquo; the connection, notably the &lt;code&gt;container/attach&lt;/code&gt; endpoint, also known as &amp;#8220;forward my container&amp;rsquo;s console.&amp;#8221;&lt;/p&gt;

&lt;p&gt;Knowing that, a common setup, already well covered by blog posts around the web, recommend to setup an &lt;code&gt;NGinx&lt;/code&gt; reverse proxy and add basic authentication for the security.&lt;/p&gt;

&lt;p&gt;Sadly, there are 2 downsides with this approach:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Stock Docker client does not &amp;#8220;speak&amp;#8221; HTTP basic authentication&lt;/li&gt;
&lt;li&gt;Stock Nginx is completely lost when Docker hijacks the connection&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Regarding the authentication issue, I recommend to rather rely on Docker TLS certificate as they are supported out of the box. Then, using some LUA magic, we could use them as &amp;#8220;public keys&amp;#8221; to balance to the appropriate. This would in itself a good subject for a dedicated post.&lt;/p&gt;

&lt;p&gt;How do we deal with the second point, namely, Nginx being lost?&lt;/p&gt;

&lt;p&gt;Once the mechanism behind the &amp;#8220;hijack&amp;#8221; is well identified, things quickly becomes straight forward: A usual HTTP connection could be seen as &amp;#8220;half-duplex&amp;#8221; network. One peer talks and, when it is done, the other peer can talk and so on, using a well known protocol. When doing a docker attach, Docker uses the raw TCP connection in &amp;#8220;full duplex&amp;#8221; mode, any peer can talk whenever they have something to say. This is why reverse proxies are lost: they expect - and rely - a lot on the HTTP protocol being well respected.&lt;/p&gt;

&lt;p&gt;Interestingly, there is another mainstream protocol doing just this. As it turns out, this standard protocol is so popular that it has been integrated in Nginx years ago. I named &lt;code&gt;WebSocket&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;So, basically, the idea is to teach Nginx how to handle Docker&amp;rsquo;s custom protocol just as it does with websockets. Here is the patch:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;--- a/src/http/ngx_http_upstream.c Tue Nov 04 19:56:23 2014 +0900
+++ b/src/http/ngx_http_upstream.c  Sat Nov 15 16:21:58 2014 +0100
@@ -89,6 +89,8 @@
     ngx_table_elt_t *h, ngx_uint_t offset);
 static ngx_int_t ngx_http_upstream_process_content_length(ngx_http_request_t *r,
     ngx_table_elt_t *h, ngx_uint_t offset);
+static ngx_int_t ngx_http_upstream_process_content_type(ngx_http_request_t *r,
+    ngx_table_elt_t *h, ngx_uint_t offset);
 static ngx_int_t ngx_http_upstream_process_last_modified(ngx_http_request_t *r,
     ngx_table_elt_t *h, ngx_uint_t offset);
 static ngx_int_t ngx_http_upstream_process_set_cookie(ngx_http_request_t *r,
@@ -175,7 +177,7 @@
                  ngx_http_upstream_copy_header_line, 0, 0 },

     { ngx_string(&#34;Content-Type&#34;),
-                 ngx_http_upstream_process_header_line,
+                 ngx_http_upstream_process_content_type,
                  offsetof(ngx_http_upstream_headers_in_t, content_type),
                  ngx_http_upstream_copy_content_type, 0, 1 },

@@ -2716,6 +2718,7 @@
     u-&amp;gt;write_event_handler = ngx_http_upstream_upgraded_write_upstream;
     r-&amp;gt;read_event_handler = ngx_http_upstream_upgraded_read_downstream;
     r-&amp;gt;write_event_handler = ngx_http_upstream_upgraded_write_downstream;
+    u-&amp;gt;headers_in.chunked = 0;

     if (clcf-&amp;gt;tcp_nodelay) {
         tcp_nodelay = 1;
@@ -3849,6 +3852,25 @@

 static ngx_int_t
+ngx_http_upstream_process_content_type(ngx_http_request_t *r, ngx_table_elt_t *h,
+    ngx_uint_t offset)
+{
+    ngx_int_t ret = ngx_http_upstream_process_header_line(r, h, offset);
+    if (ret != NGX_OK) {
+        return ret;
+    }
+
+    // is docker header ?
+    if (ngx_strstrn(h-&amp;gt;value.data,
+                    &#34;application/vnd.docker.raw-stream&#34;, 34 - 1) != NULL) {
+        r-&amp;gt;upstream-&amp;gt;upgrade = 1;
+    }
+
+    return NGX_OK;
+}
+
+
+static ngx_int_t
 ngx_http_upstream_process_last_modified(ngx_http_request_t *r,
     ngx_table_elt_t *h, ngx_uint_t offset)
 {
1

The only remaining step is then to configure the reverse proxy, as usual. This should be easy 😉

Just for the record, here is my test &amp;lt;code&amp;gt;nginx.conf&amp;lt;/code&amp;gt;:

1
worker_processes  1;

events {
    worker_connections  1024;
}

http {
    include       mime.types;
    default_type  application/octet-stream;

    sendfile        on;

    keepalive_timeout  65;

    server {
        listen 9000;

        location / {
            proxy_buffering off;
            proxy_pass http://localhost:8080;
        }
    }
}
&lt;/pre&gt;

&lt;p&gt;You just need to run Docker on port 8080 with a command like the following or just add your params to &lt;code&gt;/etc/default/docker&lt;/code&gt;&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;docker -d -H tcp://localhost:8080&lt;/pre&gt;

&lt;p&gt;And we&amp;rsquo;re done!&lt;/p&gt;

&lt;h3 id=&#34;final-thought&#34;&gt;Final thought&lt;/h3&gt;

&lt;p&gt;While hacking this, I noticed that all Nginx needs to switch protocols for websockets was proper HTTP Headers:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;# Request
Connection: Upgrade
Upgrade: websocket

# Response
HTTP/1.1 101 Upgraded
Connection: Upgrade
Upgrade: websocket
&lt;/pre&gt;

&lt;p&gt;So that another approach could be to inject proper headers in Docker protocol.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Getting Docker to run on Power8</title>
      <link>http://blog.yadutaf.fr/2014/10/28/getting-docker-to-run-on-power8/</link>
      <pubDate>Tue, 28 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>http://blog.yadutaf.fr/2014/10/28/getting-docker-to-run-on-power8/</guid>
      <description>&lt;p&gt;Last Week-End, I wanted to play around with Docker on a &lt;a href=&#34;http://en.wikipedia.org/wiki/POWER8&#34;&gt;Power8 processor&lt;/a&gt;. Unfortunately, there no &amp;#8220;ready-to-use&amp;#8221; build available (yet) and Go support is still quite rough. Anyway, I love challenges and the process was eased a lot by the work of &lt;a href=&#34;http://dave.cheney.net/&#34;&gt;Dave Cheney&lt;/a&gt; from Canonical who did the hard work of &lt;a href=&#34;http://go-talks.appspot.com/github.com/davecheney/gosyd/gccgo.slide#1&#34;&gt;porting the go command line to Power8&lt;/a&gt; and IBM&amp;rsquo;s who is working with Docker to bring necessary fixes to gccgo.&lt;/p&gt;

&lt;p&gt;[UPDATE 2014-11-19]: IBM is currently porting Docker to gccgo/Power8, see the comments below for more informations.&lt;/p&gt;

&lt;p&gt;Power8 is the name of a 64bits RISC processor micro-architecture of the same family as the G5 for example. This was the processor powering the venerable Mac G5. It is extremely parallel with up to 8 threads per core. This makes it especially good at running databases. Notably, &lt;a href=&#34;https://www.flamingspork.com/blog/2014/06/03/1-million-sql-queries-per-second-mysql-5-7-on-power8/&#34;&gt;Stewart Smith tuned MySQL 7 to get up to 1M request per seconds&lt;/a&gt;. This is just amazing!&lt;/p&gt;

&lt;p&gt;Docker is a tool helping developers to build, ship and run code anywhere just like containers helps shipping anything anywhere. It is increasingly used in production to cleanly isolate processes on a same physical machine without the overhead of a Virtual Machine.&lt;/p&gt;

&lt;p&gt;So, let&amp;rsquo;s get started. My goal was to get docker running and, if possible the latest version (it turns out it actually **is** the latest version). The goal was not to make it the shiniest way. That&amp;rsquo;s for later.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Here is the state of the art:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Docker depends on Go and cgo 1.2.1 until version 1.1.1&lt;/li&gt;
&lt;li&gt;Docker depends on Go and cgo 1.3+ after then&lt;/li&gt;
&lt;li&gt;gccgo 4.9, shipped with Ubuntu 14.04 supports go 1.2.1 but lacks some reflexivity implementation for Power8 and Elf parsing for Power8 in libcgo&lt;/li&gt;
&lt;li&gt;gccgo trunk supports go 1.4 (yes), fixes the reflexivity but still lacks the Elf parsing&lt;/li&gt;
&lt;li&gt;golang 1.3 has no support for Power8&lt;/li&gt;
&lt;li&gt;golang dev.power64 is still very work in progress but supports ELF parsing for Power8 (hint, hint)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As you can see, this is not &lt;span class=&#34;span9&#34;&gt;attempting to square the circle but not so close.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;It is also worth noting that gccgo is only the compiler parts. It brings no support for the &amp;#8220;go&amp;#8221; command line itself (which is written in pure go) neither for cgo (which bridges the gap between Go and C worlds). Fortunately, Dave Cheney, of Canonical, did the hard work of getting &amp;#8220;go&amp;#8221; to build with gccgo and in turn seamlessly work with gccgo backend by default. His work is now available through &amp;#8216;apt-get&amp;rsquo;. He also did a great presentation of his work which is available online &lt;a href=&#34;http://go-talks.appspot.com/github.com/davecheney/gosyd/gccgo.slide&#34;&gt;http://go-talks.appspot.com/github.com/davecheney/gosyd/gccgo.slide&lt;/a&gt;. And, honestly, after a full week-end battling to get it right, I totally share his opinions when he writes &amp;#8220;ʕ╯◔ϖ◔ʔ╯︵ ┻━┻&amp;#8221;.&lt;/p&gt;

&lt;p&gt;Among the discarded, aborted, failed attempts: cross compile from my laptop, find ready to use instructions, use stock gcc 4.9, build dev.power64 Go branch (it&amp;rsquo;s completely broken / Work in progress), fly a unicorn.&lt;/p&gt;

&lt;p&gt;Anyway, let&amp;rsquo;s start over. What we&amp;rsquo;ll do:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;get a Power8 machine. No cross build sorry.&lt;/li&gt;
&lt;li&gt;grab latest version of GCC from trunk (SVN, that&amp;rsquo;s 1 VCS)&lt;/li&gt;
&lt;li&gt;grab latest WIP version of Power8 from dev.power64 (Mercurial, that&amp;rsquo;s a 2nd VCS)&lt;/li&gt;
&lt;li&gt;copy required bits from go to gccgo, namely the ELF parser of libcgo&lt;/li&gt;
&lt;li&gt;patch, build and install gccgo in /opt/gcc-trunk&lt;/li&gt;
&lt;li&gt;build &amp;#8220;go&amp;#8221; and &amp;#8220;cgo&amp;#8221; commands to use our updated libgo.so.6 instead of libgo.so.5&lt;/li&gt;
&lt;li&gt;grab lastest version of Docker from master (Git, that&amp;rsquo;s a 3rd VCS)&lt;/li&gt;
&lt;li&gt;patch, build, install Docker&lt;/li&gt;
&lt;li&gt;celebrate&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;1-get-a-power8-machine&#34;&gt;1. Get a Power8 Machine&lt;/h3&gt;

&lt;p&gt;The easiest way to get one is to &lt;a href=&#34;http://labs.runabove.com/power8/&#34;&gt;join RunAbove&amp;rsquo;s public beta&lt;/a&gt; which comes with a $32 Voucher. That&amp;rsquo;s one month worth of Power8.&lt;/p&gt;

&lt;p&gt;Common setup:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;sudo locale-gen
sudo apt-get -y update
sudo apt-get -y install subversion mercurial git build-essential gccgo-go
&lt;/pre&gt;

&lt;h3 id=&#34;2-grab-gcc&#34;&gt;2. Grab GCC&lt;/h3&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;cd
svn checkout svn://gcc.gnu.org/svn/gcc/trunk gcc
# Be *very* patient
&lt;/pre&gt;

&lt;h3 id=&#34;3-grab-go-dev-power64&#34;&gt;3. Grab Go dev.power64&lt;/h3&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;cd
hg clone -u release https://code.google.com/p/go
cd go
hg update dev.power64
&lt;/pre&gt;

&lt;h3 id=&#34;4-patch-gcc&#34;&gt;4. Patch GCC&lt;/h3&gt;

&lt;p&gt;GCC&amp;rsquo;s libcgo implementation lakes elf parsing supporting for PPC64 instruction set. As this is required by &lt;code&gt;cgo&lt;/code&gt;, we&amp;rsquo;ll get it from Go itself.&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;cd
cp go/src/debug/elf/file.go gcc/libgo/go/debug/elf/
cp go/src/debug/elf/elf.go gcc/libgo/go/debug/elf/
&lt;/pre&gt;

&lt;p&gt;It also lacks some termios related symbols required to build docker command line interface. They&amp;rsquo;re easily added with this patch (extracted from `svn diff`):&lt;/p&gt;

&lt;p&gt;[UPDATE 2014-11-11]: This patch is no longer needed thanks to IBM&amp;rsquo;s upstream work.&lt;/p&gt;

&lt;pre class=&#34;brush: diff; title: ; notranslate&#34; title=&#34;&#34;&gt;--- libgo/mksysinfo.sh  (revision 216693)
+++ libgo/mksysinfo.sh  (working copy)
@@ -174,6 +174,15 @@
 #ifdef TIOCGWINSZ
   TIOCGWINSZ_val = TIOCGWINSZ,
 #endif
+#ifdef TIOCSWINSZ
+  TIOCSWINSZ_val = TIOCSWINSZ,
+#endif
+#ifdef TCGETS
+  TCGETS_val = TCGETS,
+#endif
+#ifdef TCSETS
+  TCSETS_val = TCSETS,
+#endif
 #ifdef TIOCNOTTY
   TIOCNOTTY_val = TIOCNOTTY,
 #endif
@@ -790,6 +799,21 @@
     echo &#39;const TIOCGWINSZ = _TIOCGWINSZ_val&#39; &amp;gt;&amp;gt; ${OUT}
   fi
 fi
+if ! grep &#39;^const TIOCSWINSZ&#39; ${OUT} &amp;gt;/dev/null 2&amp;gt;&amp;1; then
+  if grep &#39;^const _TIOCSWINSZ_val&#39; ${OUT} &amp;gt;/dev/null 2&amp;gt;&amp;1; then
+    echo &#39;const TIOCSWINSZ = _TIOCSWINSZ_val&#39; &amp;gt;&amp;gt; ${OUT}
+  fi
+fi
+if ! grep &#39;^const TCGETS&#39; ${OUT} &amp;gt;/dev/null 2&amp;gt;&amp;1; then
+  if grep &#39;^const _TCGETS_val&#39; ${OUT} &amp;gt;/dev/null 2&amp;gt;&amp;1; then
+    echo &#39;const TCGETS = _TCGETS_val&#39; &amp;gt;&amp;gt; ${OUT}
+  fi
+fi
+if ! grep &#39;^const TCSETS&#39; ${OUT} &amp;gt;/dev/null 2&amp;gt;&amp;1; then
+  if grep &#39;^const _TCSETS_val&#39; ${OUT} &amp;gt;/dev/null 2&amp;gt;&amp;1; then
+    echo &#39;const TCSETS = _TCSETS_val&#39; &amp;gt;&amp;gt; ${OUT}
+  fi
+fi
 if ! grep &#39;^const TIOCNOTTY&#39; ${OUT} &amp;gt;/dev/null 2&amp;gt;&amp;1; then
   if grep &#39;^const _TIOCNOTTY_val&#39; ${OUT} &amp;gt;/dev/null 2&amp;gt;&amp;1; then
     echo &#39;const TIOCNOTTY = _TIOCNOTTY_val&#39; &amp;gt;&amp;gt; ${OUT}
&lt;/pre&gt;

&lt;p&gt;If you&amp;rsquo;re planning on making a break, just wait one more minute. We&amp;rsquo;ll launch GCC&amp;rsquo;s build&amp;#8230;&lt;/p&gt;

&lt;h3 id=&#34;5-build-gcc&#34;&gt;5. Build GCC&lt;/h3&gt;

&lt;p&gt;As usual, except that we built it out of tree.&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;cd
mkdir build-gcc
cd build-gcc
sudo apt-get install -y libgmp-dev libmpfr-dev libmpc-dev flex bison
../gcc/configure --enable-languages=go --disable-multilib --prefix=/opt/gcc-trunk
make -j200 # if using the big instance
sudo make install
&lt;/pre&gt;

&lt;p&gt;Be patient, read a book, watch a movie, go visit friends&amp;#8230; It takes a while. On the &amp;#8216;S&amp;rsquo; instance, it took me around 98 minutes.&lt;/p&gt;

&lt;p&gt;Once done, we have some additional setup:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;export PATH=/opt/gcc-trunk/bin:$PATH
echo &#34;/opt/gcc-trunk/lib64&#34; | sudo tee /etc/ld.so.conf.d/gcc-trunk.conf
sudo ldconfig
&lt;/pre&gt;

&lt;h3 id=&#34;6-build-and-install-cgo&#34;&gt;6. Build (and install) CGO&lt;/h3&gt;

&lt;p&gt;Cgo is the component bridging the gap between Go and C world. It is notably required to build the devmapper driver of Docker.&lt;/p&gt;

&lt;p&gt;As we won&amp;rsquo;t attempt to build the full go toolchain (it does&amp;rsquo;nt work yet), we&amp;rsquo;ll need to patch &amp;#8220;gcc.go&amp;#8220; to insert `const defaultCC = &amp;#8220;gcc&amp;#8221;` near the top of the file.&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;cd go/src/cmd/cgo
go build
&lt;/pre&gt;

&lt;p&gt;You can now install it. It&amp;rsquo;s hackish but it does the job. But I still can&amp;rsquo;t figure out why I needed to copy the source files to `/usr/src/cmd/cgo`. Anyway, it&amp;rsquo;s working.&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;sudo mkdir -p /usr/pkg/tool/linux_ppc64
sudo mkdir -p /usr/src/cmd/cgo
sudo cp cgo /usr/pkg/tool/linux_ppc64/cgo
sudo cp * /usr/src/cmd/cgo
&lt;/pre&gt;

&lt;p&gt;One more thing: to let `go build` know we prepared to using cgo, we need to switch `CGO_ENABLED` environment variable on.&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;export CGO_ENABLED=1
&lt;/pre&gt;

&lt;h3 id=&#34;7-grab-docker-1-3-0&#34;&gt;7. Grab Docker 1.3.0&lt;/h3&gt;

&lt;p&gt;This is the last stable release at the time of writing. Let&amp;rsquo;s use it.&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;cd
git clone https://github.com/docker/docker.git
cd docker
git checkout v1.3.1
&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;ll also need to prepare a little the build environment:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;sudo mkdir -p /go/src/github.com/docker/
sudo ln -s $HOME/docker /go/src/github.com/docker/docker
export PATH=/opt/gcc-trunk/bin/:$PATH
export GOPATH=/go:/go/src/github.com/docker/docker/vendor
&lt;/pre&gt;

&lt;h3 id=&#34;8-build-docker&#34;&gt;8. Build Docker&lt;/h3&gt;

&lt;p&gt;Just issue &amp;#8216;docker build&amp;rsquo;. I&amp;rsquo;m kidding.&lt;/p&gt;

&lt;p&gt;This is the trickiest part of the job as all the full build systems assumes a working docker environment. So we&amp;rsquo;ll mostly emulate it.&lt;/p&gt;

&lt;p&gt;First, let&amp;rsquo;s apply a couple of patches.&lt;/p&gt;

&lt;p&gt;Remove a runtime (?!) check preventing Docker to run on non amd64 platforms:&lt;/p&gt;

&lt;pre class=&#34;brush: diff; title: ; notranslate&#34; title=&#34;&#34;&gt;diff --git a/daemon/daemon.go b/daemon/daemon.go
index 235788c..b75a94e 100644
--- a/daemon/daemon.go
+++ b/daemon/daemon.go
@@ -1104,9 +1104,9 @@ func (daemon *Daemon) ImageGetCached(imgID string, config *runconfig.Config) (*i
 
 func checkKernelAndArch() error {
    // Check for unsupported architectures
-   if runtime.GOARCH != &#34;amd64&#34; {
-       return fmt.Errorf(&#34;The Docker runtime currently only supports amd64 (not %s). This will change in the future. Aborting.&#34;, runtime.GOARCH)
-   }
+   //if runtime.GOARCH != &#34;amd64&#34; {
+   //  return fmt.Errorf(&#34;The Docker runtime currently only supports amd64 (not %s). This will change in the future. Aborting.&#34;, runtime.GOARCH)
+   //}
    // Check for unsupported kernel versions
    // FIXME: it would be cleaner to not test for specific versions, but rather
    // test for specific functionalities.
&lt;/pre&gt;

&lt;p&gt;Next, we need to workaround hard-coded references to official go compiler:&lt;/p&gt;

&lt;pre class=&#34;brush: diff; title: ; notranslate&#34; title=&#34;&#34;&gt;diff --git a/vendor/src/github.com/kr/pty/pty_linux.go b/vendor/src/github.com/kr/pty/pty_linux.go
index 6e5a042..8525f80 100644
--- a/vendor/src/github.com/kr/pty/pty_linux.go
+++ b/vendor/src/github.com/kr/pty/pty_linux.go
@@ -7,6 +7,11 @@ import (
    &#34;unsafe&#34;
 )
 
+type (
+        _C_int  int32
+        _C_uint uint32
+)
+
 var (
    ioctl_TIOCGPTN   = _IOR(&#39;T&#39;, 0x30, unsafe.Sizeof(_C_uint(0))) /* Get Pty Number (of pty-mux device) */
    ioctl_TIOCSPTLCK = _IOW(&#39;T&#39;, 0x31, unsafe.Sizeof(_C_int(0)))  /* Lock/unlock Pty */
&lt;/pre&gt;

&lt;p&gt;And, finally, change the link flags. Note that for some reason `-static` breaks network communication. It seems to be related to name resolution but I did not investigate further as dynamic linking works just fine.&lt;/p&gt;

&lt;pre class=&#34;brush: diff; title: ; notranslate&#34; title=&#34;&#34;&gt;diff --git a/hack/make/binary b/hack/make/binary
index b97069a..f5398ae 100755
--- a/hack/make/binary
+++ b/hack/make/binary
@@ -6,9 +6,8 @@ DEST=$1
 go build \
    -o &#34;$DEST/docker-$VERSION&#34; \
    &#34;${BUILDFLAGS[@]}&#34; \
-   -ldflags &#34;
-       $LDFLAGS
-       $LDFLAGS_STATIC_DOCKER
+   -gccgoflags &#34;
+       -static-libgo -static-libgcc
    &#34; \
    ./docker
 echo &#34;Created binary: $DEST/docker-$VERSION&#34;
&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s start to build. Most of the following steps are normally handled by the Dockerfile but&amp;#8230; we don&amp;rsquo;t have a working Docker yet.&lt;/p&gt;

&lt;p&gt;Grab the dependencies:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;sudo apt-get install -y \
        aufs-tools \
        automake \
        btrfs-tools \
        build-essential \
        curl \
        dpkg-sig \
        git \
        iptables \
        libapparmor-dev \
        libcap-dev \
        libsqlite3-dev \
        lxc=1.0* \
        mercurial \
        parallel \
        reprepro \
        ruby1.9.1 \
        ruby1.9.1-dev \
        s3cmd=1.1.0* \
        --no-install-recommends
&lt;/pre&gt;

&lt;p&gt;Docker needs a pretty recent devmapper build to run. Get it.&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;cd
git clone --no-checkout https://git.fedorahosted.org/git/lvm2.git
cd lvm2
git checkout -q v2_02_103
&lt;/pre&gt;

&lt;p&gt;Now we&amp;rsquo;ll hit an outdated file `config.guess`, overload it.&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;mkdir -p autoconf
wget &#39;http://git.savannah.gnu.org/gitweb/?p=config.git;a=blob_plain;f=config.guess;hb=HEAD&#39; -O autoconf/config.guess
&lt;/pre&gt;

&lt;p&gt;Build it:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;./configure --enable-static_link
make device-mapper
sudo make install_device-mapper
&lt;/pre&gt;

&lt;p&gt;Make sure you have the the ldconfig, PATH and CGO_ENABLED tricks then:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;cd
cd docker
./hack/make.sh binary
sudo cp /home/admin/docker/bundles/1.3.1/binary/docker-1.3.1 /usr/bin/docker
&lt;/pre&gt;

&lt;p&gt;And we&amp;rsquo;re done !&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Using Docker to triage Nasty-Bugs(tm)</title>
      <link>http://blog.yadutaf.fr/2014/09/27/using-docker-to-triage-nasty-bugs/</link>
      <pubDate>Sat, 27 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://blog.yadutaf.fr/2014/09/27/using-docker-to-triage-nasty-bugs/</guid>
      <description>&lt;p&gt;Docker is the container system for reproducible builds. This is precisely what you want when dealing with bugs, especially the nastiest one: an environment where to reproduce it in a fully deterministic way.&lt;/p&gt;

&lt;p&gt;Not long ago, I had to troubleshoot the install process of a new cool piece of software. The weird and really uncool thing with this bug is that it only occurred on the first install install attempt. Even with a full (well, in theory) wipe, there still remained some kind of side effect on the system causing the subsequents install attempts to succeed. Anyone who has ever dealt with Q/A will know what I mean when I say this is pretty damn frustrating. (1)&lt;/p&gt;

&lt;p&gt;Traditional approach: use a smart combination of script and snapshots.&lt;/p&gt;

&lt;p&gt;Wait, isn&amp;rsquo;t it exactly what Docker those ? Sure it is !&lt;/p&gt;

&lt;p&gt;Even better than that: Docker saves one snapshot for each step. This is awesome&lt;/p&gt;

&lt;p&gt;when iterating.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s build a &lt;code&gt;Dockerfile&lt;/code&gt; for a Python project (Whoops, did I just name the perpetrator?):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;dockerfile&#34;&gt;# start from clean, minimalist system
FROM debian:stable

# step 1: make it less minimalist
RUN apt-get update &amp;&amp; apt-get install -y git vim python-pip

# step 2: grab code from GIT repo + switch to dev branch
RUN mkdir -p /usr/src &amp;&amp; git clone http://some-server/my-project /usr/src/my-project --branch fix-nastybugtm

# step 3: change workdir so it spares me one &#39;cd&#39; one each attempt
WORKDIR /usr/src/my-project
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As recommended by &lt;a href=&#34;https://docs.docker.com/articles/dockerfile_best-practices/&#34;&gt;Docker&amp;rsquo;s best practices&lt;/a&gt;, each logical step is grouped on its own dedicated line so that we keep the number of intermediate snapshots reasonable.&lt;/p&gt;

&lt;p&gt;Speaking of snapshots, let&amp;rsquo;s build our lab environment:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;docker build -t my-project-lab .
&lt;/pre&gt;

&lt;p&gt;And work on it!&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;docker run -t -i –rm my-project-lab /bin/bash
&lt;/pre&gt;

&lt;p&gt;This is where all the magic happens. We tell Docker to fire our &lt;code&gt;my-project-lab&lt;/code&gt; env from a clean copy in interactive mode (&lt;code&gt;-i&lt;/code&gt;) and do not attempt to retain data for later use, we won&amp;rsquo;t need it (&lt;code&gt;--rm&lt;/code&gt;). As we&amp;rsquo;re interactive, we&amp;rsquo;ll need a shell. I use &lt;code&gt;/bin/bash&lt;/code&gt; but given recent security context, I may want to be a better hipster and user &lt;code&gt;/bin/zsh&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;See how easy it is to industrialize bug fighting with Docker!&lt;/p&gt;

&lt;p&gt;Any time you&amp;rsquo;ve come closer to you bug, feel free to update your &lt;code&gt;Dockerfile&lt;/code&gt; and rebuild the image. That&amp;rsquo;s one less step to do manually.&lt;/p&gt;

&lt;p&gt;(1) actually, it was even more fun: the bug only occurred when installing from&lt;/p&gt;

&lt;p&gt;release website. Installing from GIT was always successful.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to seccomp: BPF linux syscall filter</title>
      <link>http://blog.yadutaf.fr/2014/05/29/introduction-to-seccomp-bpf-linux-syscall-filter/</link>
      <pubDate>Thu, 29 May 2014 00:00:00 +0000</pubDate>
      
      <guid>http://blog.yadutaf.fr/2014/05/29/introduction-to-seccomp-bpf-linux-syscall-filter/</guid>
      <description>&lt;p&gt;Seccomp is basic yet efficient way to filter syscalls issued by a program. It is especially useful when running untrusted third party programs. Actually, it was first &lt;a href=&#34;http://git.kernel.org/cgit/linux/kernel/git/tglx/history.git/commit/?id=d949d0ec9c601f2b148bed3cdb5f87c052968554&#34; title=&#34;Initial seccomp commit&#34;&gt;introduced in linux 2.6.12&lt;/a&gt; as an essential building block of &lt;a href=&#34;http://mashable.com/2005/12/21/cpushare-distributed-computing-marketplace/&#34;&gt;&amp;#8220;cpushare&amp;#8221; program&lt;/a&gt;. The idea behind this project was to allow anyone with the proper agent installed to rent cpu cycles to third parties, without compromising its the security.&lt;/p&gt;

&lt;p&gt;The initial implementation, also known as &amp;#8220;mode 1 seccomp&amp;#8221; only allowed &amp;#8216;&lt;code&gt;read&lt;/code&gt;&amp;#8216;, &amp;#8216;&lt;code&gt;write&lt;/code&gt;&amp;#8216;, &amp;#8216;&lt;code&gt;_exit&lt;/code&gt;&amp;#8216; and &amp;#8216;&lt;code&gt;sigreturn&lt;/code&gt;&amp;#8216; syscalls to be issued making it only possible to read/write to already opened files and to exit. It is also trivial get started with:&lt;/p&gt;

&lt;pre class=&#34;brush: cpp; highlight: [2,3,10]; title: 01-nothing.c; notranslate&#34; title=&#34;01-nothing.c&#34;&gt;#include &amp;lt;stdio.h&amp;gt;         /* printf */
#include &amp;lt;sys/prctl.h&amp;gt;     /* prctl */
#include &amp;lt;linux/seccomp.h&amp;gt; /* seccomp&#39;s constants */
#include &amp;lt;unistd.h&amp;gt;        /* dup2: just for test */

int main() {
  printf(&#34;step 1: unrestricted\n&#34;);

  // Enable filtering
  prctl(PR_SET_SECCOMP, SECCOMP_MODE_STRICT);
  printf(&#34;step 2: only &#39;read&#39;, &#39;write&#39;, &#39;_exit&#39; and &#39;sigreturn&#39; syscalls\n&#34;);
  
  // Redirect stderr to stdout
  dup2(1, 2);
  printf(&#34;step 3: !! YOU SHOULD NOT SEE ME !!\n&#34;);

  // Success (well, not so in this case...)
  return 0; 
}
&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Build, run, test:&lt;/strong&gt;&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;gcc 01-nothing.c -o 01-nothing &amp;&amp; ./01-nothing; echo &#34;Status: $?&#34;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt;&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;step 1: unrestricted
step 2: only &#39;read&#39;, &#39;write&#39;, &#39;_exit&#39; and &#39;sigreturn&#39; syscalls
Processus arrêté
Status: 137        &amp;lt;------ 128+9 ==&amp;gt; SIGKILL
&lt;/pre&gt;

&lt;p&gt;See the return status ? Whenever a forbidden syscall is issued, the program is immediately killed.&lt;/p&gt;

&lt;p&gt;While this is really cool, this is also somewhat over-restrictive. This is the reason why it saw such a little adoption. Linus Torvald even suggested to ax it out of the kernel!&lt;/p&gt;

&lt;p&gt;Fortunately, since linux 3.5, it is also possible to define advanced custom filters based on the BPF (Berkley Packet Filters). These filters may apply on any of the syscall argument but only on their value. In other words, a filter won&amp;rsquo;t be able to dereference a pointer. For example one could write a rule to forbid any call to &amp;#8216;&lt;code&gt;dup2&lt;/code&gt;&amp;#8216; as long as it targets &amp;#8216;&lt;code&gt;stderr&lt;/code&gt;&amp;#8216; (fd=2) but would not be able to restrict &amp;#8216;&lt;code&gt;open&lt;/code&gt;&amp;#8216; to a given set of files neither bind to a specific interface or port number.&lt;/p&gt;

&lt;p&gt;Once installed, each syscall is sent to the filter which tells what action to take:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;SECCOMP_RET_KILL&lt;/code&gt;: Immediate kill with SIGSYS&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SECCOMP_RET_TRAP&lt;/code&gt;: Send a catchable SIGSYS, giving a chance to emulate the syscall&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SECCOMP_RET_ERRNO&lt;/code&gt;: Force &lt;code&gt;errno&lt;/code&gt; value&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SECCOMP_RET_TRACE&lt;/code&gt;: Yield decision to ptracer or set &lt;code&gt;errno&lt;/code&gt; to &lt;code&gt;-ENOSYS&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SECCOMP_RET_ALLOW&lt;/code&gt;: Allow&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Enough words. Let&amp;rsquo;s allow the program to redirect its &lt;code&gt;stderr&lt;/code&gt; to &lt;code&gt;stdout&lt;/code&gt; but nothing else. Writing BPF directly is cumbersome and far beyond the scope of this post, we&amp;rsquo;ll use the &lt;code&gt;libseccomp&lt;/code&gt; helper to make the code easier to write&amp;#8230; and read. Error checking stripped for brevity.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Grab the library:&lt;/strong&gt;&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;sudo apt-get install libseccomp-dev&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Write the code:&lt;/strong&gt;&lt;/p&gt;

&lt;pre class=&#34;brush: cpp; title: 02-bpf-only-dup-sudo.c; notranslate&#34; title=&#34;02-bpf-only-dup-sudo.c&#34;&gt;#include &amp;lt;stdio.h&amp;gt;   /* printf */
#include &amp;lt;unistd.h&amp;gt;  /* dup2: just for test */
#include &amp;lt;seccomp.h&amp;gt; /* libseccomp */

int main() {
  printf(&#34;step 1: unrestricted\n&#34;);

  // Init the filter
  scmp_filter_ctx ctx;
  ctx = seccomp_init(SCMP_ACT_KILL); // default action: kill

  // setup basic whitelist
  seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(rt_sigreturn), 0);
  seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(exit), 0);
  seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(read), 0);
  seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(write), 0);
  
  // setup our rule
  seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(dup2), 2, 
                        SCMP_A0(SCMP_CMP_EQ, 1),
                        SCMP_A1(SCMP_CMP_EQ, 2));

  // build and load the filter
  seccomp_load(ctx);
  printf(&#34;step 2: only &#39;write&#39; and dup2(1, 2) syscalls\n&#34;);
  
  // Redirect stderr to stdout
  dup2(1, 2);
  printf(&#34;step 3: stderr redirected to stdout\n&#34;);

  // Duplicate stderr to arbitrary fd
  dup2(2, 42);
  printf(&#34;step 4: !! YOU SHOULD NOT SEE ME !!\n&#34;);

  // Success (well, not so in this case...)
  return 0; 
}
&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Build, run, test:&lt;/strong&gt;&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;gcc 02-bpf-only-dup-sudo.c -o 02-bpf-only-dup-sudo -lseccomp &amp;&amp; sudo ./02-bpf-only-dup-sudo; echo &#34;Status: $?&#34;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt;&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;step 1: unrestricted
step 2: only &#39;write&#39; and dup2(1, 2) syscalls
step 3: stderr redirected to stdout
Appel système erroné
Status: 159        &amp;lt;------ 128+31 ==&amp;gt; SIGSYS
&lt;/pre&gt;

&lt;p&gt;Just as expected.&lt;/p&gt;

&lt;p&gt;As you probably noticed, we ran the previous example as root which somewhat limits the security benefice of syscall filtering as we actually have MORE privileges than before&amp;#8230;&lt;/p&gt;

&lt;p&gt;This is where it really gets interesting: filters are inherited by child processes so that one could technically apply syscall filters to &amp;#8216;sudo&amp;rsquo; and maybe defeat some of its security measures and gain root on the machine ? To prevent this, one must either be &amp;#8216;&lt;code&gt;CAP_SYS_ADMIN&lt;/code&gt;&amp;#8216; (read: root), either explicitly accept to never get any more privileges. For example the &amp;#8216;&lt;code&gt;setuid&lt;/code&gt;&amp;#8216; bit of &amp;#8216;&lt;code&gt;sudo&lt;/code&gt;&amp;#8216; would not be honored.&lt;/p&gt;

&lt;p&gt;This can easily be achieved by adding this snippet &lt;em&gt;before&lt;/em&gt; installing the filter:&lt;/p&gt;

&lt;pre class=&#34;brush: cpp; title: ; notranslate&#34; title=&#34;&#34;&gt;prctl(PR_SET_NO_NEW_PRIVS, 1);&lt;/pre&gt;

&lt;p&gt;Another security note, remember the &lt;code&gt;SECCOMP_RET_TRACE&lt;/code&gt; filter return value ? It instructs the kernel to notify the ptracer program, if any, to take the final decision. Hence the &amp;#8220;secured&amp;#8221; program could be run under a malicious ptracer possibly defeating the security measures. This is why another &lt;code&gt;prctl&lt;/code&gt; is highly recommended to forbid any attempt to attach a ptracer:&lt;/p&gt;

&lt;pre class=&#34;brush: cpp; title: ; notranslate&#34; title=&#34;&#34;&gt;prctl(PR_SET_DUMPABLE, 0);&lt;/pre&gt;

&lt;p&gt;Putting it all together we get:&lt;/p&gt;

&lt;pre class=&#34;brush: cpp; title: 03-bpf-only-dup.c; notranslate&#34; title=&#34;03-bpf-only-dup.c&#34;&gt;#include &amp;lt;stdio.h&amp;gt;     /* printf */
#include &amp;lt;unistd.h&amp;gt;    /* dup2: just for test */
#include &amp;lt;seccomp.h&amp;gt;   /* libseccomp */
#include &amp;lt;sys/prctl.h&amp;gt; /* prctl */

int main() {
  printf(&#34;step 1: unrestricted\n&#34;);

  // ensure none of our children will ever be granted more priv
  // (via setuid, capabilities, ...)
  prctl(PR_SET_NO_NEW_PRIVS, 1);
  // ensure no escape is possible via ptrace
  prctl(PR_SET_DUMPABLE, 0);

  // Init the filter
  scmp_filter_ctx ctx;
  ctx = seccomp_init(SCMP_ACT_KILL); // default action: kill

  // setup basic whitelist
  seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(rt_sigreturn), 0);
  seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(exit), 0);
  seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(read), 0);
  seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(write), 0);
  
  // setup our rule
  seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(dup2), 2, 
                        SCMP_A0(SCMP_CMP_EQ, 1),
                        SCMP_A1(SCMP_CMP_EQ, 2));

  // build and load the filter
  seccomp_load(ctx);
  printf(&#34;step 2: only &#39;write&#39; and dup2(1, 2) syscalls\n&#34;);
  
  // Redirect stderr to stdout
  dup2(1, 2);
  printf(&#34;step 3: stderr redirected to stdout\n&#34;);

  // Duplicate stderr to arbitrary fd
  dup2(2, 42);
  printf(&#34;step 4: !! YOU SHOULD NOT SEE ME !!\n&#34;);

  // Success (well, not so in this case...)
  return 0;
}
&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Build, run, test:&lt;/strong&gt;&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;gcc 03-bpf-only-dup.c -o 03-bpf-only-dup -lseccomp &amp;&amp; ./03-bpf-only-dup; echo &#34;Status: $?&#34;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt;&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;step 1: unrestricted
step 2: only &#39;write&#39; and dup2(1, 2) syscalls
step 3: stderr redirected to stdout
Appel système erroné
Status: 159        &amp;lt;------ 128+31 ==&amp;gt; SIGSYS
&lt;/pre&gt;

&lt;p&gt;There we are: no more &amp;#8220;sudo&amp;#8221; to run it &lt;img src=&#34;https://blog.jtlebi.fr/wp-includes/images/smilies/simple-smile.png&#34; alt=&#34;:)&#34; class=&#34;wp-smiley&#34; style=&#34;height: 1em; max-height: 1em;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Linux&amp;rsquo;s seccomp is an extremely powerful tool when dealing with untrusted program&amp;rsquo;s on Linux. (who said in &amp;#8220;shared hosting environment&amp;#8221;?). And we only scratched its surface. Please, keep in mind that seccomp is only a tool and should be used in combination with other Linux&amp;rsquo;s security building blocks such as &lt;a href=&#34;https://blog.jtlebi.fr/2013/12/22/introduction-to-linux-namespaces-part-1-uts/&#34; title=&#34;Introduction to Linux namespaces – Part 1: UTS&#34;&gt;namespaces&lt;/a&gt; and capabilities to unleash its full power.&lt;/p&gt;

&lt;p&gt;Example applications:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;prevent &amp;#8220;virtual priv esc&amp;#8221; -&amp;gt; clone &amp;amp;&amp;amp; unshare CLONE_NEW_USER&lt;/li&gt;
&lt;li&gt;prevent std{in,out,err} escape -&amp;gt; block &lt;code&gt;close&lt;/code&gt;, &lt;code&gt;dup2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;restrict read/write to std{in,out,err}&lt;/li&gt;
&lt;li&gt;change limits (rlimits)&lt;/li&gt;
&lt;li&gt;&amp;#8230; -&amp;gt; see man 2 syscalls for more ideas 😉&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What you still can&amp;rsquo;t do:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;filter base on filename: no pointer dereference&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;filter base on port/ip: same reason&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Going further:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://sourceforge.net/p/libseccomp/libseccomp/ci/master/tree/tests/&#34;&gt;libseccomp tests&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kernel seccomp &lt;a href=&#34;https://www.kernel.org/doc/Documentation/prctl/seccomp_filter.txt&#34;&gt;documentation&lt;/a&gt; and &lt;a href=&#34;https://github.com/torvalds/linux/tree/master/samples/seccomp&#34;&gt;samples&lt;/a&gt; (low level BPF)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ptrace interaction: overcome the &amp;#8220;What you still can&amp;rsquo;t do&amp;#8221; section.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Linux namespaces – Part 5: NET</title>
      <link>http://blog.yadutaf.fr/2014/01/19/introduction-to-linux-namespaces-part-5-net/</link>
      <pubDate>Sun, 19 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>http://blog.yadutaf.fr/2014/01/19/introduction-to-linux-namespaces-part-5-net/</guid>
      <description>&lt;p&gt;Following the &lt;a href=&#34;https://blog.jtlebi.fr/2014/01/12/introduction-to-linux-namespaces-part-4-ns-fs/&#34; title=&#34;Introduction to Linux namespaces – Part 4: PID&#34;&gt;previous post on PID namespace&lt;/a&gt; (Restart process numbering to &amp;#8220;1&amp;#8221;), would like to go further and fly eve closer to full-featured VMs ? Great ! The two last posts of this series will precisely focus on this. Isolate network interfaces with the &amp;#8220;NET&amp;#8221; namespace (Yes, really) and user/group identifier for even more transparency. If you haven&amp;rsquo;t done so already, I encourage you to read &lt;a href=&#34;https://blog.jtlebi.fr/2013/12/22/introduction-to-linux-namespaces-part-1-uts/&#34; title=&#34;Introduction to Linux namespaces – Part 1: UTS&#34;&gt;the first post of this series for an introduction to linux namespace isolation mechanism&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[EDIT 2014-01-08] A Chinese translation of this post is available &lt;a href=&#34;http://blog.lucode.net/linux/intro-Linux-namespace-5.html&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For once we won&amp;rsquo;t start with the addition of the &amp;#8220;CLONE_NEWNET&amp;#8221; flag to the &amp;#8220;clone&amp;#8221; syscall. I keep it for later. For now, IMHO, the best way to get started with this namespace is the incredibly mighty &amp;#8220;&lt;a href=&#34;http://www.linuxfoundation.org/collaborate/workgroups/networking/iproute2&#34; title=&#34;IPRoute2 official website&#34;&gt;iproute2&lt;/a&gt;&amp;#8221; net-admin swiss army knife. If you don&amp;rsquo;t have it (yet) I highly encourage you to install it. Nonetheless, if don&amp;rsquo;t want to / can&amp;rsquo;t, you may as well skip the explanation part and go straight to the full code sample.&lt;/p&gt;

&lt;p&gt;First, let&amp;rsquo;s see what network interfaces we have at the moment:&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;ip link list&lt;/pre&gt;

&lt;p&gt;Which outputs something like:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0: &amp;lt;NO-CARRIER,BROADCAST,MULTICAST,UP&amp;gt; mtu 1500 qdisc pfifo_fast state DOWN mode DEFAULT qlen 1000
    link/ether **:**:**:**:**:** brd ff:ff:ff:ff:ff:ff
3: wlan0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc mq state UP mode DORMANT qlen 1000
    link/ether **:**:**:**:**:** brd ff:ff:ff:ff:ff:ff
# ...
&lt;/pre&gt;

&lt;p&gt;Nothing unexpected here. I have a working loopback, UP (Yeah, &amp;#8216;UNKNOWN&amp;rsquo; means &amp;#8216;UP&amp;rsquo;&amp;#8230;) and am connected to my wireless network + a couple of extra connections eclipsed for this article.&lt;/p&gt;

&lt;p&gt;Now, let&amp;rsquo;s create a network namespace and run the same from inside:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;# create a network namespace called &#34;demo&#34;
ip netns add demo
# exec &#34;ip link list&#34; inside the namespace
ip netns exec demo ip link list
&lt;/pre&gt;

&lt;p&gt;Output is now:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;1: lo: &amp;lt;LOOPBACK&amp;gt; mtu 65536 qdisc noop state DOWN mode DEFAULT
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
&lt;/pre&gt;

&lt;p&gt;Huuu, not only is there only a loopback but also it is &amp;#8220;DOWN&amp;#8221;. Even more interesting, it is fully isolated from the main loopback. That is to say, any application inside the namespace binding on &amp;#8220;the&amp;#8221; loopback would only be able to communicate with applications inside the same namespace. Exactly the same level of isolation as with the IPC namespace. Neat, isnt&amp;rsquo;t ?&lt;/p&gt;

&lt;p&gt;Right, but how do I communicate with the interwebz now ?&lt;/p&gt;

&lt;p&gt;There are multiple solutions. The easiest and most common one is to create a Point-to-Point tunnel between your &amp;#8220;Host&amp;#8221; and &amp;#8220;Guest&amp;#8221; system. Once, again, the Linux Kernel provides multiple alternatives. I recommend to use the &amp;#8220;veth&amp;#8221; interfaces as these are the best integrated in the ecosystem especially with iproute2. This is also an extremely well tested piece of code as it is used by LXC and actually comes from the &lt;a href=&#34;http://openvz.org&#34; title=&#34;OpenVZ offical website&#34;&gt;OpenVZ project&lt;/a&gt;. Another alternative could be the &amp;#8220;etun&amp;#8221; driver. It conceptually is the same with another name but I&amp;rsquo;m not aware of any project using it.&lt;/p&gt;

&lt;p&gt;Both &amp;#8220;veth&amp;#8221; and &amp;#8220;etun&amp;#8221; create a pair of virtual interfaces linked on with the other in the current namespace. You can then pick one and move it in the target namespace to get a communication channel. You could think of it as intricate particles if it makes it easier to understand ;).&lt;/p&gt;

&lt;p&gt;The next step is to give them an IP, set them up and ping ! Here is an example bash session doing just that:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;# Create a &#34;demo&#34; namespace
ip netns add demo

# create a &#34;veth&#34; pair
ip link add veth0 type veth peer name veth1

# and move one to the namespace
ip link set veth1 netns demo

# configure the interfaces (up + IP)
ip netns exec demo ip link set lo up
ip netns exec demo ip link set veth1 up
ip netns exec demo ip addr add 169.254.1.2/30 dev veth1
ip link set veth0 up
ip addr add 169.254.1.1/30 dev veth0
&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s it ! Nothing scary.&lt;/p&gt;

&lt;p&gt;If you need to get Internet access from the &amp;#8220;guest&amp;#8221; system using the &amp;#8220;veth&amp;#8221; technique, you could setup masquerding, commonly known as &amp;#8220;NAT&amp;#8221;. In the same way, to make a webserver listening on the :80 inside the namespace appear to listen directly on the main interface, one could use &amp;#8220;DNAT&amp;#8221; commonly known as port &amp;#8220;forwarding&amp;#8221;. I&amp;rsquo;ll leave this up to the reader.&lt;/p&gt;

&lt;p&gt;Here is a basic example to quickly get started:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;# make sure ip forwarding is enabled
echo 1 &amp;gt; /proc/sys/net/ipv4/ip_forward
# enable Internet access for the namespace, assuming you ran the previous example
iptables -t nat -A POSTROUTING -i veth0 -j  MASQUERADE
# Forward main &#34;:80&#34; to guest &#34;:80&#34;
iptables -t nat -A PREROUTING -d &amp;lt;your main ip&amp;gt;/32 -p tcp --dport 80 -j  DNAT --to-destination  169.254.1.2:80
&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s put it all together and finally append the &lt;code&gt;CLONE_NEWNET&lt;/code&gt; flag to the &lt;code&gt;clone&lt;/code&gt; syscall. For the sake of simplicity we&amp;rsquo;ll simply stick with direct calls to &amp;#8220;ip&amp;#8221; using the &lt;code&gt;system()&lt;/code&gt; syscall.&lt;/p&gt;

&lt;pre class=&#34;brush: cpp; highlight: [9,40,41,42,57,60,61,62,63,64,65,66]; title: main-5-net.c; notranslate&#34; title=&#34;main-5-net.c&#34;&gt;#define _GNU_SOURCE
#include &amp;lt;sys/types.h&amp;gt;
#include &amp;lt;sys/wait.h&amp;gt;
#include &amp;lt;sys/mount.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;sched.h&amp;gt;
#include &amp;lt;signal.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;

#define STACK_SIZE (1024 * 1024)

// sync primitive
int checkpoint[2];

static char child_stack[STACK_SIZE];
char* const child_args[] = {
  &#34;/bin/bash&#34;,
  NULL
};

int child_main(void* arg)
{
  char c;

  // init sync primitive
  close(checkpoint[1]);

  // setup hostname
  printf(&#34; - [%5d] World !\n&#34;, getpid());
  sethostname(&#34;In Namespace&#34;, 12);

  // remount &#34;/proc&#34; to get accurate &#34;top&#34; &amp;&amp; &#34;ps&#34; output
  mount(&#34;proc&#34;, &#34;/proc&#34;, &#34;proc&#34;, 0, NULL);

  // wait for network setup in parent
  read(checkpoint[0], &amp;c, 1);

  // setup network
  system(&#34;ip link set lo up&#34;);
  system(&#34;ip link set veth1 up&#34;);
  system(&#34;ip addr add 169.254.1.2/30 dev veth1&#34;);

  execv(child_args[0], child_args);
  printf(&#34;Ooops\n&#34;);
  return 1;
}

int main()
{
  // init sync primitive
  pipe(checkpoint);

  printf(&#34; - [%5d] Hello ?\n&#34;, getpid());

  int child_pid = clone(child_main, child_stack+STACK_SIZE,
      CLONE_NEWUTS | CLONE_NEWIPC | CLONE_NEWPID | CLONE_NEWNS | CLONE_NEWNET | SIGCHLD, NULL);

  // further init: create a veth pair
  char* cmd;
  asprintf(&amp;cmd, &#34;ip link set veth1 netns %d&#34;, child_pid);
  system(&#34;ip link add veth0 type veth peer name veth1&#34;);
  system(cmd);
  system(&#34;ip link set veth0 up&#34;);
  system(&#34;ip addr add 169.254.1.1/30 dev veth0&#34;);
  free(cmd);

  // signal &#34;done&#34;
  close(checkpoint[1]);

  waitpid(child_pid, NULL, 0);
  return 0;
}
&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s give it a test run !&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;jean-tiare@jeantiare-Ubuntu:~/blog$ gcc -Wall main.c -o ns &amp;&amp; sudo ./ns
 - [22094] Hello ?
 - [    1] World !
root@In Namespace:~/blog$ # run a super-powerful server, fully isolated
root@In Namespace:~/blog$ nc -l 4242
Hi !
Bye...
root@In Namespace:~/blog$ exit
jean-tiare@jeantiare-Ubuntu:~/blog$ # done !
&lt;/pre&gt;

&lt;p&gt;This is what you would have seen if, from another terminal, you had:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;jean-tiare@jeantiare-Ubuntu:~$ nc 169.254.1.2 4242
Hi !
Bye...
jean-tiare@jeantiare-Ubuntu:~$
&lt;/pre&gt;

&lt;p&gt;To go further on the path to network virtualization, you could have a look at new interfaces types recently introduced in the Linux kernel: macvlan, vlan, vxlans, &amp;#8230;&lt;/p&gt;

&lt;p&gt;If you feel that running a bunch of &lt;code&gt;system()&lt;/code&gt; calls into a production system is a dirty hack (and it is !), you could have look at the &lt;code&gt;rtnetlink&lt;/code&gt; kernel communication interface. This is the barely documented API used by iproute under the hood.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s all for &amp;#8220;NET&amp;#8221; namespace. It&amp;rsquo;s so powerful that it&amp;rsquo;s used as the foundation of the &lt;a href=&#34;http://cs.itd.nrl.navy.mil/work/core/index.php&#34;&gt;&amp;#8220;CORE&amp;#8221; lightweight network simulator&lt;/a&gt;. With the next article we&amp;rsquo;ll explore the last and most tricky namespace &amp;#8220;USER&amp;#8221;. Thanks for reading !&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Introduction to Linux namespaces - Part 4: NS (FS)</title>
      <link>http://blog.yadutaf.fr/2014/01/12/introduction-to-linux-namespaces-part-4-ns-fs/</link>
      <pubDate>Sun, 12 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>http://blog.yadutaf.fr/2014/01/12/introduction-to-linux-namespaces-part-4-ns-fs/</guid>
      <description>&lt;p&gt;Following the &lt;a href=&#34;https://blog.jtlebi.fr/2014/01/05/introduction-to-linux-namespaces-part-3-pid/&#34; title=&#34;Introduction to Linux namespaces – Part 4: NS (FS)&#34;&gt;previous post on FS namespace&lt;/a&gt; (mountpoints table isolation), we will now have a look at an amazing one: isolated mount table. If you haven&amp;rsquo;t done so already, I encourage you to read &lt;a href=&#34;https://blog.jtlebi.fr/2013/12/22/introduction-to-linux-namespaces-part-1-uts/&#34; title=&#34;Introduction to Linux namespaces – Part 1: UTS&#34;&gt;the first post of this series for an introduction to linux namespace isolation mechanism&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[EDIT 2014-01-08] A Chinese translation of this post is available &lt;a href=&#34;http://blog.lucode.net/linux/intro-Linux-namespace-4.html&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In the previous post we &amp;#8220;chrooted&amp;#8221; the PID namespace and got a new &amp;#8220;1&amp;#8221; process. But even with this namespace activated, there still lacked isolation for tools like &amp;#8220;top&amp;#8221; because they rely on the &amp;#8220;/proc&amp;#8221; virtual filesystem which is still shared (identical) between namespaces. In this post, let me introduce the namespace that will solve this: &amp;#8220;NS&amp;#8221;. This is historically the first Linux Namespace, hence the name.&lt;/p&gt;

&lt;p&gt;Activating it is only a matter of adding &amp;#8220;CLONE_NEWNS&amp;#8221; to the &amp;#8220;clone&amp;#8221; call. It requires no additional setup. It may also be freely combined with other namespaces.&lt;/p&gt;

&lt;p&gt;Once activated, any (un)mount operations from the child will only affect the child and vice-versa.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s start experimenting. In the previous example, just activate the NS:&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;pre class=&#34;brush: cpp; first-line: 43; title: activate-ns-snippet.c; notranslate&#34; title=&#34;activate-ns-snippet.c&#34;&gt;int child_pid = clone(child_main, child_stack+STACK_SIZE, 
      CLONE_NEWUTS | CLONE_NEWIPC | CLONE_NEWPID | CLONE_NEWNS | SIGCHLD, NULL);
&lt;/pre&gt;

&lt;p&gt;Now, if we run it, we finally can fix the issue from the previous post on PID:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; highlight: [4,7,8]; title: ; notranslate&#34; title=&#34;&#34;&gt;jean-tiare@jeantiare-Ubuntu:~/blog$ gcc -Wall ns.c -o ns &amp;&amp; sudo ./ns
 - [14472] Hello ?
 - [    1] World !
root@In Namespace:~/blog# mount -t proc proc /proc
root@In Namespace:~/blog# ps aux
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  1.0  0.0  23620  4680 pts/4    S    00:07   0:00 /bin/bash
root        79  0.0  0.0  18492  1328 pts/4    R+   00:07   0:00 ps aux
root@In Namespace:~/blog# exit
&lt;/pre&gt;

&lt;p&gt;Tadaaa ! &amp;#8220;/proc&amp;#8221; is now working as expected from the container, without breaking the parent.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s automate it to finalize previous post&amp;rsquo;s example:&lt;/p&gt;

&lt;pre class=&#34;brush: cpp; highlight: [4,33,51]; title: main-4-ns.c; notranslate&#34; title=&#34;main-4-ns.c&#34;&gt;#define _GNU_SOURCE
#include &amp;lt;sys/types.h&amp;gt;
#include &amp;lt;sys/wait.h&amp;gt;
#include &amp;lt;sys/mount.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;sched.h&amp;gt;
#include &amp;lt;signal.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;

#define STACK_SIZE (1024 * 1024)

// sync primitive
int checkpoint[2];

static char child_stack[STACK_SIZE];
char* const child_args[] = {
  &#34;/bin/bash&#34;,
  NULL
};

int child_main(void* arg)
{
  char c;

  // init sync primitive
  close(checkpoint[1]);

  // setup hostname
  printf(&#34; - [%5d] World !\n&#34;, getpid());
  sethostname(&#34;In Namespace&#34;, 12);

  // remount &#34;/proc&#34; to get accurate &#34;top&#34; &amp;&amp; &#34;ps&#34; output
  mount(&#34;proc&#34;, &#34;/proc&#34;, &#34;proc&#34;, 0, NULL);

  // wait...
  read(checkpoint[0], &amp;c, 1);

  execv(child_args[0], child_args);
  printf(&#34;Ooops\n&#34;);
  return 1;
}

int main()
{
  // init sync primitive
  pipe(checkpoint);

  printf(&#34; - [%5d] Hello ?\n&#34;, getpid());

  int child_pid = clone(child_main, child_stack+STACK_SIZE,
      CLONE_NEWUTS | CLONE_NEWIPC | CLONE_NEWPID | CLONE_NEWNS | SIGCHLD, NULL);

  // further init here (nothing yet)

  // signal &#34;done&#34;
  close(checkpoint[1]);

  waitpid(child_pid, NULL, 0);
  return 0;
}
&lt;/pre&gt;

&lt;p&gt;If you run this snippet, you should get exactly the same behavior as the previous test without manually remounting &amp;#8220;/proc&amp;#8221; neither messing with your real parent&amp;rsquo;s &amp;#8220;/proc&amp;#8221;. Neat isn&amp;rsquo;t it ?&lt;/p&gt;

&lt;p&gt;To leverage the power of this technique you could now prepare and enter a chroot to further enhance the isolation. Steps involved would be to prepare a &amp;#8220;debootstrap&amp;#8221;, remount some essentials filesystems like &amp;#8220;/tmp&amp;#8221;, &amp;#8220;/dev/shm&amp;#8221;, &amp;#8220;/proc&amp;#8221;, optionally all or part of &amp;#8220;/dev&amp;#8221; and &amp;#8220;/sys&amp;#8221; and then &amp;#8220;&lt;a href=&#34;http://linux.die.net/man/2/chdir&#34; title=&#34;man chdir&#34;&gt;chdir&lt;/a&gt;&amp;#8221; + &amp;#8220;&lt;a href=&#34;http://linux.die.net/man/1/chroot&#34; title=&#34;man Chroot&#34;&gt;chroot&lt;/a&gt;&amp;#8220;. I&amp;rsquo;ll leave it as an exercise for the reader.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s all for &amp;#8220;NS&amp;#8221; namespace. With the next article we&amp;rsquo;ll explore an incredibly powerful namespace &amp;#8220;NET&amp;#8221;. It&amp;rsquo;s so powerful that it&amp;rsquo;s used as the foundation of the &lt;a href=&#34;http://cs.itd.nrl.navy.mil/work/core/index.php&#34;&gt;&amp;#8220;CORE&amp;#8221; lightweight network simulator&lt;/a&gt;. Thanks for reading !&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Introduction to Linux namespaces - Part 3: PID</title>
      <link>http://blog.yadutaf.fr/2014/01/05/introduction-to-linux-namespaces-part-3-pid/</link>
      <pubDate>Sun, 05 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>http://blog.yadutaf.fr/2014/01/05/introduction-to-linux-namespaces-part-3-pid/</guid>
      <description>&lt;p&gt;Following the &lt;a href=&#34;https://blog.jtlebi.fr/2013/12/28/introduction-to-linux-namespaces-part-2-ipc/&#34; title=&#34;Introduction to Linux namespaces – Part 2: IPC&#34;&gt;previous post on IPC namespace&lt;/a&gt; (Inter Process Communication isolation), I would now like to introduce my personal favorite one (as sysadmin): PID namespaces. If you haven&amp;rsquo;t done so already, I encourage you to read &lt;a href=&#34;https://blog.jtlebi.fr/2013/12/22/introduction-to-linux-namespaces-part-1-uts/&#34; title=&#34;Introduction to Linux namespaces – Part 1: UTS&#34;&gt;the first post of this series for an introduction to linux namespace isolation mechanism&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[EDIT 2014-01-08] A Chinese translation of this post is available &lt;a href=&#34;http://blog.lucode.net/linux/intro-Linux-namespace-3.html&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Yes, that&amp;rsquo;s it, with this namespace it is possible to restart PID numbering and get your own &amp;#8220;1&amp;#8221; process. This could be seen as a &amp;#8220;chroot&amp;#8221; in the process identifier tree. It&amp;rsquo;s extremely handy when you need to deal with pids in day to day work and are stuck with 4 digits numbers&amp;#8230;&lt;/p&gt;

&lt;p&gt;Activating it is only a matter of adding &amp;#8220;CLONE_NEWPID&amp;#8221; to the &amp;#8220;clone&amp;#8221; call. It requires no additional setup. It may also be freely combined with other namespaces.&lt;/p&gt;

&lt;p&gt;Once activated, the result of getpid() from child process will invariably be &amp;#8220;1&amp;#8221;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;But, WAIT! I know have to &amp;#8220;1&amp;#8221; process right ? What about process management ?&lt;/p&gt;

&lt;p&gt;Well, actually, this *really* is much like a &amp;#8220;chroot&amp;#8221;. That is to say, a change of view point.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Host: &lt;em&gt;all&lt;/em&gt; processes are visible, &lt;em&gt;global&lt;/em&gt; PIDs (init=1, &amp;#8230;, child=xxx, &amp;#8230;.)&lt;/li&gt;
&lt;li&gt;Container: &lt;em&gt;only child + descendant&lt;/em&gt; are visible, local PIDs (child=1, &amp;#8230;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is an illustration:&lt;/p&gt;

&lt;pre class=&#34;brush: cpp; highlight: [29,41,44]; title: ; notranslate&#34; title=&#34;&#34;&gt;#define _GNU_SOURCE
#include &amp;lt;sys/types.h&amp;gt;
#include &amp;lt;sys/wait.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;sched.h&amp;gt;
#include &amp;lt;signal.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;

#define STACK_SIZE (1024 * 1024)

// sync primitive
int checkpoint[2];

static char child_stack[STACK_SIZE];
char* const child_args[] = {
  &#34;/bin/bash&#34;,
  NULL
};

int child_main(void* arg)
{
  char c;

  // init sync primitive
  close(checkpoint[1]);
  // wait...
  read(checkpoint[0], &amp;c, 1);

  printf(&#34; - [%5d] World !\n&#34;, getpid());
  sethostname(&#34;In Namespace&#34;, 12);
  execv(child_args[0], child_args);
  printf(&#34;Ooops\n&#34;);
  return 1;
}

int main()
{
  // init sync primitive
  pipe(checkpoint);

  printf(&#34; - [%5d] Hello ?\n&#34;, getpid());

  int child_pid = clone(child_main, child_stack+STACK_SIZE,
      CLONE_NEWUTS | CLONE_NEWIPC | CLONE_NEWPID | SIGCHLD, NULL);

  // further init here (nothing yet)

  // signal &#34;done&#34;
  close(checkpoint[1]);

  waitpid(child_pid, NULL, 0);
  return 0;
}
&lt;/pre&gt;

&lt;p&gt;And an example run:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;jean-tiare@jeantiare-Ubuntu:~/blog$ gcc -Wall main-3-pid.c -o ns &amp;&amp; sudo ./ns
 - [ 7823] Hello ?
 - [    1] World !
root@In Namespace:~/blog# echo &#34;=&amp;gt; My PID: $$&#34;
=&amp;gt; My PID: 1
root@In Namespace:~/blog# exit
&lt;/pre&gt;

&lt;p&gt;As expected, even thought the parent process as a PID of &amp;#8220;7823&amp;#8221;, the child&amp;rsquo;s PID is &amp;#8220;1&amp;#8221;. If you are playfull, you could try to &amp;#8220;kill -KILL 7823&amp;#8221; the parent process. It would do exactly&amp;#8230; nothing:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;jean-tiare@jeantiare-Ubuntu:~/blog$ gcc -Wall main-3-pid.c -o ns &amp;&amp; sudo ./ns
 - [ 7823] Hello ?
 - [    1] World !
root@In Namespace:~/blog# kill -KILL 7823
bash: kill: (7823) - No such process
root@In Namespace:~/blog# exit
&lt;/pre&gt;

&lt;p&gt;The isolation is working as expected. And, as written earlier, this behaves much like a &amp;#8220;chroot&amp;#8221; meaning that with a &amp;#8220;top&amp;#8221; or &amp;#8220;ps exf&amp;#8221; from the parent process will show the child process with its real un-mapped PID. This is an essential feature for process control like &amp;#8220;kill&amp;#8221;, &amp;#8220;cgroups&amp;#8221;, &amp;#8230; and various policies.&lt;/p&gt;

&lt;p&gt;Wait! Speaking of &amp;#8220;top&amp;#8221; and &amp;#8220;ps exf&amp;#8221;, I just ran them from the child and saw exactly the same as from the parent. You lied to me about isolation !&lt;/p&gt;

&lt;p&gt;Well, not at all. This is because these tools get their informations from the virtual &amp;#8220;/proc&amp;#8221; filesystem which is not (yet) isolated. This is the purpose of the next article.&lt;/p&gt;

&lt;p&gt;In the mean time, an easy workaround could be:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; highlight: [3,5]; title: ; notranslate&#34; title=&#34;&#34;&gt;# from child
root@In Namespace:~/blog# mkdir -p proc
root@In Namespace:~/blog# mount -t proc proc proc
root@In Namespace:~/blog# ls proc
1          dma          key-users      net            sysvipc
80         dri          kmsg           pagetypeinfo   timer_list
acpi       driver       kpagecount     partitions     timer_stats
asound     execdomains  kpageflags     sched_debug    tty
buddyinfo  fb           latency_stats  schedstat      uptime
bus        filesystems  loadavg        scsi           version
cgroups    fs           locks          self           version_signature
cmdline    interrupts   mdstat         slabinfo       vmallocinfo
consoles   iomem        meminfo        softirqs       vmstat
cpuinfo    ioports      misc           stat           zoneinfo
crypto     irq          modules        swaps
devices    kallsyms     mounts         sys
diskstats  kcore        mtrr           sysrq-trigger
&lt;/pre&gt;

&lt;p&gt;Everything seems reasonable again. As expected, you get PID &amp;#8220;1&amp;#8221; for /bin/bash itself and &amp;#8220;80&amp;#8221; corresponds to the running &amp;#8220;/bin/ls proc&amp;#8221; command. Much nicer to read than usual /proc, isn&amp;rsquo;t it ? That&amp;rsquo;s why I love it.&lt;/p&gt;

&lt;p&gt;If you attempt to run this command directly on the &amp;#8220;/proc&amp;#8221; from the namespace, it will &lt;em&gt;seem&lt;/em&gt; to work in the child but BREAK your main namespace. Example:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;jean-tiare@jeantiare-Ubuntu:~/blog$ ps aux
Error, do this: mount -t proc proc /proc
&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s all for PID namespace. With the next article, we&amp;rsquo;ll be able to re-mount /proc itself and hence fix &amp;#8220;top&amp;#8221; and any similar tools without breaking the parent namespace. Thanks for reading !&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Introduction to Linux namespaces - Part 2: IPC</title>
      <link>http://blog.yadutaf.fr/2013/12/28/introduction-to-linux-namespaces-part-2-ipc/</link>
      <pubDate>Sat, 28 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>http://blog.yadutaf.fr/2013/12/28/introduction-to-linux-namespaces-part-2-ipc/</guid>
      <description>&lt;p&gt;Following the &lt;a href=&#34;https://blog.jtlebi.fr/2013/12/22/introduction-to-linux-namespaces-part-1-uts/&#34; title=&#34;Introduction to Linux namespaces – Part 1: UTS&#34;&gt;previous post on UTS namespace&lt;/a&gt; (hostname isolation), we will now go deeper and look at a more security oriented namespace: IPC, Inter-Process Communications. If you haven&amp;rsquo;t done so already, I encourage you to read &lt;a href=&#34;https://blog.jtlebi.fr/2013/12/22/introduction-to-linux-namespaces-part-1-uts/&#34; title=&#34;Introduction to Linux namespaces – Part 1: UTS&#34;&gt;the first post of this series for an introduction to linux namespace isolation mechanism&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[EDIT 2014-01-08] A Chinese translation of this post is available &lt;a href=&#34;http://blog.lucode.net/linux/intro-Linux-namespace-2.html&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Activating the IPC namespace is only a matter of adding &amp;#8220;CLONE_NEWIPC&amp;#8221; to the &amp;#8220;clone&amp;#8221; call. It requires no additional setup. It may also be freely combined with other namespaces.&lt;/p&gt;

&lt;p&gt;Once activated, you are free to create any IPC as usual, even named one, without any risk of collision with other applications.&lt;/p&gt;

&lt;p&gt;But, WAIT! My &amp;#8220;parent process&amp;#8221; is now isolated from my &amp;#8220;child process&amp;#8221; right ? What if I need to do some kind of communication between them ?&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s a good question. A common use case for this is you need some additional setup from the parent before letting the child take full control. Fortunately, not everything is isolated and clone shares memory space with its parent so that you can still use:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;signal&lt;/li&gt;
&lt;li&gt;poll memory&lt;/li&gt;
&lt;li&gt;sockets&lt;/li&gt;
&lt;li&gt;use files and file-descriptors&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Because of it&amp;rsquo;s context changes, signaling is probably not the most practical one while polling memory is damn inefficient way of communicating !&lt;/p&gt;

&lt;p&gt;If you don&amp;rsquo;t plan to fully isolate the network stack, you could go with sockets. Same remark applies with filesystem. But, in the case of this series this is precisely what we intend to do: isolate everything, step by step.&lt;/p&gt;

&lt;p&gt;A little known / rarely used solution is to watch events on a pipe pair. In fact this is the technique used (with no explanation) by Lennart Poettering in &lt;a href=&#34;http://cgit.freedesktop.org/systemd/systemd/tree/src/nspawn/nspawn.c&#34; title=&#34;systemd nspawn - git&#34;&gt;Systemd&amp;rsquo;s &amp;#8220;nspawn&amp;#8221;&lt;/a&gt; command. This is an extremely powerful technique that I would like to introduce here. This is also the one we will rely upon in the next articles.&lt;/p&gt;

&lt;p&gt;We first need to init a pair of pipes. Let&amp;rsquo;s call them a &amp;#8220;checkpoint&amp;#8221;.&lt;/p&gt;

&lt;pre class=&#34;brush: cpp; title: checkpoint-global-init.c; notranslate&#34; title=&#34;checkpoint-global-init.c&#34;&gt;// required headers: 
#include &amp;lt;unistd.h&amp;gt;

// global status:
int checkpoint[2];

// [parent] init:
pipe(checkpoint);
&lt;/pre&gt;

&lt;p&gt;The idea is to trigger a &amp;#8220;close&amp;#8221; event from the parent and wait for &amp;#8220;EOF&amp;#8221; to be received on the reading end, in the child. Something crucial to understand is that *all* writing file-descriptors must be closed for an EOF to be received. Hence, the first thing to do before waiting in the child is to close our own write fd copy.&lt;/p&gt;

&lt;pre class=&#34;brush: cpp; title: checkpoint-child-init.c; notranslate&#34; title=&#34;checkpoint-child-init.c&#34;&gt;// required headers: 
#include &amp;lt;unistd.h&amp;gt;

// [child] init:
close(checkpoint[1]);
&lt;/pre&gt;

&lt;p&gt;Actual &amp;#8220;signaling&amp;#8221; is now straightforward:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;close write fd in parent&lt;/li&gt;
&lt;li&gt;wait for EOF from child&lt;/li&gt;
&lt;/ol&gt;

&lt;pre class=&#34;brush: cpp; title: checkpoint-signal.c; notranslate&#34; title=&#34;checkpoint-signal.c&#34;&gt;// required headers: 
#include &amp;lt;unistd.h&amp;gt;

// [child] wait:
char c; // stub char
read(checkpoint[0], &amp;c, 1);

// [parent] signal ready code:
close(checkpoint[1]);
&lt;/pre&gt;

&lt;p&gt;If we put it together the first example on UTS namespace, it could look like:&lt;/p&gt;

&lt;pre class=&#34;brush: cpp; highlight: [7,12,25,27,39,49]; title: main-2-ipc.c; notranslate&#34; title=&#34;main-2-ipc.c&#34;&gt;#define _GNU_SOURCE
#include &amp;lt;sys/types.h&amp;gt;
#include &amp;lt;sys/wait.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;sched.h&amp;gt;
#include &amp;lt;signal.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;

#define STACK_SIZE (1024 * 1024)

// sync primitive
int checkpoint[2];

static char child_stack[STACK_SIZE];
char* const child_args[] = {
  &#34;/bin/bash&#34;,
  NULL
};

int child_main(void* arg)
{
  char c;

  // init sync primitive
  close(checkpoint[1]);
  // wait...
  read(checkpoint[0], &amp;c, 1);

  printf(&#34; - World !\n&#34;);
  sethostname(&#34;In Namespace&#34;, 12);
  execv(child_args[0], child_args);
  printf(&#34;Ooops\n&#34;);
  return 1;
}

int main()
{
  // init sync primitive
  pipe(checkpoint);

  printf(&#34; - Hello ?\n&#34;);

  int child_pid = clone(child_main, child_stack+STACK_SIZE,
      CLONE_NEWUTS | CLONE_NEWIPC | SIGCHLD, NULL);

  // some damn long init job
  sleep(4);
  // signal &#34;done&#34;
  close(checkpoint[1]);

  waitpid(child_pid, NULL, 0);
  return 0;
}
&lt;/pre&gt;

&lt;p&gt;As this requires advanced capabilities, this snippets needs root or equivalent privileges to run. Obviously, there is no need to keep &amp;#8220;CLONE_NEWUTS&amp;#8221; in this example. I kept it only to show that multiple namespaces may be used together.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s all for IPC. IPC in itself is nothing complicated. It just becomes tricky when it comes to parent/child synchronization as we will do later. This is where the &amp;#8220;pipe&amp;#8221; technique comes as a handy solution. It actually works and is used in production.&lt;/p&gt;

&lt;p&gt;The next article will be on my favorite one (as sysadmin): PID namespaces.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>