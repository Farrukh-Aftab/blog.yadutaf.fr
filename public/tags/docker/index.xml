<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Docker on Yet another enthusiast blog!</title>
    <link>http://blog.yadutaf.fr/tags/docker/</link>
    <description>Recent content in Docker on Yet another enthusiast blog!</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 10 Sep 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://blog.yadutaf.fr/tags/docker/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Running a graphical app in a Docker container, on a remote server</title>
      <link>http://blog.yadutaf.fr/2017/09/10/running-a-graphical-app-in-a-docker-container-on-a-remote-server/</link>
      <pubDate>Sun, 10 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>http://blog.yadutaf.fr/2017/09/10/running-a-graphical-app-in-a-docker-container-on-a-remote-server/</guid>
      <description>

&lt;p&gt;A few days ago, I found myself trying to launch Qemu, the system emulator, into a dedicated &lt;a href=&#34;http://blog.yadutaf.fr/2014/01/19/introduction-to-linux-namespaces-part-5-net/&#34;&gt;network namespace&lt;/a&gt; (that&amp;rsquo;s easy), on a remote host (that&amp;rsquo;s the fun part). As it was fun, I looked for a way to do it. Spoiler alert, it involves &amp;ldquo;xauth&amp;rdquo; tricks, bind mounts and TCP to UNIX domain socket forwarding. No more.&lt;/p&gt;

&lt;p&gt;My first though was that it&amp;rsquo;s probably not worth a blog post. Who uses network namespaces on a daily basis? Not everyone (I do).&lt;/p&gt;

&lt;p&gt;Except that network namespaces are one of the containers corner stone. And quite a lot of people is using Docker on a daily basis, possibly on a remote server, possibly with graphical applications.&lt;/p&gt;

&lt;p&gt;Hmmm, that&amp;rsquo;s another story. Let&amp;rsquo;s write a new container torture post then.&lt;/p&gt;

&lt;p&gt;At the end of this post, you will have a working setup to run graphical applications in a Docker container on a remote server accessed via SSH with X11 forwarding with not particular setup on the container side.&lt;/p&gt;

&lt;h3 id=&#34;running-a-graphical-app-via-ssh:6dc1c5ae07315872c30f93a6902bf01e&#34;&gt;Running a graphical app via SSH&lt;/h3&gt;

&lt;p&gt;Running a graphical application on a remote (Linux) server via SSH is considered a solved problem (at least until we run Wayland on servers&amp;hellip;). Here is a quick walk through anyway.&lt;/p&gt;

&lt;p&gt;By default, if you try to run an X11 based graphical application via SSH, it will not display anything and instead print a rather cryptic error message:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh localhost -- xterm
xterm: Xt error: Can&#39;t open display: 
xterm: DISPLAY is not set
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is easily fixed by explicitly enabling X11 forwarding, if the server policy allows it, by adding the &amp;ldquo;-X&amp;rdquo; argument:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ssh -X localhost -- xterm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should now see a beautiful xterm window. Granted, it&amp;rsquo;s not the most impressive. Who cares?&lt;/p&gt;

&lt;h3 id=&#34;running-a-graphical-app-inside-a-docker:6dc1c5ae07315872c30f93a6902bf01e&#34;&gt;Running a graphical app inside a Docker&lt;/h3&gt;

&lt;p&gt;Running a graphical application in a Docker container, is also considered a solved problem, mostly thanks to &lt;a href=&#34;https://blog.jessfraz.com/post/docker-containers-on-the-desktop/&#34;&gt;Jessie Frazelle&amp;rsquo;s posts on the topic&lt;/a&gt; and numerous other SO questions and posts. Let&amp;rsquo;s do one more :)&lt;/p&gt;

&lt;p&gt;For the sake of this post, I&amp;rsquo;ll use this simple Dockerfile:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM ubuntu

RUN apt-get update &amp;amp;&amp;amp; apt-get install -y xterm
RUN useradd -ms /bin/bash xterm
USER xterm
WORKDIR /home/xterm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All it does is start from a Ubuntu image, install the xterm package, creates a dedicated user and set the env to use this user. This is in no way the perfect Dockerfile, but it serves its goal!&lt;/p&gt;

&lt;p&gt;If you try to run it the &amp;ldquo;naive&amp;rdquo; way, here is what it should look like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$&amp;gt; docker run -it --rm xterm xterm 
xterm: Xt error: Can&#39;t open display: 
xterm: DISPLAY is not set
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OK, let&amp;rsquo;s bring in the DISPLAY variable&amp;hellip; and bring the related X11 Unix socket:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -it --rm -e DISPLAY=${DISPLAY} -v /tmp/.X11-unix:/tmp/.X11-unix xterm xterm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Great, an Xterm window. Told you this is one was easy.&lt;/p&gt;

&lt;h3 id=&#34;next-level-running-a-graphical-application-in-a-docker-via-ssh:6dc1c5ae07315872c30f93a6902bf01e&#34;&gt;Next level: Running a graphical application in a Docker via SSH&lt;/h3&gt;

&lt;p&gt;Now that we know how to run a graphical app from a Docker container &lt;em&gt;OR&lt;/em&gt; from a remote server via SSH, let&amp;rsquo;s see how we can do &lt;em&gt;both&lt;/em&gt; at a time. Let&amp;rsquo;s run a graphical application inside a Docker container, on a remote server.&lt;/p&gt;

&lt;p&gt;First, let&amp;rsquo;s try to naively chain both tricks: (spoiler: it won&amp;rsquo;t work)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ssh -X localhost
docker run -it --rm -e DISPLAY=${DISPLAY} -v /tmp/.X11-unix:/tmp/.X11-unix xterm xterm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should get an error like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;xterm: Xt error: Can&#39;t open display: localhost:10.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OK, so the brute force approach does not work. What&amp;rsquo;s going on exactly?&lt;/p&gt;

&lt;p&gt;Have a look at the &lt;code&gt;DISPLAY&lt;/code&gt; environment variable, when running a graphical app:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In a Docker container: &lt;code&gt;:0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;On a remote server via SSH: &lt;code&gt;localhost:10.0&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Your mileage may vary, but the first obvious things is they do not describe the same thing.&lt;/p&gt;

&lt;p&gt;The first one, the Docker version, instructs the X11 client to look for the &lt;code&gt;/tmp/.X11-unix/X0&lt;/code&gt; UNIX domain socket to talk to the local X server. Obviously, using a UNIX domain socket has not chance to work on a remote system. Hence the different value. It instructs the X11 client to talk to the &amp;ldquo;remote&amp;rdquo; X server on localhost, &amp;ldquo;slot&amp;rdquo; 10 over TCP.&lt;/p&gt;

&lt;p&gt;I said &amp;ldquo;slot&amp;rdquo; as this is not an actual port number. The actual port number is &lt;code&gt;6000 + slot&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We can now see that the process listening on port 6010 on the server is the SSH daemon itself:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo lsof -i4:6010
COMMAND   PID       USER   FD   TYPE   DEVICE SIZE/OFF NODE NAME
sshd    30333 jean-tiare   10u  IPv4 23770030      0t0  TCP localhost:6010 (LISTEN)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this case the SSH daemon opens a TCP tunnel between the client and the remote server and forwards all data to the local server as specified by the local &lt;code&gt;DISPLAY&lt;/code&gt; environment variable.&lt;/p&gt;

&lt;p&gt;One way to overcome this limitation is to launch the Docker container using the host&amp;rsquo;s network namespace with &lt;code&gt;--net host&lt;/code&gt; but this is generally not what you want as you&amp;rsquo;d break your network isolation in the same time.&lt;/p&gt;

&lt;p&gt;Another way to overcome this, is to re-export the TCP connection as a UNIX domain socket and launch the application like we are used to. Fortunately, a well-known tool can do exactly this: socat.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Log into the remote server
ssh -X localhost

# Get the Display number from the DISPLAY variable
DISPLAY_NUMBER=$(echo $DISPLAY | cut -d. -f1 | cut -d: -f2)

# Proxy between the TCP and the Unix domain world, in the background
socat TCP4:localhost:60${DISPLAY_NUMBER} UNIX-LISTEN:/tmp/.X11-unix/X${DISPLAY_NUMBER} &amp;amp;

# Expose the &amp;quot;new&amp;quot; display address
export DISPLAY=:$(echo $DISPLAY | cut -d. -f1 | cut -d: -f2)

# Finally, open xterm in the Docker....
docker run -it --rm -e &#39;DISPLAY=${DISPLAY}&#39; -v /tmp/.X11-unix:/tmp/.X11-unix xterm xterm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which should output something like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;X11 connection rejected because of wrong authentication.
xterm: Xt error: Can&#39;t open display: :10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;ldquo;Wrong authentication&amp;rdquo;? What do you mean by &amp;ldquo;wrong authentication&amp;rdquo;?&lt;/p&gt;

&lt;p&gt;X11 uses a concept of &amp;ldquo;magic cookies&amp;rdquo; to grant access to the server. A bit like web cookies. If you don&amp;rsquo;t have the cookie, you can not open a connection to the server and then not display anything. This authentication information is stored in &lt;code&gt;~/.Xauthority&lt;/code&gt; and can be manipulated using the &lt;code&gt;xauth&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;Still from the SSH connection, Let&amp;rsquo;s retry the proxy trick with the authority file mounted in the container:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;socat TCP4:localhost:60${DISPLAY_NUMBER} UNIX-LISTEN:/tmp/.X11-unix/X${DISPLAY_NUMBER} &amp;amp;
docker run -it --rm \
  -e DISPLAY=${DISPLAY} \
  -v /tmp/.X11-unix:/tmp/.X11-unix \
  -v ${HOME}/.Xauthority:/home/xterm/.Xauthority \
  xterm xterm  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And&amp;hellip;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;X11 connection rejected because of wrong authentication.
xterm: Xt error: Can&#39;t open display: :10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Still not&amp;hellip;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s have a look at what &lt;code&gt;xauth list&lt;/code&gt; outputs:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;...
jt-laptop/unix:10  MIT-MAGIC-COOKIE-1  aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The cookie is obviously bogus in the example. But the interesting part is: the hostname is actually part of the authentication. Each Docker container get its own hostname. Maybe that&amp;rsquo;s the cause of the last failure?&lt;/p&gt;

&lt;p&gt;We can cheat and force the hostname inside the container to be the same as the SSH host and retry:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;socat TCP4:localhost:60${DISPLAY_NUMBER} UNIX-LISTEN:/tmp/.X11-unix/X${DISPLAY_NUMBER} &amp;amp;
docker run -it --rm \
  -e DISPLAY=${DISPLAY} \
  -v /tmp/.X11-unix:/tmp/.X11-unix \
  -v $HOME/.Xauthority:/home/xterm/.Xauthority \
  --hostname $(hostname) \
  xterm xterm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It works! &amp;trade;&lt;/p&gt;

&lt;p&gt;To sum it up:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Open an SSH connection to the remote server
ssh -X localhost

# Get the DISPLAY slot and create the new DISPLAY variable
DISPLAY_NUMBER=$(echo $DISPLAY | cut -d. -f1 | cut -d: -f2)
export DISPLAY=:${DISPLAY_NUMBER}

# Proxy between the TCP and the Unix domain world, in the background
socat TCP4:localhost:60${DISPLAY_NUMBER} UNIX-LISTEN:/tmp/.X11-unix/X${DISPLAY_NUMBER} &amp;amp;

# Finally, open xterm in the Docker....
docker run -it --rm \
  -e &#39;DISPLAY=${DISPLAY}&#39; \
  -v /tmp/.X11-unix:/tmp/.X11-unix \
  --hostname $(hostname) \
  xterm xterm
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;stretch-goal-clean-it-up:6dc1c5ae07315872c30f93a6902bf01e&#34;&gt;Stretch goal: clean it up!&lt;/h3&gt;

&lt;p&gt;The proof of concept works, that&amp;rsquo;s great. How can we make it cleaner? Well, first remember that containers goal is to to isolate as much as possible an application from the host. Sharing the X11 server is not exactly the best way to do it&amp;hellip; but we we can&amp;rsquo;t really help on that. What we can (should) isolate:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The hostname. We overloaded it such that it matches the one of the SSH server.&lt;/li&gt;
&lt;li&gt;The Unix sockets. They are all shared in all containers sharing the same trick.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can set the DISPLAY variable to whatever we want, provided&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;We have the corresponding UNIX domain socket&lt;/li&gt;
&lt;li&gt;We have the corresponding authentication cookie&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The later is also true for the hostname.&lt;/p&gt;

&lt;p&gt;On the remote server, let&amp;rsquo;s do some setup:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Prepare target env
CONTAINER_DISPLAY=&amp;quot;0&amp;quot;
CONTAINER_HOSTNAME=&amp;quot;xterm&amp;quot;

# Create a directory for the socket
mkdir -p display/socket
touch display/Xauthority

# Get the DISPLAY slot
DISPLAY_NUMBER=$(echo $DISPLAY | cut -d. -f1 | cut -d: -f2)

# Extract current authentication cookie
AUTH_COOKIE=$(xauth list | grep &amp;quot;^$(hostname)/unix:${DISPLAY_NUMBER} &amp;quot; | awk &#39;{print $3}&#39;)

# Create the new X Authority file
xauth -f display/Xauthority add ${CONTAINER_HOSTNAME}/unix:${CONTAINER_DISPLAY} MIT-MAGIC-COOKIE-1 ${AUTH_COOKIE}

# Proxy with the :0 DISPLAY
socat TCP4:localhost:60${DISPLAY_NUMBER} UNIX-LISTEN:display/socket/X${CONTAINER_DISPLAY} &amp;amp;

# Launch the container
docker run -it --rm \
  -e DISPLAY=:${CONTAINER_DISPLAY} \
  -v ${PWD}/display/socket:/tmp/.X11-unix \
  -v ${PWD}/display/Xauthority:/home/xterm/.Xauthority \
  --hostname ${CONTAINER_HOSTNAME} \
  xterm xterm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And voila! (french accent inside) You no have the most beautifully awesome graphical app running on a remote (as far as localhost is remote) server, inside a Docker container.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tracing a packet journey using Linux tracepoints, perf and eBPF</title>
      <link>http://blog.yadutaf.fr/2017/07/28/tracing-a-packet-journey-using-linux-tracepoints-perf-ebpf/</link>
      <pubDate>Fri, 28 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>http://blog.yadutaf.fr/2017/07/28/tracing-a-packet-journey-using-linux-tracepoints-perf-ebpf/</guid>
      <description>

&lt;p&gt;I&amp;rsquo;ve been looking for a low level Linux network debugging tool for quite some time. Linux allows to build complex networks running directly on the host, using a combination of virtual interfaces and &lt;a href=&#34;http://blog.yadutaf.fr/2014/01/19/introduction-to-linux-namespaces-part-5-net/&#34;&gt;network namespaces&lt;/a&gt;. When something goes wrong, troubleshooting is rather tedious. If this is a L3 routing issue, &lt;code&gt;mtr&lt;/code&gt; has a good chance of being of some help. But if this is a lower level issue, I typically end up manually checking each interface / bridge / network namespace / iptables and firing up a couple of tcpdumps as an attempt to get a sense of what&amp;rsquo;s going on. If you have no prior knowledge of the network setup, this may feel like a maze.&lt;/p&gt;

&lt;p&gt;What I&amp;rsquo;d need is a tool which could tell me &amp;ldquo;Hey, I&amp;rsquo;ve seen your packet: It&amp;rsquo;s gone this way, on this interface, in this network namespace&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Basically, what I&amp;rsquo;d need is a &lt;code&gt;mtr&lt;/code&gt; for L2.&lt;/p&gt;

&lt;p&gt;Does not exist? Let&amp;rsquo;s build one!&lt;/p&gt;

&lt;p&gt;At the end of this post, we&amp;rsquo;ll have a simple and easy to use low level packet tracer. If you ping a local Docker container, it will show something like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# ping -4 172.17.0.2
[  4026531957]          docker0 request #17146.001 172.17.0.1 -&amp;gt; 172.17.0.2
[  4026531957]      vetha373ab6 request #17146.001 172.17.0.1 -&amp;gt; 172.17.0.2
[  4026532258]             eth0 request #17146.001 172.17.0.1 -&amp;gt; 172.17.0.2
[  4026532258]             eth0   reply #17146.001 172.17.0.2 -&amp;gt; 172.17.0.1
[  4026531957]      vetha373ab6   reply #17146.001 172.17.0.2 -&amp;gt; 172.17.0.1
[  4026531957]          docker0   reply #17146.001 172.17.0.2 -&amp;gt; 172.17.0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;tracing-to-the-rescue:722df47ecbedbece281b05064cb93eb9&#34;&gt;Tracing to the rescue&lt;/h3&gt;

&lt;p&gt;One way to get out of a maze, is by exploring. This is what you do when getting out of the maze is part of a game. Another way to get out is to shift your point of view, looking from above, and observing the path taken by those who know the path.&lt;/p&gt;

&lt;p&gt;In Linux terms, that would mean shifting to the kernel point of view, where network namespaces are just labels, instead of &amp;ldquo;containers&amp;rdquo;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:722df47ecbedbece281b05064cb93eb9:containers&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:722df47ecbedbece281b05064cb93eb9:containers&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. In the kernel, packets, interfaces and so on are plain observable objects.&lt;/p&gt;

&lt;p&gt;In this post, I&amp;rsquo;ll focus on 2 tracing tools. &lt;code&gt;perf&lt;/code&gt; and &lt;code&gt;eBPF&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;introducing-perf-and-ebpf:722df47ecbedbece281b05064cb93eb9&#34;&gt;Introducing &lt;code&gt;perf&lt;/code&gt; and &lt;code&gt;eBPF&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;perf&lt;/code&gt; is a the baseline tool for every performance related analysis on Linux. It is developed in the same source tree as the Linux kernel and must be specifically compiled for the kernel you will use to trace. It can trace the kernel as well as user programs. It may also work by sampling or using tracepoints. Think of it as a massive superset of &lt;code&gt;strace&lt;/code&gt; with a much lower overhead. We&amp;rsquo;ll use it only in a very simple way here. If you want to know more about &lt;code&gt;perf&lt;/code&gt;, I highly encourage you to &lt;a href=&#34;http://www.brendangregg.com/perf.html&#34;&gt;visit Brendan Gregg&amp;rsquo;s blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;eBPF&lt;/code&gt; is a relatively recent addition to the Linux Kernel. As its name suggests, this is an extended version of the BPF bytecode known as &amp;ldquo;Berkeley Packet Filter&amp;rdquo; used to&amp;hellip; filter packets on the BSD family. You name it. On Linux, it can also be used to safely run platform independent code in the live kernel, provided that it meets some safety criteria. For instance, memory accesses are validated BEFORE the program can run and it must be possible to prove that the program will end in a restricted amount of time. If the kernel can&amp;rsquo;t prove it, even if it&amp;rsquo;s safe and always terminates, it will be rejected.&lt;/p&gt;

&lt;p&gt;Such programs can be used as network classifier for QOS, very low level networking and filtering as part of eXpress Data Plane (XDP), as a tracing agent and many other places. Tracing probes can be attached to any function whose symbol is exported in &lt;code&gt;/proc/kallsyms&lt;/code&gt; or any tracepoints. In this post, I&amp;rsquo;ll focus on tracing agents attached to tracepoints.&lt;/p&gt;

&lt;p&gt;For an example of tracing probe attached to a kernel function or as a gentler introduction, I invite you to &lt;a href=&#34;http://blog.yadutaf.fr/2016/03/30/turn-any-syscall-into-event-introducing-ebpf-kernel-probes/&#34;&gt;read my previous post on eBPF&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;lab-setup:722df47ecbedbece281b05064cb93eb9&#34;&gt;Lab setup&lt;/h3&gt;

&lt;p&gt;For this post, we need &lt;code&gt;perf&lt;/code&gt; and some tools to work with eBPF. As I&amp;rsquo;m not a great fan of handwritten assembly, I&amp;rsquo;ll use &lt;a href=&#34;https://github.com/iovisor/bcc&#34;&gt;&lt;code&gt;bcc&lt;/code&gt;&lt;/a&gt; here. This is a powerful and flexible tool allowing you to write kernel probes as restricted C and instrument them in userland with Python. Heavyweight for production, but perfect for development!&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll reproduce here install instructions for Ubuntu 17.04 (Zesty) which is the OS powering my laptop. Instructions for &amp;ldquo;perf&amp;rdquo; should not diverge much from distributions to other and specific &lt;a href=&#34;https://github.com/iovisor/bcc/blob/master/INSTALL.md&#34;&gt;&lt;code&gt;bcc&lt;/code&gt; install instructions can be found on Github&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: attaching eBPF to tracepoints requires at least Linux kernel &amp;gt; 4.7.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Install &lt;code&gt;perf&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Grab &#39;perf&#39;
sudo apt install linux-tools-generic

# Test it
perf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you see an error message, it probably means that your kernel was updated recently but you did not reboot yet.&lt;/p&gt;

&lt;p&gt;Install &lt;code&gt;bcc&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install dependencies
sudo apt install bison build-essential cmake flex git libedit-dev python zlib1g-dev libelf-dev libllvm4.0 llvm-dev libclang-dev luajit luajit-5.1-dev

# Grab the sources
git clone https://github.com/iovisor/bcc.git

# Build and install
mkdir bcc/build
cd bcc/build
cmake .. -DCMAKE_INSTALL_PREFIX=/usr
make
sudo make install
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;finding-good-tracepoints-aka-as-manually-tracing-a-packet-s-journey-with-perf:722df47ecbedbece281b05064cb93eb9&#34;&gt;Finding good tracepoints aka as &amp;ldquo;manually tracing a packet&amp;rsquo;s journey with &lt;code&gt;perf&lt;/code&gt;&amp;ldquo;&lt;/h3&gt;

&lt;p&gt;There are multiple ways to find good tracepoints. In a previous version of this post, I started from the code of the &lt;code&gt;veth&lt;/code&gt; driver and followed the trail from there to find functions to trace. While it did lead to acceptable results, I could not catch all the packets. Indeed, the common paths crossed by all packets are in un-exported (inline or static) methods. This is also when I realized Linux had tracepoints and decided to rewrite this post and the associated code using tracepoints instead. This was quite frustrating, but also much more interesting (to me).&lt;/p&gt;

&lt;p&gt;Enough talks on myself, back to work.&lt;/p&gt;

&lt;p&gt;The goal is to trace the path taken by a packet. Depending on the crossed interfaces, the crossed tracepoints may differ (spoiler alert: they do).&lt;/p&gt;

&lt;p&gt;To find suitable tracepoints, I used ping with 2 internal and 2 external targets under &lt;code&gt;perf trace&lt;/code&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;localhost with IP &lt;em&gt;127.0.0.1&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;An innocent Docker container with IP &lt;em&gt;172.17.0.2&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;My phone via USB tethering with IP &lt;em&gt;192.168.42.129&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;My phone via WiFi with IP &lt;em&gt;192.168.43.1&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;code&gt;perf trace&lt;/code&gt; is a sub command of perf, which produces an output similar to strace (with a MUCH lower overhead) by default. We can easily tweak it to hide the syscalls themselves and instead print events of the &amp;lsquo;net&amp;rsquo; category. For instance, tracing a ping to a Docker container with IP 172.17.0.2 would look like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo perf trace --no-syscalls --event &#39;net:*&#39; ping 172.17.0.2 -c1 &amp;gt; /dev/null
     0.000 net:net_dev_queue:dev=docker0 skbaddr=0xffff96d481988700 len=98)
     0.008 net:net_dev_start_xmit:dev=docker0 queue_mapping=0 skbaddr=0xffff96d481988700 vlan_tagged=0 vlan_proto=0x0000 vlan_tci=0x0000 protocol=0x0800 ip_summed=0 len=98 data_len=0 network_offset=14 transport_offset_valid=1 transport_offset=34 tx_flags=0 gso_size=0 gso_segs=0 gso_type=0)
     0.014 net:net_dev_queue:dev=veth79215ff skbaddr=0xffff96d481988700 len=98)
     0.016 net:net_dev_start_xmit:dev=veth79215ff queue_mapping=0 skbaddr=0xffff96d481988700 vlan_tagged=0 vlan_proto=0x0000 vlan_tci=0x0000 protocol=0x0800 ip_summed=0 len=98 data_len=0 network_offset=14 transport_offset_valid=1 transport_offset=34 tx_flags=0 gso_size=0 gso_segs=0 gso_type=0)
     0.020 net:netif_rx:dev=eth0 skbaddr=0xffff96d481988700 len=84)
     0.022 net:net_dev_xmit:dev=veth79215ff skbaddr=0xffff96d481988700 len=98 rc=0)
     0.024 net:net_dev_xmit:dev=docker0 skbaddr=0xffff96d481988700 len=98 rc=0)
     0.027 net:netif_receive_skb:dev=eth0 skbaddr=0xffff96d481988700 len=84)
     0.044 net:net_dev_queue:dev=eth0 skbaddr=0xffff96d481988b00 len=98)
     0.046 net:net_dev_start_xmit:dev=eth0 queue_mapping=0 skbaddr=0xffff96d481988b00 vlan_tagged=0 vlan_proto=0x0000 vlan_tci=0x0000 protocol=0x0800 ip_summed=0 len=98 data_len=0 network_offset=14 transport_offset_valid=1 transport_offset=34 tx_flags=0 gso_size=0 gso_segs=0 gso_type=0)
     0.048 net:netif_rx:dev=veth79215ff skbaddr=0xffff96d481988b00 len=84)
     0.050 net:net_dev_xmit:dev=eth0 skbaddr=0xffff96d481988b00 len=98 rc=0)
     0.053 net:netif_receive_skb:dev=veth79215ff skbaddr=0xffff96d481988b00 len=84)
     0.060 net:netif_receive_skb_entry:dev=docker0 napi_id=0x3 queue_mapping=0 skbaddr=0xffff96d481988b00 vlan_tagged=0 vlan_proto=0x0000 vlan_tci=0x0000 protocol=0x0800 ip_summed=2 hash=0x00000000 l4_hash=0 len=84 data_len=0 truesize=768 mac_header_valid=1 mac_header=-14 nr_frags=0 gso_size=0 gso_type=0)
     0.061 net:netif_receive_skb:dev=docker0 skbaddr=0xffff96d481988b00 len=84)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Keeping only the event names and skbaddr, this looks more readable.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;net_dev_queue           dev=docker0     skbaddr=0xffff96d481988700
net_dev_start_xmit      dev=docker0     skbaddr=0xffff96d481988700
net_dev_queue           dev=veth79215ff skbaddr=0xffff96d481988700
net_dev_start_xmit      dev=veth79215ff skbaddr=0xffff96d481988700
netif_rx                dev=eth0        skbaddr=0xffff96d481988700
net_dev_xmit            dev=veth79215ff skbaddr=0xffff96d481988700
net_dev_xmit            dev=docker0     skbaddr=0xffff96d481988700
netif_receive_skb       dev=eth0        skbaddr=0xffff96d481988700

net_dev_queue           dev=eth0        skbaddr=0xffff96d481988b00
net_dev_start_xmit      dev=eth0        skbaddr=0xffff96d481988b00
netif_rx                dev=veth79215ff skbaddr=0xffff96d481988b00
net_dev_xmit            dev=eth0        skbaddr=0xffff96d481988b00
netif_receive_skb       dev=veth79215ff skbaddr=0xffff96d481988b00
netif_receive_skb_entry dev=docker0     skbaddr=0xffff96d481988b00
netif_receive_skb       dev=docker0     skbaddr=0xffff96d481988b00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are multiple things to be said here. The most obvious being that the &lt;code&gt;skbaddr&lt;/code&gt; changes in the middle, but stays the same otherwise. This is when the echo reply packet is generated as a reply to this echo request (ping). The rest of the time, the same network packet is moved between interfaces, with hopefully no copy. Copying is expensive&amp;hellip;&lt;/p&gt;

&lt;p&gt;The other interesting point is, we clearly see the packet going through the &lt;code&gt;docker0&lt;/code&gt; bridge, then the host side of the veth, &lt;code&gt;veth79215ff&lt;/code&gt; in my case, and finally the container side of the veth, pretending to be &lt;code&gt;eth0&lt;/code&gt;. We don&amp;rsquo;t see the network namespaces yet, but it already gives a good overview.&lt;/p&gt;

&lt;p&gt;Finally, after seeing the packet on &lt;code&gt;eth0&lt;/code&gt; we hit tracepoints in reverse order. This is not the response, but the finalization of the transmission.&lt;/p&gt;

&lt;p&gt;By repeating a similar process on the 4 target scenarios, we can pick the most appropriate tracing points to track our packet&amp;rsquo;s journey. I picked 4 of them:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;net_dev_queue&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;netif_receive_skb_entry&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;netif_rx&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;napi_gro_receive_entry&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Taking these 4 tracepoints will give me trace events in order with no duplication, saving some de-duplication work. Still good to take.&lt;/p&gt;

&lt;p&gt;We can easily double check this selection like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo perf trace --no-syscalls           \
  --event &#39;net:net_dev_queue&#39;           \
  --event &#39;net:netif_receive_skb_entry&#39; \
  --event &#39;net:netif_rx&#39;                \
  --event &#39;net:napi_gro_receive_entry&#39;  \
  ping 172.17.0.2 -c1 &amp;gt; /dev/null
     0.000 net:net_dev_queue:dev=docker0 skbaddr=0xffff8e847720a900 len=98)
     0.010 net:net_dev_queue:dev=veth7781d5c skbaddr=0xffff8e847720a900 len=98)
     0.014 net:netif_rx:dev=eth0 skbaddr=0xffff8e847720a900 len=84)
     0.034 net:net_dev_queue:dev=eth0 skbaddr=0xffff8e849cb8cd00 len=98)
     0.036 net:netif_rx:dev=veth7781d5c skbaddr=0xffff8e849cb8cd00 len=84)
     0.045 net:netif_receive_skb_entry:dev=docker0 napi_id=0x1 queue_mapping=0 skbaddr=0xffff8e849cb8cd00 vlan_tagged=0 vlan_proto=0x0000 vlan_tci=0x0000 protocol=0x0800 ip_summed=2 hash=0x00000000 l4_hash=0 len=84 data_len=0 truesize=768 mac_header_valid=1 mac_header=-14 nr_frags=0 gso_size=0 gso_type=0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mission accomplished!&lt;/p&gt;

&lt;p&gt;If you want to go further and explore a list of available network tracepoints, you may user &lt;code&gt;perf list&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo perf list &#39;net:*&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This should return a list of tracepoints names like &lt;code&gt;net:netif_rx&lt;/code&gt;. The part before the colon (&amp;rsquo;:&amp;lsquo;) is the event category (&amp;lsquo;net&amp;rsquo;). The part after is the event name, in this category.&lt;/p&gt;

&lt;h3 id=&#34;writing-a-custom-tracer-with-ebpf-bcc:722df47ecbedbece281b05064cb93eb9&#34;&gt;Writing a custom tracer with &lt;code&gt;eBPF&lt;/code&gt; / &lt;code&gt;bcc&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;This would be more than enough for most situations. If you were reading this post to learn how to trace a packet&amp;rsquo;s journey on a Linux box, you already got all you need. But, if you want to dive deeper, run a custom filter, track more data like the network namespaces crossed by the packets or the source and destination IPs, please, bear with me.&lt;/p&gt;

&lt;p&gt;Starting with Linux Kernel 4.7, eBPF programs can be attached to kernel tracepoints. Before that, the only alternative to build this tracer would have been to attach the probes to exported kernel symbols. While this could work, it would have a couple of drawbacks:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The kernel internal API is not stable. Tracepoints are (although the data structures ae not necessarily&amp;hellip;).&lt;/li&gt;
&lt;li&gt;For performance reasons, most of the networking inner functions are inlined or static. Neither of which can be probed.&lt;/li&gt;
&lt;li&gt;It is tedious to find all potential call sites for this functions, and sometime not all required data is available at this stage.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;An earlier version of this post attempted to use kprobes, which are easier to use, but the results were at best incomplete.&lt;/p&gt;

&lt;p&gt;Now, let&amp;rsquo;s be honest, accessing data via tracepoints is a lot more tedious than with there kprobe counterpart. While I tried to keep this post as gentle as possible, you may want to start with the (slightly older) post &lt;a href=&#34;http://blog.yadutaf.fr/2016/03/30/turn-any-syscall-into-event-introducing-ebpf-kernel-probes/&#34;&gt;&amp;ldquo;How to turn any syscall into an event: Introducing eBPF Kernel probes&amp;rdquo;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This disclaimer aside, let&amp;rsquo;s start with a simple hello world and get the low level plumbing into place. In this hello world, we&amp;rsquo;ll build an event every time 1 of the 4 tracepoints we chose earlier (&lt;code&gt;net_dev_queue&lt;/code&gt;, &lt;code&gt;netif_receive_skb_entry&lt;/code&gt;, &lt;code&gt;netif_rx&lt;/code&gt; and &lt;code&gt;napi_gro_receive_entry&lt;/code&gt;) is triggered. To keep things simple at this stage, we&amp;rsquo;ll send the program&amp;rsquo;s &lt;code&gt;comm&lt;/code&gt;, that is, a 16 char string that&amp;rsquo;s basically the program name.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#include &amp;lt;bcc/proto.h&amp;gt;
#include &amp;lt;linux/sched.h&amp;gt;

// Event structure
struct route_evt_t {
        char comm[TASK_COMM_LEN];
};
BPF_PERF_OUTPUT(route_evt);

static inline int do_trace(void* ctx, struct sk_buff* skb)
{
    // Built event for userland
    struct route_evt_t evt = {};
    bpf_get_current_comm(evt.comm, TASK_COMM_LEN);

    // Send event to userland
    route_evt.perf_submit(ctx, &amp;amp;evt, sizeof(evt));

    return 0;
}

/**
  * Attach to Kernel Tracepoints
  */

TRACEPOINT_PROBE(net, netif_rx) {
    return do_trace(args, (struct sk_buff*)args-&amp;gt;skbaddr);
}

TRACEPOINT_PROBE(net, net_dev_queue) {
    return do_trace(args, (struct sk_buff*)args-&amp;gt;skbaddr);
}

TRACEPOINT_PROBE(net, napi_gro_receive_entry) {
    return do_trace(args, (struct sk_buff*)args-&amp;gt;skbaddr);
}

TRACEPOINT_PROBE(net, netif_receive_skb_entry) {
    return do_trace(args, (struct sk_buff*)args-&amp;gt;skbaddr);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This snippet attaches to the 4 tracepoints of the &amp;ldquo;net&amp;rdquo; category, loads the &lt;code&gt;skbaddr&lt;/code&gt; field and passes it to the common section which only loads the program name for now. If you wonder where this &lt;code&gt;args-&amp;gt;skbaddr&lt;/code&gt; come from (and I&amp;rsquo;d be glad you do), the &lt;code&gt;args&lt;/code&gt; structure is generated for you by bcc whenever you define a tracepoint with &lt;code&gt;TRACEPOINT_PROBE&lt;/code&gt;. As it is generated on the fly, there is no easy way to see its definition BUT, there is a better way. We can directly look at the data source, from the kernel. Fortunately there is a &lt;code&gt;/sys/kernel/debug/tracing/events&lt;/code&gt; entry for each tracepoint. For instance, for the &lt;code&gt;net:netif_rx&lt;/code&gt;, one could just &amp;ldquo;cat&amp;rdquo; &lt;code&gt;/sys/kernel/debug/tracing/events/net/netif_rx/format&lt;/code&gt; which should output something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;name: netif_rx
ID: 1183
format:
	field:unsigned short common_type;         offset:0; size:2; signed:0;
	field:unsigned char common_flags;         offset:2; size:1; signed:0;
	field:unsigned char common_preempt_count; offset:3; size:1; signed:0;
	field:int common_pid;                     offset:4; size:4; signed:1;

	field:void * skbaddr;         offset:8;  size:8; signed:0;
	field:unsigned int len;       offset:16; size:4; signed:0;
	field:__data_loc char[] name; offset:20; size:4; signed:1;

print fmt: &amp;quot;dev=%s skbaddr=%p len=%u&amp;quot;, __get_str(name), REC-&amp;gt;skbaddr, REC-&amp;gt;len
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You may notice the &lt;code&gt;print fmt&lt;/code&gt; line at the end of the record. This is exactly what&amp;rsquo;s used by &lt;code&gt;perf trace&lt;/code&gt; to generate its output.&lt;/p&gt;

&lt;p&gt;With the low level plumbing in place and well understood, we can wrap it in a Python script to display a line for every event send by the eBPF side of the probe:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/env python
# coding: utf-8

from socket import inet_ntop
from bcc import BPF
import ctypes as ct

bpf_text = &#39;&#39;&#39;&amp;lt;SEE CODE SNIPPET ABOVE&amp;gt;&#39;&#39;&#39;

TASK_COMM_LEN = 16 # linux/sched.h

class RouteEvt(ct.Structure):
    _fields_ = [
        (&amp;quot;comm&amp;quot;,    ct.c_char * TASK_COMM_LEN),
    ]

def event_printer(cpu, data, size):
    # Decode event
    event = ct.cast(data, ct.POINTER(RouteEvt)).contents

    # Print event
    print &amp;quot;Just got a packet from %s&amp;quot; % (event.comm)

if __name__ == &amp;quot;__main__&amp;quot;:
    b = BPF(text=bpf_text)
    b[&amp;quot;route_evt&amp;quot;].open_perf_buffer(event_printer)

    while True:
        b.kprobe_poll()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You may test it now. You will need to be root.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: There is no filtering at this stage. Even a low background network usage may flood your terminal!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;$&amp;gt; sudo python ./tracepkt.py
...
Just got a packet from ping6
Just got a packet from ping6
Just got a packet from ping
Just got a packet from irq/46-iwlwifi
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this case, you can see that I was using ping and ping6 and the WiFi driver just received some packets. In that case, that was the echo reply.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s start adding some useful data / filters.&lt;/p&gt;

&lt;p&gt;I will not focus on performance in this post. This will better demonstrate the the power and limitations of eBPF. To make it (much) faster, we could use the packet size as a heuristic, assuming there is no strange IP options. Using the example programs as is will slow down your network traffic.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: to limit the length of this post, I&amp;rsquo;ll focus on the C/eBPF part here. I&amp;rsquo;ll put a link to the full source code at the end of this post.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;add-in-network-interface-information:722df47ecbedbece281b05064cb93eb9&#34;&gt;Add in network interface information&lt;/h3&gt;

&lt;p&gt;First, you can safely remove the &amp;ldquo;comm&amp;rdquo; fields, loading and sched.h header. It&amp;rsquo;s of no real use here, sorry.&lt;/p&gt;

&lt;p&gt;Then you can include &lt;code&gt;net/inet_sock.h&lt;/code&gt; so that we have all necessary declarations and add &lt;code&gt;char ifname[IFNAMSIZ];&lt;/code&gt; to the event structure.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll now load the device name from the device structure. This is interesting as this is an actually useful piece of information and it demonstrates on a manageable scale the techniques to load any data:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// Get device pointer, we&#39;ll need it to get the name and network namespace
struct net_device *dev;
bpf_probe_read(&amp;amp;dev, sizeof(skb-&amp;gt;dev), ((char*)skb) + offsetof(typeof(*skb), dev));

// Load interface name
bpf_probe_read(&amp;amp;evt.ifname, IFNAMSIZ, dev-&amp;gt;name);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can test it, it works as is. Do not forget to add the related part on the Python side though :)&lt;/p&gt;

&lt;p&gt;OK, so how does it work? To load the interface name, we need the interface device structure. I&amp;rsquo;ll start from the last statement as it&amp;rsquo;s the easiest to understand and the previous one is actually just or trickier version. It uses &lt;code&gt;bpf_probe_read&lt;/code&gt; to read data of length &lt;code&gt;IFNAMSIZ&lt;/code&gt; from &lt;code&gt;dev-&amp;gt;name&lt;/code&gt; and copy it to &lt;code&gt;evt.ifname&lt;/code&gt;. The fist line follows exactly the same logic. It loads the value of the &lt;code&gt;skb-&amp;gt;dev&lt;/code&gt; pointer into &lt;code&gt;dev&lt;/code&gt;. Unfortunately, I could not find another way to load the field address without this nice offsetof / typeof tricks.&lt;/p&gt;

&lt;p&gt;As a reminder, the goal of eBPF is to allow &lt;em&gt;safe&lt;/em&gt; scripting of the kernel. This implies that random memory access are forbidden. All memory accesses must be validated. Unless the memory you access in on the stack, you need to use the &lt;code&gt;bpf_probe_read&lt;/code&gt; read accessor. This makes to code cumbersome to read / write but makes it safe too. &lt;code&gt;bpf_probe_read&lt;/code&gt; is somehow like a safe version of &lt;code&gt;memcpy&lt;/code&gt;. It is defined in &lt;a href=&#34;http://elixir.free-electrons.com/linux/v4.10.17/source/kernel/trace/bpf_trace.c#L64&#34;&gt;bpf_trace.c in the kernel&lt;/a&gt;. The interesting parts being:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It&amp;rsquo;s like memcpy. Beware of the cost of copies on performance.&lt;/li&gt;
&lt;li&gt;In case of error, it will return a buffer initialized to 0 and return an error. It will &lt;em&gt;not&lt;/em&gt; crash or stop the program.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For the remaining parts of this post, I&amp;rsquo;ll use the following macro to help keep things readable:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#define member_read(destination, source_struct, source_member)                 \
  do{                                                                          \
    bpf_probe_read(                                                            \
      destination,                                                             \
      sizeof(source_struct-&amp;gt;source_member),                                    \
      ((char*)source_struct) + offsetof(typeof(*source_struct), source_member) \
    );                                                                         \
  } while(0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which allows us to write:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;member_read(&amp;amp;dev, skb, dev);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s better!&lt;/p&gt;

&lt;h3 id=&#34;add-in-the-network-namespace-id:722df47ecbedbece281b05064cb93eb9&#34;&gt;Add in the network namespace ID&lt;/h3&gt;

&lt;p&gt;That&amp;rsquo;s probably the most valuable piece of information. In itself, it is a valid reason to all these efforts. Unfortunately, this is also the hardest to load.&lt;/p&gt;

&lt;p&gt;The namespace identifier can be loaded from 2 places:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;the socket &amp;lsquo;sk&amp;rsquo; structure&lt;/li&gt;
&lt;li&gt;the device &amp;lsquo;dev&amp;rsquo; structure&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I was initially using the socket structure as this is the one I was using when writing &lt;a href=&#34;https://github.com/iovisor/bcc/blob/master/tools/solisten.py&#34;&gt;solisten.py&lt;/a&gt;. Unfortunately, and I&amp;rsquo;m not sure why, the namespace identifier is no longer readable as soon as the packet crosses a namespace boundary. The field is all 0s, which is a clear indicator of an invalid memory access (remember how bpf_probe_read works in case of errors) and defeats the whole point.&lt;/p&gt;

&lt;p&gt;Fortunately, the device approach works. Think of it like asking the packet on which interface it is and asking the interface in which namespace it belongs.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;struct net* net;

// Get netns id. Equivalent to: evt.netns = dev-&amp;gt;nd_net.net-&amp;gt;ns.inum
possible_net_t *skc_net = &amp;amp;dev-&amp;gt;nd_net;
member_read(&amp;amp;net, skc_net, net);
struct ns_common* ns = member_address(net, ns);
member_read(&amp;amp;evt.netns, ns, inum);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which uses the following additional macro for improved readability:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#define member_address(source_struct, source_member) \
({                                                   \
  void* __ret;                                       \
  __ret = (void*) (((char*)source_struct) + offsetof(typeof(*source_struct), source_member)); \
  __ret;                                             \
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As a side effect, it allows to simplify the &lt;code&gt;member_read&lt;/code&gt; macro. I&amp;rsquo;ll leave it as an exercise for the reader.&lt;/p&gt;

&lt;p&gt;Plug this together, and&amp;hellip; Tadaa!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$&amp;gt; sudo python ./tracepkt.py
[  4026531957]          docker0
[  4026531957]      vetha373ab6
[  4026532258]             eth0
[  4026532258]             eth0
[  4026531957]      vetha373ab6
[  4026531957]          docker0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is what you should see if you send a ping to a Docker container. The packet goes through the local &lt;code&gt;docker0&lt;/code&gt; bridge and then moves to the the &lt;code&gt;veth&lt;/code&gt; pair, crossing the network namespace boundary and the reply follows the exact reverse path.&lt;/p&gt;

&lt;p&gt;That was a nasty one!&lt;/p&gt;

&lt;h3 id=&#34;going-further-trace-only-requests-reply-and-echo-replies-packets:722df47ecbedbece281b05064cb93eb9&#34;&gt;Going further: trace only requests reply and echo replies packets&lt;/h3&gt;

&lt;p&gt;As a bonus, we&amp;rsquo;ll also load the IP from the packets. We have to read the IP header anyway. I&amp;rsquo;ll stick to IPv4 here, but the same logic applies for IPv6.&lt;/p&gt;

&lt;p&gt;Bad news is, nothing is really simple. Remember, we are dealing with the kernel, in the network path. Some packets have not yet been opened. This means that some headers offsets are still uninitialized. We&amp;rsquo;ll have to compute all of them, going from the MAC header to the IP header and finally to the ICMP header.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s start gently by loading the MAC header address and deducing the IP header address. We won&amp;rsquo;t load the MAC header itself and instead assume it is 14 bytes long.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Compute MAC header address
char* head;
u16 mac_header;

member_read(&amp;amp;head,       skb, head);
member_read(&amp;amp;mac_header, skb, mac_header);

// Compute IP Header address
#define MAC_HEADER_SIZE 14;
char* ip_header_address = head + mac_header + MAC_HEADER_SIZE;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This basically means that the IP header starts at &lt;code&gt;skb-&amp;gt;head + skb-&amp;gt;mac_header + MAC_HEADER_SIZE;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We can now decode the IP version in the first 4 bits of the IP header, that is, the first half of the first byte, and make sure it is IPv4:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// Load IP protocol version
u8 ip_version;
bpf_probe_read(&amp;amp;ip_version, sizeof(u8), ip_header_address);
ip_version = ip_version &amp;gt;&amp;gt; 4 &amp;amp; 0xf;

// Filter IPv4 packets
if (ip_version != 4) {
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We now load the full IP header, grab the IPs to make the Python info even more useful, make sure the next header is ICMP and derive the ICMP header offset. Yes all this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// Load IP Header
struct iphdr iphdr;
bpf_probe_read(&amp;amp;iphdr, sizeof(iphdr), ip_header_address);

// Load protocol and address
u8 icmp_offset_from_ip_header = iphdr.ihl * 4;
evt.saddr[0] = iphdr.saddr;
evt.daddr[0] = iphdr.daddr;

// Filter ICMP packets
if (iphdr.protocol != IPPROTO_ICMP) {
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we can load the ICMP header itself, make sure this is an echo request of reply and load the id and seq from it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// Compute ICMP header address and load ICMP header
char* icmp_header_address = ip_header_address + icmp_offset_from_ip_header;
struct icmphdr icmphdr;
bpf_probe_read(&amp;amp;icmphdr, sizeof(icmphdr), icmp_header_address);

// Filter ICMP echo request and echo reply
if (icmphdr.type != ICMP_ECHO &amp;amp;&amp;amp; icmphdr.type != ICMP_ECHOREPLY) {
    return 0;
}

// Get ICMP info
evt.icmptype = icmphdr.type;
evt.icmpid   = icmphdr.un.echo.id;
evt.icmpseq  = icmphdr.un.echo.sequence;

// Fix endian
evt.icmpid  = be16_to_cpu(evt.icmpid);
evt.icmpseq = be16_to_cpu(evt.icmpseq);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s all folks!&lt;/p&gt;

&lt;p&gt;If you want to filter ICMP from a specific ping instance, you may assume &lt;code&gt;evt.icmpid&lt;/code&gt; &lt;a href=&#34;https://github.com/iputils/iputils/blob/master/ping_common.c&#34;&gt;is the PID of the ping&lt;/a&gt; at least using Linux&amp;rsquo;s ping.&lt;/p&gt;

&lt;h3 id=&#34;show-time:722df47ecbedbece281b05064cb93eb9&#34;&gt;Show time!&lt;/h3&gt;

&lt;p&gt;With some straightforward Python to handle the event, we can test it in a couple of scenarios. Start the program as root, launch some &amp;ldquo;ping&amp;rdquo; in another terminal and observe:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# ping -4 localhost
[  4026531957]               lo request #20212.001 127.0.0.1 -&amp;gt; 127.0.0.1
[  4026531957]               lo request #20212.001 127.0.0.1 -&amp;gt; 127.0.0.1
[  4026531957]               lo   reply #20212.001 127.0.0.1 -&amp;gt; 127.0.0.1
[  4026531957]               lo   reply #20212.001 127.0.0.1 -&amp;gt; 127.0.0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;An ICMP echo request is sent by process 20212 (the ICMP id on Linux&amp;rsquo;s ping) on the loopback interface, delivered to the very same loopback interface where an echo reply is generated and sent back. The loopback interface is both the emitting and receiving interface.&lt;/p&gt;

&lt;p&gt;What about my WiFi gateway?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# ping -4 192.168.43.1
[  4026531957]           wlp2s0 request #20710.001 192.168.43.191 -&amp;gt; 192.168.43.1
[  4026531957]           wlp2s0   reply #20710.001 192.168.43.1 -&amp;gt; 192.168.43.191
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this case, the echo request and echo reply go through the WiFi interface. Easy.&lt;/p&gt;

&lt;p&gt;On a slightly unrelated note, remember when we were only printing the &amp;ldquo;comm&amp;rdquo; of the process owning the packet? In this case, the echo request would belong to ping process while the reply would belong to the WiFi driver as this is the one generating it as far as Linux is concerned.&lt;/p&gt;

&lt;p&gt;And the last one, my personal favorite, ping a Docker container. It&amp;rsquo;s not my favorite because of Docker. It is my favorite because it best shows the the power of eBPF. It allowed to build an &amp;ldquo;x-ray&amp;rdquo; like tool for ping.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# ping -4 172.17.0.2
[  4026531957]          docker0 request #17146.001 172.17.0.1 -&amp;gt; 172.17.0.2
[  4026531957]      vetha373ab6 request #17146.001 172.17.0.1 -&amp;gt; 172.17.0.2
[  4026532258]             eth0 request #17146.001 172.17.0.1 -&amp;gt; 172.17.0.2
[  4026532258]             eth0   reply #17146.001 172.17.0.2 -&amp;gt; 172.17.0.1
[  4026531957]      vetha373ab6   reply #17146.001 172.17.0.2 -&amp;gt; 172.17.0.1
[  4026531957]          docker0   reply #17146.001 172.17.0.2 -&amp;gt; 172.17.0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With some art, it now looks like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;       Host netns           | Container netns
+---------------------------+-----------------+
| docker0 ---&amp;gt; veth0e65931 ---&amp;gt; eth0          |
+---------------------------+-----------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;final-word:722df47ecbedbece281b05064cb93eb9&#34;&gt;Final word&lt;/h3&gt;

&lt;p&gt;eBPF/bcc enables us to write a new range of tools to deeply troubleshoot, trace and track issues in places previously unreachable without patching the kernel. Tracepoints are also quite handy as they give a good hint on interesting places, removing the need to tediously read the kernel code and can be placed in portions of the code that would otherwise be unreachable from kprobes, like inline or static functions.&lt;/p&gt;

&lt;p&gt;To go further, we could add IPv6 support. This is quite easy to do and I&amp;rsquo;ll leave it as an exercise for the reader. Ideally, I&amp;rsquo;d like to measure the impact on performance as well. But this post is already very, very long. It could be interesting to improve this tool by tracing routing and iptables decisions and tracing ARP packets. All this would turn this tool into a perfect &amp;ldquo;x-ray&amp;rdquo; packet tracer for people like me, sometime struggling with non-trivial Linux network setups.&lt;/p&gt;

&lt;p&gt;As promised, you can see the full code (with IPv6 support) on Github:  &lt;a href=&#34;https://github.com/yadutaf/tracepkt&#34;&gt;https://github.com/yadutaf/tracepkt&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Finally, I&amp;rsquo;d like to acknowledge the help of &lt;a href=&#34;https://twitter.com/fcabestre&#34;&gt;@fcabestre&lt;/a&gt; who helped me rescue the working draft of this post from a malfunctioning hard disk, &lt;a href=&#34;https://twitter.com/bluxte&#34;&gt;@bluxte&lt;/a&gt; for his patient proof reading and the people of &lt;a href=&#34;https://github.com/iovisor/bcc&#34;&gt;bcc&lt;/a&gt; who made this post technically possible.&lt;/p&gt;

&lt;h3 id=&#34;note-s:722df47ecbedbece281b05064cb93eb9&#34;&gt;Note(s)&lt;/h3&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:722df47ecbedbece281b05064cb93eb9:containers&#34;&gt;I&amp;rsquo;ve put &amp;ldquo;containers&amp;rdquo; into quotes as, technically speaking, network namespaces are one of the many building blocks of Linux containers.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:722df47ecbedbece281b05064cb93eb9:containers&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Docker for your users - Introducing user namespace</title>
      <link>http://blog.yadutaf.fr/2016/04/14/docker-for-your-users-introducing-user-namespace/</link>
      <pubDate>Thu, 14 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>http://blog.yadutaf.fr/2016/04/14/docker-for-your-users-introducing-user-namespace/</guid>
      <description>

&lt;p&gt;A few years ago, back when I was a student, my school had rooms full of counters running Linux that any student could use at any time. We all had a personal account on the machines and the machine management was done by a dedicated team.&lt;/p&gt;

&lt;p&gt;Every once in a while, we found ourselves needing a specific tool like &lt;code&gt;valgrind&lt;/code&gt; which was not readily available or a more recent version of another tool. Like &lt;code&gt;gcc&lt;/code&gt;. Replace &amp;ldquo;valgring&amp;rdquo; and &amp;ldquo;gcc&amp;rdquo; with &amp;ldquo;Node&amp;rdquo;, &amp;ldquo;Rust&amp;rdquo; or &amp;ldquo;Go&amp;rdquo;. You get the idea.&lt;/p&gt;

&lt;p&gt;At that point, we basically had 2 options. Either the tool was vital to our study, and it was possible to get it installed for everybody. Or it was not, we were just experimenting on our own as part of a random project.&lt;/p&gt;

&lt;p&gt;In the later case, the only solution was to build it from scratch, put it in our &lt;code&gt;$HOME&lt;/code&gt;, mess up with^W^W^W tweak the &lt;code&gt;$PATH&lt;/code&gt; and &lt;code&gt;$LD_LIBRARY_PATH&lt;/code&gt; environment variables and sometime get some voodoo involved.&lt;/p&gt;

&lt;p&gt;It &lt;em&gt;usually&lt;/em&gt; worked.&lt;/p&gt;

&lt;p&gt;A year ago, I was giving a talk to introduce Docker and, in the question section, I was asked whether I believed Docker could be a solution to this kind of problem. I answered that it was a dangerous idea. Giving docker access to user was basically like giving him the &lt;code&gt;root&lt;/code&gt; password. He would be better with traditional VMs&lt;/p&gt;

&lt;p&gt;Recently, the question came back to me.&lt;/p&gt;

&lt;p&gt;As it turns out, the response has changed and this is a good occasion to talk about the 6th namespace. The one I never blogged about in my &lt;a href=&#34;https://blog.yadutaf.fr/2013/12/22/introduction-to-linux-namespaces-part-1-uts/&#34;&gt;introduction to Linux Namespaces&lt;/a&gt;&amp;hellip; But I will soon #teaser.&lt;/p&gt;

&lt;h3 id=&#34;user-namespaces-and-docker:beda4b7909718c488f1040442ce702e5&#34;&gt;User Namespaces and Docker&lt;/h3&gt;

&lt;p&gt;(If you don&amp;rsquo;t like or care about the technical background, you can safely skip this part)&lt;/p&gt;

&lt;p&gt;In a nutshell, a user namespace is a special Linux kernel mechanism allowing Docker container&amp;rsquo;s to have a &amp;ldquo;faked&amp;rdquo; root user. For example, the root user in a container would be able to manage it&amp;rsquo;s root owned files in the container, act as any user in the container, manage his own network interfaces and some of his mountpoints (restrictions apply) and at the same time being &amp;ldquo;mapped&amp;rdquo; or &amp;ldquo;translated&amp;rdquo; to, say, user &amp;ldquo;ubuntu&amp;rdquo; with uid 1000 on the host system.&lt;/p&gt;

&lt;p&gt;User namespaces are have been introduced as early as Linux 3.5 and are considered as stable &lt;a href=&#34;https://lwn.net/Articles/657432/&#34;&gt;starting with Linux 4.3&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I won&amp;rsquo;t dive too much in the details of user namespaces here, I&amp;rsquo;d really love too, low level bits are by far my favorite topic, but that would be far out of the scope of this post. But stay tuned. While writing this post, I started a more technical one on this very subject ;)&lt;/p&gt;

&lt;p&gt;As far as docker is concerned, starting with Docker 1.10 (the current stable version at the time of writing), it supports a new &lt;code&gt;daemon&lt;/code&gt; option &lt;code&gt;--userns-remap=[USERNAME]&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Neat.&lt;/p&gt;

&lt;p&gt;Wait, what does this &lt;code&gt;--userns-remap&lt;/code&gt; and &lt;code&gt;[USERNAME]&lt;/code&gt; stuff stand for exactly?&lt;/p&gt;

&lt;p&gt;As suggested just earlier, user namespace works by mapping some virtual user ids like root to other user ids on the host. Hence the option name &lt;code&gt;--userns-remap&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Regarding, &amp;ldquo;[USERNAME]&amp;ldquo;, refers to &lt;a href=&#34;http://man7.org/linux/man-pages/man5/subuid.5.html&#34;&gt;&lt;code&gt;/etc/subuid&lt;/code&gt; and &lt;code&gt;/etc/subgid&lt;/code&gt;&lt;/a&gt; files. In a word, these files define the user and group ids a given user can use, beyond his own user id. Just like root can impersonate any user id. If you wonder where this file come from, it&amp;rsquo;s from &lt;a href=&#34;https://github.com/shadow-maint/shadow/blob/ef45bb2496182b5df90ad0323bef75d1a5d69887/src/useradd.c#L2188&#34;&gt;stock &lt;code&gt;useradd&lt;/code&gt; command&lt;/a&gt;. Every time a real user (not a system user) is created on the system, a range of 65536 sub-ids is allocated.&lt;/p&gt;

&lt;p&gt;Does is sound new? Well, not that much. It was &lt;a href=&#34;https://github.com/shadow-maint/shadow/commit/f28ad4b251a42a35c29685850d1686a083cac725&#34;&gt;introduced in early August&amp;hellip; 2013&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Anyway, it maintains simple flat text files looking like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yadutaf:100000:65536
somuser:165536:65536
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It reads as: &amp;ldquo;Let user &amp;lsquo;yadutaf&amp;rsquo; use 65536 uids, starting at 100000&amp;rdquo; and &amp;ldquo;Let user &amp;lsquo;someuser&amp;rsquo; use 65536 uids, starting at 165536&amp;rdquo;. Which is basically the next adjacent range.&lt;/p&gt;

&lt;p&gt;The rule is not set in stone, but the start sub-uid can be guessed as&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FIRST_SUB_UID = 100000 + (UID - 1000) * 65536
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Again, this is only a convention. We can do something slightly different like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yadutaf:1000:1
yadutaf:100000:65535
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It reads as &amp;ldquo;let user yadutaf use his own uid as well as 65535 uids, starting at 100000 and making the total of uids to 65536&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;And this won&amp;rsquo;t break anything.&lt;/p&gt;

&lt;p&gt;Well, actually, this is where is starts to get interesting.&lt;/p&gt;

&lt;p&gt;When starting docker with &lt;code&gt;docker daemon --userns-remap=yadutaf&lt;/code&gt;, docker will parse the subuid and subgid files for &lt;code&gt;yadutaf&lt;/code&gt;, sort all read entries by growing start id and generate kernel userns mapping rules. Without diving too much into the details, this will generate the following rules in &lt;code&gt;/proc/[PID]/uid_map&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;         0       1000          1
         1     100000      65535
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which should look familiar. This structures looks like the one above, but the meaning it slightly different. This time, it reads as:&lt;/p&gt;

&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;Let uid 1000 &lt;em&gt;outside&lt;/em&gt; the container act as &lt;code&gt;root&lt;/code&gt; &lt;em&gt;inside&lt;/em&gt; the container&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Let the 65535 uids starting with 100000 &lt;em&gt;outside&lt;/em&gt; the container act the 65535 uids starting with 1 &lt;em&gt;inside&lt;/em&gt;&amp;ldquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;In other words, 1000 will be 1 and 100002 will be 3.&lt;/p&gt;

&lt;p&gt;This is extremely powerful as this is key to share files between your main host system and your container without loosing access to them. You need a common uid. This common uid will be root in the container while being yours in the real system context.&lt;/p&gt;

&lt;h3 id=&#34;give-power-back-to-the-user-no-security-compromise:beda4b7909718c488f1040442ce702e5&#34;&gt;Give power back to the user, no (security) compromise&lt;/h3&gt;

&lt;p&gt;With all this in mind, we can put the pieces together and let the magic happen. We need to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;get latest Docker release (&amp;gt;=1.10.0)&lt;/li&gt;
&lt;li&gt;configure the subids so that our user will act as root in the container&lt;/li&gt;
&lt;li&gt;configure docker so that it used our ranges&lt;/li&gt;
&lt;li&gt;use real-world applications&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Of course, as the name is passed on the command line of the docker daemon, this will only work for a single user. But keep in mind that Docker 1.10 is the first version to support this feature. It may evolve in the future and get more flexible&lt;/p&gt;

&lt;p&gt;OK, let&amp;rsquo;s start. Assuming our user is &amp;ldquo;yadutaf&amp;rdquo; (that&amp;rsquo;s me) with uid 1000, we&amp;rsquo;ll want &lt;code&gt;/etc/subuid&lt;/code&gt; and &lt;code&gt;/etc/subgid&lt;/code&gt; to contain:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yadutaf:1000:1
yadutaf:100000:65535
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we want docker daemon to use it, without messing with systemd&amp;rsquo;s unit files (trust me, you don&amp;rsquo;t want to), so we&amp;rsquo;ll use the docker configuration file &lt;code&gt;/etc/docker/daemon.json&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
        &amp;quot;userns-remap&amp;quot;: &amp;quot;yadutaf&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All we have to do is restart the daemon, run an innocent, random, test container and see the result:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo systemctl restart docker
$ docker run -d --name redis-userns redis
$ cat /proc/$(docker inspect -f &#39;{{ .State.Pid }}&#39; redis-userns)/uid_map
         0       1000          1
         1     100000      65535
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hooray!&lt;/p&gt;

&lt;p&gt;What about graphical? What about sound? You promised read applications didn&amp;rsquo;t you? Sure I did. Here is a working Firefox:&lt;/p&gt;

&lt;p&gt;First, the Dockerfile:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM ubuntu
MAINTAINER Jean-Tiare Le Bigot &amp;lt;jt AT yadutaf DOT fr&amp;gt;

# Get PulseAudio for the sound, Firefox for, well, you know...
RUN apt-get update &amp;amp;&amp;amp; apt-get -y install firefox pulseaudio

ENTRYPOINT [&amp;quot;firefox&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Build and run it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker build -t firefox .
$ docker run --rm -it \
    -v /tmp/.X11-unix:/tmp/.X11-unix \
    -v /run/user/$UID/pulse/native:/run/pulse \
    -e DISPLAY=unix$DISPLAY \
    -e PULSE_SERVER=unix:/run/pulse \
    --name firefox \
    firefox --new-instance &amp;quot;https://www.youtube.com/watch?v=k1-TrAvp_xs&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What it does is:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;share the X11 socket&lt;/li&gt;
&lt;li&gt;share the user&amp;rsquo;s pulseaudio socket as root&amp;rsquo;s&lt;/li&gt;
&lt;li&gt;expose them via environment variables&lt;/li&gt;
&lt;li&gt;start it!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As a (desirable) side-effect, setting docker daemon with user namespaces effectively disables a variety of security sensitive options like starting privileged containers or sharing the host&amp;rsquo;s network. This extra-security comes with the kernel&amp;rsquo;s implementation and we&amp;rsquo;ll certainly not refuse it!&lt;/p&gt;

&lt;p&gt;Of course, this has limitations. For example, if you try with chrome, you&amp;rsquo;ll be disappointed to realize there is no sound. This is because chrome requires the older Alsa sound system which are only accessible to the &amp;ldquo;audio&amp;rdquo; group. But this group is not and can&amp;rsquo;t be mapped in Docker just yet. This is supported by the kernel though. Just not Docker. By the way, if you want to test out chrome, make sure to add the &lt;code&gt;--disable-setuid-sandbox&lt;/code&gt; flag&lt;/p&gt;

&lt;p&gt;This limitation aside, this is fairly interesting. Using similar setups, you can have docker on your host, exploit most of it power, without ever taking the risk to compromise your security or integrity.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How I shrunk a Docker image by 98.8% – featuring fanotify</title>
      <link>http://blog.yadutaf.fr/2015/04/25/how-i-shrunk-a-docker-image-by-98-8-featuring-fanotify/</link>
      <pubDate>Sat, 25 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://blog.yadutaf.fr/2015/04/25/how-i-shrunk-a-docker-image-by-98-8-featuring-fanotify/</guid>
      <description>

&lt;p&gt;Some weeks ago, I did an internal presentation on Docker. During the presentation, one of the ops asked an seemingly trivial question: Is there anything like a &amp;#8220;diet program for Docker Images&amp;#8221; ?&lt;/p&gt;

&lt;p&gt;You can find a couple of pretty decent common-sense powered approach &lt;a href=&#34;https://intercityup.com/blog/downsizing-docker-containers.html&#34;&gt;on the web&lt;/a&gt; like removing well known cache folders, temporary files, installing all superfluous packages and flatten layers if not the full image. There is also the &lt;code&gt;-slim&lt;/code&gt; declination of the official language images.&lt;/p&gt;

&lt;p&gt;But, thinking at it, do we &lt;em&gt;really&lt;/em&gt; need a full consistent base Linux install? Which files do we &lt;em&gt;really&lt;/em&gt; need in a given image? I found a radical and pretty efficient approaches with a go binary. It was statically build, almost no external dependency. &lt;a href=&#34;http://blog.codeship.com/building-minimal-docker-containers-for-go-applications/&#34;&gt;Resulting image&lt;/a&gt;: 6.12MB.&lt;/p&gt;

&lt;p&gt;Whaou! Is there any chance to do something comparable, deterministic with any random application?&lt;/p&gt;

&lt;p&gt;It turns out there could be one. The idea is simple: We could profile the image at run time one way or another to determine which files are ever accessed/opened/&amp;#8230;, then remove all the remaining files. Hmm, sounds promising. Let&amp;rsquo;s PoC it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Target definition&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Start image&lt;/strong&gt;: Ubuntu (~200MB)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Application that MUST run&lt;/strong&gt;: &lt;code&gt;/bin/ls&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Build the smallest possible image&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;/bin/ls&lt;/code&gt; is a good target: It is simple enough for a PoC with no nasty behavior but still not trivial, it uses dynamic linking.&lt;/p&gt;

&lt;p&gt;Now that we have a target, let&amp;rsquo;s pick a tool. As this is a proof of concept, using dynamites where a hole puncher would  be enough &lt;em&gt;IS&lt;/em&gt; an option, as long as it does the job.&lt;/p&gt;

&lt;p&gt;The base idea it to record all file accesses. Be it a stat or a open. There are a couple of good candidates to help with the task. We could use &lt;a href=&#34;http://linux.die.net/man/7/inotify&#34; title=&#34;Man Inotify&#34;&gt;inotify&lt;/a&gt; but it is a pain to setup and watches needs to be attached on every single files, which potentially mean a *lot* of watches. We could use LD_PRELOAD but 1/ it&amp;rsquo;s no fun to use, 2/ it won&amp;rsquo;t catch direct syscalls 3/ it won&amp;rsquo;t work with statically linked programs (who said golang&amp;rsquo;s?). A solution that would work well even for statically linked program would be to use &lt;a href=&#34;http://linux.die.net/man/2/ptrace&#34; title=&#34;Man ptrace&#34;&gt;ptrace&lt;/a&gt; to trace all syscalls, in realtime. It is also a pain to setup but, it would be a reliable and flexible option. A lesser known linux syscall is &lt;a href=&#34;http://man7.org/linux/man-pages/man7/fanotify.7.html&#34;&gt;fanotify&lt;/a&gt;. As the title suggests, This is the one we&amp;rsquo;ll go with&lt;sup&gt;1&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;fanotify&lt;/code&gt; syscall has originally been implemented as &amp;#8220;decent&amp;#8221; mechanism for anti-virus vendors to intercept file access events, potentially on a whole mountpoint at once. Sounds familiar? While it may be used to deny file accesses, it may also just report file access events in a non-blocking fashion, potentially dropping&lt;sup&gt;2&lt;/sup&gt; events if the kernel queue overflows. In this last case, a special message will be generated to notify user-land listener about the message loss. This is perfectly what I needed. Non intrusive, a whole mountpoint at once, simple setup (well, provided that you find the documentation, no comment&amp;#8230;). This may seem anecdotal but it has its importance, as a learned after.&lt;/p&gt;

&lt;p&gt;Using it is fairly simple:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1/ Init &lt;code&gt;fanotify&lt;/code&gt; in &lt;code&gt;FAN_CLASS_NOTIF&lt;/code&gt;ication mode using &lt;a href=&#34;http://man7.org/linux/man-pages/man2/fanotify_init.2.html&#34;&gt;&lt;code&gt;fanotify_init&lt;/code&gt; syscall&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre class=&#34;brush: cpp; title: ; notranslate&#34; title=&#34;&#34;&gt;// Open ``fan`` fd for fanotify notifications. Messages will embed a 
// filedescriptor on accessed file. Expect it to be read-only
fan = fanotify_init(FAN_CLASS_NOTIF, O_RDONLY);
&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;2/ Subscribe to &lt;code&gt;FAN_ACCESS&lt;/code&gt; and &lt;code&gt;FAN_OPEN&lt;/code&gt; events on &amp;#8220;/&amp;#8221; &lt;code&gt;FAN_MARK_MOUNT&lt;/code&gt;point using &lt;a href=&#34;http://man7.org/linux/man-pages/man2/fanotify_mark.2.html&#34;&gt;&lt;code&gt;fanotify_mark&lt;/code&gt; syscall&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre class=&#34;brush: cpp; title: ; notranslate&#34; title=&#34;&#34;&gt;// Watch open/access events on root mountpoint
fanotify_mark(
    fan, 
    FAN_MARK_ADD | FAN_MARK_MOUNT, // Add mountpoint mark to fan
    FAN_ACCESS | FAN_OPEN,         // Report open and access events, non blocking
    -1, &#34;/&#34;                        // Watch root mountpoint (-1 is ignored for FAN_MARK_MOUNT type calls)
);
&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;3/ read&lt;/code&gt; pending event messages from the filedescriptor returned by &lt;code&gt;fanotify_init&lt;/code&gt; and iterate using &lt;code&gt;FAN_EVENT_NEXT&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre class=&#34;brush: cpp; title: ; notranslate&#34; title=&#34;&#34;&gt;// Read pending events from ``fan`` into ``buf``
buflen = read(fan, buf, sizeof(buf));

// Position cursor on first message
metadata = (struct fanotify_event_metadata*)&amp;buf;

// Loop until we reached the last event
while(FAN_EVENT_OK(metadata, buflen)) {
    // Do something interesting with the notification
    // ``metadata-&amp;gt;fd`` will contain a valid, RO fd to accessed file.

    // Close opened fd, otherwise we&#39;ll quickly exhaust the fd pool.
    close(metadata-&amp;gt;fd);

    // Move to next event in buffer
    metadata = FAN_EVENT_NEXT(metadata, buflen);
}
&lt;/pre&gt;

&lt;p&gt;Putting it all together, we&amp;rsquo;ll print the full name of all accessed files and add queue overflow detection. This should be plain enough for us (comments and error checks stripped for the purpose of this illustration):&lt;/p&gt;

&lt;pre class=&#34;brush: cpp; title: ; notranslate&#34; title=&#34;&#34;&gt;#include &amp;lt;fcntl.h&amp;gt;
#include &amp;lt;limits.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;sys/fanotify.h&amp;gt;

int main(int argc, char** argv) {
    int fan;
    char buf[4096];
    char fdpath[32];
    char path[PATH_MAX + 1];
    ssize_t buflen, linklen;
    struct fanotify_event_metadata *metadata;

    // Init fanotify structure
    fan = fanotify_init(FAN_CLASS_NOTIF, O_RDONLY);

    // Watch open/access events on root mountpoint
    fanotify_mark(
        fan,
        FAN_MARK_ADD | FAN_MARK_MOUNT,
        FAN_ACCESS | FAN_OPEN,
        -1, &#34;/&#34;
    );

    while(1) {
        buflen = read(fan, buf, sizeof(buf));
        metadata = (struct fanotify_event_metadata*)&amp;buf;

        while(FAN_EVENT_OK(metadata, buflen)) {
            if (metadata-&amp;gt;mask &amp; FAN_Q_OVERFLOW) {
                printf(&#34;Queue overflow!\n&#34;);
                continue;
            }

            // Resolve path, using automatically opened fd
            sprintf(fdpath, &#34;/proc/self/fd/%d&#34;, metadata-&amp;gt;fd);
            linklen = readlink(fdpath, path, sizeof(path) - 1);
            path[linklen] = &#39;&amp;#92;&amp;#48;&#39;;
            printf(&#34;%s\n&#34;, path);

            close(metadata-&amp;gt;fd);
            metadata = FAN_EVENT_NEXT(metadata, buflen);
        }
    }
}
&lt;/pre&gt;

&lt;p&gt;To build it, use:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;gcc main.c --static -o fanotify-profiler
&lt;/pre&gt;

&lt;p&gt;We basically now have a tool to report any file access on the active &amp;#8216;/&amp;rsquo; mountpoint in real time. Good.&lt;/p&gt;

&lt;p&gt;What now? Let&amp;rsquo;s create an Ubuntu container, start the recorder and run &lt;code&gt;/bin/ls&lt;/code&gt;. &lt;code&gt;fanotify&lt;/code&gt; requires require the &amp;#8220;&lt;code&gt;CAP_SYS_ADMIN&lt;/code&gt;&amp;#8221; capability. This is basically the &amp;#8220;catch-all&amp;#8221; root &lt;a href=&#34;http://linux.die.net/man/7/capabilities&#34;&gt;capability&lt;/a&gt;. Still better than running in &lt;code&gt;--privileged&lt;/code&gt; mode though.&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;# Run image
docker run --name profiler_ls \
           --volume $PWD:/src \
           --cap-add SYS_ADMIN \
           -it ubuntu /src/fanotify-profiler

# Run the command to profile, from another shell
docker exec -it profiler_ls ls

# Interrupt Running image using
docker kill profiler_ls # You know, the &#34;dynamite&#34;
&lt;/pre&gt;

&lt;p&gt;This should produce an output like:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;/etc/passwd
/etc/group
/etc/passwd
/etc/group
/bin/ls
/bin/ls
/bin/ls
/lib/x86_64-linux-gnu/ld-2.19.so
/lib/x86_64-linux-gnu/ld-2.19.so
/etc/ld.so.cache
/lib/x86_64-linux-gnu/libselinux.so.1
/lib/x86_64-linux-gnu/libacl.so.1.1.0
/lib/x86_64-linux-gnu/libc-2.19.so
/lib/x86_64-linux-gnu/libc-2.19.so
/lib/x86_64-linux-gnu/libpcre.so.3.13.1
/lib/x86_64-linux-gnu/libdl-2.19.so
/lib/x86_64-linux-gnu/libdl-2.19.so
/lib/x86_64-linux-gnu/libattr.so.1.1.0
&lt;/pre&gt;

&lt;p&gt;Awesome! It worked. We now know for sure what &lt;code&gt;/bin/ls&lt;/code&gt; ultimately needs to run.&lt;/p&gt;

&lt;p&gt;So we&amp;rsquo;ll just copy-paste-import all this in a &amp;#8220;&lt;code&gt;FROM scratch&lt;/code&gt;&amp;#8221; Docker Image and we&amp;rsquo;ll be done. Easy. Well, not so. But let&amp;rsquo;s do it to see by ourselves.&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;# Export base docker image
mkdir ubuntu_base
docker export profiler_ls | sudo tar -x -C ubuntu_base

# Create new image
mkdir ubuntu_lean

# Get the linker (trust me)
sudo mkdir -p ubuntu_lean/lib64
sudo cp -a ubuntu_base/lib64/ld-linux-x86-64.so.2 ubuntu_lean/lib64/

# Copy the files
sudo mkdir -p ubuntu_lean/etc
sudo mkdir -p ubuntu_lean/bin
sudo mkdir -p ubuntu_lean/lib/x86_64-linux-gnu/

sudo cp -a ubuntu_base/bin/ls ubuntu_lean/bin/ls
sudo cp -a ubuntu_base/etc/group ubuntu_lean/etc/group
sudo cp -a ubuntu_base/etc/passwd ubuntu_lean/etc/passwd
sudo cp -a ubuntu_base/etc/ld.so.cache ubuntu_lean/etc/ld.so.cache
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/ld-2.19.so ubuntu_lean/lib/x86_64-linux-gnu/ld-2.19.so
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/ld-2.19.so ubuntu_lean/lib/x86_64-linux-gnu/ld-2.19.so
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/libselinux.so.1 ubuntu_lean/lib/x86_64-linux-gnu/libselinux.so.1
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/libacl.so.1.1.0 ubuntu_lean/lib/x86_64-linux-gnu/libacl.so.1.1.0
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/libc-2.19.so ubuntu_lean/lib/x86_64-linux-gnu/libc-2.19.so
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/libpcre.so.3.13.1 ubuntu_lean/lib/x86_64-linux-gnu/libpcre.so.3.13.1
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/libdl-2.19.so ubuntu_lean/lib/x86_64-linux-gnu/libdl-2.19.so
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/libattr.so.1.1.0 ubuntu_lean/lib/x86_64-linux-gnu/libattr.so.1.1.0

# Import it back to Docker
cd ubuntu_lean
sudo tar -c . | docker import - ubuntu_lean
&lt;/pre&gt;

&lt;p&gt;Run the resulting image:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;docker run --rm -it ubuntu_lean /bin/ls
&lt;/pre&gt;

&lt;p&gt;And, Tadaaaaa:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;# If you did not trust me with the linker (as it was already loaded when the profiler started, it does not show in the ouput)
no such file or directoryFATA[0000] Error response from daemon: Cannot start container f318adb174a9e381500431370a245275196a2948828919205524edc107626d78: no such file or directory

# Otherwise
/bin/ls: error while loading shared libraries: libacl.so.1: cannot open shared object file: No such file or directory
&lt;/pre&gt;

&lt;p&gt;Well, not so&amp;#8230; What went wrong? Remember when I said this syscall was primarily designed with antivirus in mind? The real-time part of the antivirus is supposed to detect that a file is being accessed, run some checks, take a decision. What matters here is the actual, real content of the file. In particular, filesystem races MUST be avoided at all costs. This is the reason why &lt;code&gt;fanotify&lt;/code&gt; yields filedescriptors instead of accesses path. Determining the underlying physical file is done by probing &lt;code&gt;/proc/self/fd/[fd]&lt;/code&gt;. It does not tell you through which symlink the file being accessed was accessed, only what file it is.&lt;/p&gt;

&lt;p&gt;To make this work, we need to find all links to reported files and install them in the filtered image as well. A &lt;code&gt;find&lt;/code&gt; command like this will do the job:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;# Find all files refering to a given one
find -L -samefile &#34;./lib/x86_64-linux-gnu/libacl.so.1.1.0&#34; 2&amp;gt;/dev/null

# If you want to exclude the target itself from the results
find -L -samefile &#34;./lib/x86_64-linux-gnu/libacl.so.1.1.0&#34; -a ! -path &#34;./lib/x86_64-linux-gnu/libacl.so.1.1.0&#34; 2&amp;gt;/dev/null
&lt;/pre&gt;

&lt;p&gt;This can easily be automated with a loop like:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;for f in $(cd ubuntu_lean; find)
do 
    (
        cd ubuntu_base
        find -L -samefile &#34;$f&#34; -a ! -path &#34;$f&#34;
    ) 2&amp;gt;/dev/null
done
&lt;/pre&gt;

&lt;p&gt;Which produces the list of missing symlinks. All libs.&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;./lib/x86_64-linux-gnu/libc.so.6
./lib/x86_64-linux-gnu/ld-linux-x86-64.so.2
./lib/x86_64-linux-gnu/libattr.so.1
./lib/x86_64-linux-gnu/libdl.so.2
./lib/x86_64-linux-gnu/libpcre.so.3
./lib/x86_64-linux-gnu/libacl.so.1
&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s copy them too from the source image and re-create the destination image. (Yeah, could also have created them on the fly).&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;# Copy the links
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/libc.so.6 ubuntu_lean/lib/x86_64-linux-gnu/libc.so.6
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2 ubuntu_lean/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/libdl.so.2 ubuntu_lean/lib/x86_64-linux-gnu/libdl.so.2
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/libpcre.so.3 ubuntu_lean/lib/x86_64-linux-gnu/libpcre.so.3
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/libacl.so.1 ubuntu_lean/lib/x86_64-linux-gnu/libacl.so.1
sudo cp -a ubuntu_base/lib/x86_64-linux-gnu/libattr.so.1 ubuntu_lean/lib/x86_64-linux-gnu/libattr.so.1

# Import it back to Docker
cd ubuntu_lean
docker rmi -f ubuntu_lean; sudo tar -c . | docker import - ubuntu_lean
&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: This method is limited. For example, it won&amp;rsquo;t return links to links to files neither absolute links. The later requiring at least a chroot. Or to be run in the source container itself, provided that find or equivalent is present.&lt;/p&gt;

&lt;p&gt;Run the resulting image:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;docker run --rm -it ubuntu_lean /bin/ls
&lt;/pre&gt;

&lt;p&gt;And, Tadaaaaa:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;bin  dev  etc  lib  lib64  proc  sys
&lt;/pre&gt;

&lt;p&gt;It works! &lt;sup&gt;tm&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Time is over, let&amp;rsquo;s measure:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;ubuntu&lt;/strong&gt;: 209M&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;ubuntu_lean&lt;/strong&gt;: 2,5M&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Resulting Docker image is 83.5 &lt;em&gt;times&lt;/em&gt; smaller&lt;sup&gt;3&lt;/sup&gt;. That&amp;rsquo;s a 98.8% reduction. Looks good to me, I&amp;rsquo;ll accept it. If you agree.&lt;/p&gt;

&lt;h3 id=&#34;last-thought:ed6b7f79feba98b85e08740dab416986&#34;&gt;Last Thought&lt;/h3&gt;

&lt;p&gt;Like all profiling based method, it will only tell you about what&amp;rsquo;s actually done/used in a specific scenario. For example, try to run &lt;code&gt;/bin/ls -l&lt;/code&gt; in the resulting image and see by yourself. (spoiler: it does not work. Well it does, but not as expected).&lt;/p&gt;

&lt;p&gt;The profiling technique itself is not without flaws. It does not detect how a file was opened but only which file this is. This is a problem for symlinks, especially cross-filesytems (read: cross-volumes). With fanotify, we&amp;rsquo;ll completely miss the original symlink and break the application.&lt;/p&gt;

&lt;p&gt;If I were to build a production shrinker, I would probably go for a &lt;code&gt;ptrace&lt;/code&gt; based method.&lt;/p&gt;

&lt;h3 id=&#34;footnotes:ed6b7f79feba98b85e08740dab416986&#34;&gt;Footnotes&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Let&amp;rsquo;s face the truth: What I really wanted, was experimenting with this syscall. Docker images are more of a (good) pretext.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Actually, one could use &lt;code&gt;FAN_UNLIMITED_QUEUE&lt;/code&gt; well calling &lt;code&gt;fanotify_init&lt;/code&gt; to remove this limitation, provided that the calling process is at least &lt;code&gt;CAP_SYS_ADMIN&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;That&amp;rsquo;s also 2.4 times smaller that the 6.13MB image I mentioned at the beginning of this post. But the comparison is not fair.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>How to run Docker behind an Nginx reverse proxy</title>
      <link>http://blog.yadutaf.fr/2014/12/12/how-to-run-docker-behind-an-nginx-reverse-proxy/</link>
      <pubDate>Fri, 12 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>http://blog.yadutaf.fr/2014/12/12/how-to-run-docker-behind-an-nginx-reverse-proxy/</guid>
      <description>

&lt;p&gt;A couple of weeks ago, I wanted to run some experiment to see how Docker could run in a cloud / shared hosting like environment. In the mean time, Docker released version 1.4 bringing additional security/authentication and Docker machine to automate the process of creating and running a remote Docker instance.&lt;/p&gt;

&lt;p&gt;Shared hosting farms are usually built around some kind of public gateway for incoming/outgoing traffic as well as management traffic including FTP and SSH. Te largest part of the farm - not unlike an iceberg - being &amp;#8220;hidden&amp;#8221; in a private network behind these gateways.&lt;/p&gt;

&lt;p&gt;So, my question was, is there any way we can imagine that could enable a similar gateway behavior with Docker, including multi-tenancy support and all features you&amp;rsquo;d expect?&lt;/p&gt;

&lt;p&gt;It turns out, there is.&lt;/p&gt;

&lt;p&gt;Docker binary can actually play up to 3 roles:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Docker Command line -&amp;gt; the one making it shiny and plain awesome&lt;/li&gt;
&lt;li&gt;Docker Daemon -&amp;gt; the one behind the scenes doing most of the hard work&lt;/li&gt;
&lt;li&gt;Docker init -&amp;gt; the one behind the one behind the scenes doing the early container setup&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The command line and and daemon talk together using a &lt;em&gt;&lt;strong&gt;mostly&lt;/strong&gt;&lt;/em&gt; HTTP based protocol. I say &amp;#8220;mostly&amp;#8221; because the a couple of API endpoints &amp;#8216;hijack&amp;rsquo; the connection, notably the &lt;code&gt;container/attach&lt;/code&gt; endpoint, also known as &amp;#8220;forward my container&amp;rsquo;s console.&amp;#8221;&lt;/p&gt;

&lt;p&gt;Knowing that, a common setup, already well covered by blog posts around the web, recommend to setup an &lt;code&gt;NGinx&lt;/code&gt; reverse proxy and add basic authentication for the security.&lt;/p&gt;

&lt;p&gt;Sadly, there are 2 downsides with this approach:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Stock Docker client does not &amp;#8220;speak&amp;#8221; HTTP basic authentication&lt;/li&gt;
&lt;li&gt;Stock Nginx is completely lost when Docker hijacks the connection&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Regarding the authentication issue, I recommend to rather rely on Docker TLS certificate as they are supported out of the box. Then, using some LUA magic, we could use them as &amp;#8220;public keys&amp;#8221; to balance to the appropriate. This would in itself a good subject for a dedicated post.&lt;/p&gt;

&lt;p&gt;How do we deal with the second point, namely, Nginx being lost?&lt;/p&gt;

&lt;p&gt;Once the mechanism behind the &amp;#8220;hijack&amp;#8221; is well identified, things quickly becomes straight forward: A usual HTTP connection could be seen as &amp;#8220;half-duplex&amp;#8221; network. One peer talks and, when it is done, the other peer can talk and so on, using a well known protocol. When doing a docker attach, Docker uses the raw TCP connection in &amp;#8220;full duplex&amp;#8221; mode, any peer can talk whenever they have something to say. This is why reverse proxies are lost: they expect - and rely - a lot on the HTTP protocol being well respected.&lt;/p&gt;

&lt;p&gt;Interestingly, there is another mainstream protocol doing just this. As it turns out, this standard protocol is so popular that it has been integrated in Nginx years ago. I named &lt;code&gt;WebSocket&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;So, basically, the idea is to teach Nginx how to handle Docker&amp;rsquo;s custom protocol just as it does with websockets. Here is the patch:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;--- a/src/http/ngx_http_upstream.c Tue Nov 04 19:56:23 2014 +0900
+++ b/src/http/ngx_http_upstream.c  Sat Nov 15 16:21:58 2014 +0100
@@ -89,6 +89,8 @@
     ngx_table_elt_t *h, ngx_uint_t offset);
 static ngx_int_t ngx_http_upstream_process_content_length(ngx_http_request_t *r,
     ngx_table_elt_t *h, ngx_uint_t offset);
+static ngx_int_t ngx_http_upstream_process_content_type(ngx_http_request_t *r,
+    ngx_table_elt_t *h, ngx_uint_t offset);
 static ngx_int_t ngx_http_upstream_process_last_modified(ngx_http_request_t *r,
     ngx_table_elt_t *h, ngx_uint_t offset);
 static ngx_int_t ngx_http_upstream_process_set_cookie(ngx_http_request_t *r,
@@ -175,7 +177,7 @@
                  ngx_http_upstream_copy_header_line, 0, 0 },

     { ngx_string(&#34;Content-Type&#34;),
-                 ngx_http_upstream_process_header_line,
+                 ngx_http_upstream_process_content_type,
                  offsetof(ngx_http_upstream_headers_in_t, content_type),
                  ngx_http_upstream_copy_content_type, 0, 1 },

@@ -2716,6 +2718,7 @@
     u-&amp;gt;write_event_handler = ngx_http_upstream_upgraded_write_upstream;
     r-&amp;gt;read_event_handler = ngx_http_upstream_upgraded_read_downstream;
     r-&amp;gt;write_event_handler = ngx_http_upstream_upgraded_write_downstream;
+    u-&amp;gt;headers_in.chunked = 0;

     if (clcf-&amp;gt;tcp_nodelay) {
         tcp_nodelay = 1;
@@ -3849,6 +3852,25 @@

 static ngx_int_t
+ngx_http_upstream_process_content_type(ngx_http_request_t *r, ngx_table_elt_t *h,
+    ngx_uint_t offset)
+{
+    ngx_int_t ret = ngx_http_upstream_process_header_line(r, h, offset);
+    if (ret != NGX_OK) {
+        return ret;
+    }
+
+    // is docker header ?
+    if (ngx_strstrn(h-&amp;gt;value.data,
+                    &#34;application/vnd.docker.raw-stream&#34;, 34 - 1) != NULL) {
+        r-&amp;gt;upstream-&amp;gt;upgrade = 1;
+    }
+
+    return NGX_OK;
+}
+
+
+static ngx_int_t
 ngx_http_upstream_process_last_modified(ngx_http_request_t *r,
     ngx_table_elt_t *h, ngx_uint_t offset)
 {
1

The only remaining step is then to configure the reverse proxy, as usual. This should be easy 😉

Just for the record, here is my test &amp;lt;code&amp;gt;nginx.conf&amp;lt;/code&amp;gt;:

1
worker_processes  1;

events {
    worker_connections  1024;
}

http {
    include       mime.types;
    default_type  application/octet-stream;

    sendfile        on;

    keepalive_timeout  65;

    server {
        listen 9000;

        location / {
            proxy_buffering off;
            proxy_pass http://localhost:8080;
        }
    }
}
&lt;/pre&gt;

&lt;p&gt;You just need to run Docker on port 8080 with a command like the following or just add your params to &lt;code&gt;/etc/default/docker&lt;/code&gt;&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;docker -d -H tcp://localhost:8080&lt;/pre&gt;

&lt;p&gt;And we&amp;rsquo;re done!&lt;/p&gt;

&lt;h3 id=&#34;final-thought:d5aecd33c8f89018de5951b27bb1e669&#34;&gt;Final thought&lt;/h3&gt;

&lt;p&gt;While hacking this, I noticed that all Nginx needs to switch protocols for websockets was proper HTTP Headers:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;# Request
Connection: Upgrade
Upgrade: websocket

# Response
HTTP/1.1 101 Upgraded
Connection: Upgrade
Upgrade: websocket
&lt;/pre&gt;

&lt;p&gt;So that another approach could be to inject proper headers in Docker protocol.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Getting Docker to run on Power8</title>
      <link>http://blog.yadutaf.fr/2014/10/28/getting-docker-to-run-on-power8/</link>
      <pubDate>Tue, 28 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>http://blog.yadutaf.fr/2014/10/28/getting-docker-to-run-on-power8/</guid>
      <description>

&lt;p&gt;Last Week-End, I wanted to play around with Docker on a &lt;a href=&#34;http://en.wikipedia.org/wiki/POWER8&#34;&gt;Power8 processor&lt;/a&gt;. Unfortunately, there no &amp;#8220;ready-to-use&amp;#8221; build available (yet) and Go support is still quite rough. Anyway, I love challenges and the process was eased a lot by the work of &lt;a href=&#34;http://dave.cheney.net/&#34;&gt;Dave Cheney&lt;/a&gt; from Canonical who did the hard work of &lt;a href=&#34;http://go-talks.appspot.com/github.com/davecheney/gosyd/gccgo.slide#1&#34;&gt;porting the go command line to Power8&lt;/a&gt; and IBM&amp;rsquo;s who is working with Docker to bring necessary fixes to gccgo.&lt;/p&gt;

&lt;p&gt;[UPDATE 2014-11-19]: IBM is currently porting Docker to gccgo/Power8, see the comments below for more informations.&lt;/p&gt;

&lt;p&gt;Power8 is the name of a 64bits RISC processor micro-architecture of the same family as the G5 for example. This was the processor powering the venerable Mac G5. It is extremely parallel with up to 8 threads per core. This makes it especially good at running databases. Notably, &lt;a href=&#34;https://www.flamingspork.com/blog/2014/06/03/1-million-sql-queries-per-second-mysql-5-7-on-power8/&#34;&gt;Stewart Smith tuned MySQL 7 to get up to 1M request per seconds&lt;/a&gt;. This is just amazing!&lt;/p&gt;

&lt;p&gt;Docker is a tool helping developers to build, ship and run code anywhere just like containers helps shipping anything anywhere. It is increasingly used in production to cleanly isolate processes on a same physical machine without the overhead of a Virtual Machine.&lt;/p&gt;

&lt;p&gt;So, let&amp;rsquo;s get started. My goal was to get docker running and, if possible the latest version (it turns out it actually **is** the latest version). The goal was not to make it the shiniest way. That&amp;rsquo;s for later.&lt;/p&gt;

&lt;p&gt;Here is the state of the art:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Docker depends on Go and cgo 1.2.1 until version 1.1.1&lt;/li&gt;
&lt;li&gt;Docker depends on Go and cgo 1.3+ after then&lt;/li&gt;
&lt;li&gt;gccgo 4.9, shipped with Ubuntu 14.04 supports go 1.2.1 but lacks some reflexivity implementation for Power8 and Elf parsing for Power8 in libcgo&lt;/li&gt;
&lt;li&gt;gccgo trunk supports go 1.4 (yes), fixes the reflexivity but still lacks the Elf parsing&lt;/li&gt;
&lt;li&gt;golang 1.3 has no support for Power8&lt;/li&gt;
&lt;li&gt;golang dev.power64 is still very work in progress but supports ELF parsing for Power8 (hint, hint)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As you can see, this is not &lt;span class=&#34;span9&#34;&gt;attempting to square the circle but not so close.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;It is also worth noting that gccgo is only the compiler parts. It brings no support for the &amp;#8220;go&amp;#8221; command line itself (which is written in pure go) neither for cgo (which bridges the gap between Go and C worlds). Fortunately, Dave Cheney, of Canonical, did the hard work of getting &amp;#8220;go&amp;#8221; to build with gccgo and in turn seamlessly work with gccgo backend by default. His work is now available through &amp;#8216;apt-get&amp;rsquo;. He also did a great presentation of his work which is available online &lt;a href=&#34;http://go-talks.appspot.com/github.com/davecheney/gosyd/gccgo.slide&#34;&gt;http://go-talks.appspot.com/github.com/davecheney/gosyd/gccgo.slide&lt;/a&gt;. And, honestly, after a full week-end battling to get it right, I totally share his opinions when he writes &amp;#8220;ʕ╯◔ϖ◔ʔ╯︵ ┻━┻&amp;#8221;.&lt;/p&gt;

&lt;p&gt;Among the discarded, aborted, failed attempts: cross compile from my laptop, find ready to use instructions, use stock gcc 4.9, build dev.power64 Go branch (it&amp;rsquo;s completely broken / Work in progress), fly a unicorn.&lt;/p&gt;

&lt;p&gt;Anyway, let&amp;rsquo;s start over. What we&amp;rsquo;ll do:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;get a Power8 machine. No cross build sorry.&lt;/li&gt;
&lt;li&gt;grab latest version of GCC from trunk (SVN, that&amp;rsquo;s 1 VCS)&lt;/li&gt;
&lt;li&gt;grab latest WIP version of Power8 from dev.power64 (Mercurial, that&amp;rsquo;s a 2nd VCS)&lt;/li&gt;
&lt;li&gt;copy required bits from go to gccgo, namely the ELF parser of libcgo&lt;/li&gt;
&lt;li&gt;patch, build and install gccgo in /opt/gcc-trunk&lt;/li&gt;
&lt;li&gt;build &amp;#8220;go&amp;#8221; and &amp;#8220;cgo&amp;#8221; commands to use our updated libgo.so.6 instead of libgo.so.5&lt;/li&gt;
&lt;li&gt;grab lastest version of Docker from master (Git, that&amp;rsquo;s a 3rd VCS)&lt;/li&gt;
&lt;li&gt;patch, build, install Docker&lt;/li&gt;
&lt;li&gt;celebrate&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;1-get-a-power8-machine:843e6337e67e60d94f17e70050c565c9&#34;&gt;1. Get a Power8 Machine&lt;/h3&gt;

&lt;p&gt;The easiest way to get one is to &lt;a href=&#34;http://labs.runabove.com/power8/&#34;&gt;join RunAbove&amp;rsquo;s public beta&lt;/a&gt; which comes with a $32 Voucher. That&amp;rsquo;s one month worth of Power8.&lt;/p&gt;

&lt;p&gt;Common setup:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;sudo locale-gen
sudo apt-get -y update
sudo apt-get -y install subversion mercurial git build-essential gccgo-go
&lt;/pre&gt;

&lt;h3 id=&#34;2-grab-gcc:843e6337e67e60d94f17e70050c565c9&#34;&gt;2. Grab GCC&lt;/h3&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;cd
svn checkout svn://gcc.gnu.org/svn/gcc/trunk gcc
# Be *very* patient
&lt;/pre&gt;

&lt;h3 id=&#34;3-grab-go-dev-power64:843e6337e67e60d94f17e70050c565c9&#34;&gt;3. Grab Go dev.power64&lt;/h3&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;cd
hg clone -u release https://code.google.com/p/go
cd go
hg update dev.power64
&lt;/pre&gt;

&lt;h3 id=&#34;4-patch-gcc:843e6337e67e60d94f17e70050c565c9&#34;&gt;4. Patch GCC&lt;/h3&gt;

&lt;p&gt;GCC&amp;rsquo;s libcgo implementation lakes elf parsing supporting for PPC64 instruction set. As this is required by &lt;code&gt;cgo&lt;/code&gt;, we&amp;rsquo;ll get it from Go itself.&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;cd
cp go/src/debug/elf/file.go gcc/libgo/go/debug/elf/
cp go/src/debug/elf/elf.go gcc/libgo/go/debug/elf/
&lt;/pre&gt;

&lt;p&gt;It also lacks some termios related symbols required to build docker command line interface. They&amp;rsquo;re easily added with this patch (extracted from `svn diff`):&lt;/p&gt;

&lt;p&gt;[UPDATE 2014-11-11]: This patch is no longer needed thanks to IBM&amp;rsquo;s upstream work.&lt;/p&gt;

&lt;pre class=&#34;brush: diff; title: ; notranslate&#34; title=&#34;&#34;&gt;--- libgo/mksysinfo.sh  (revision 216693)
+++ libgo/mksysinfo.sh  (working copy)
@@ -174,6 +174,15 @@
 #ifdef TIOCGWINSZ
   TIOCGWINSZ_val = TIOCGWINSZ,
 #endif
+#ifdef TIOCSWINSZ
+  TIOCSWINSZ_val = TIOCSWINSZ,
+#endif
+#ifdef TCGETS
+  TCGETS_val = TCGETS,
+#endif
+#ifdef TCSETS
+  TCSETS_val = TCSETS,
+#endif
 #ifdef TIOCNOTTY
   TIOCNOTTY_val = TIOCNOTTY,
 #endif
@@ -790,6 +799,21 @@
     echo &#39;const TIOCGWINSZ = _TIOCGWINSZ_val&#39; &amp;gt;&amp;gt; ${OUT}
   fi
 fi
+if ! grep &#39;^const TIOCSWINSZ&#39; ${OUT} &amp;gt;/dev/null 2&amp;gt;&amp;1; then
+  if grep &#39;^const _TIOCSWINSZ_val&#39; ${OUT} &amp;gt;/dev/null 2&amp;gt;&amp;1; then
+    echo &#39;const TIOCSWINSZ = _TIOCSWINSZ_val&#39; &amp;gt;&amp;gt; ${OUT}
+  fi
+fi
+if ! grep &#39;^const TCGETS&#39; ${OUT} &amp;gt;/dev/null 2&amp;gt;&amp;1; then
+  if grep &#39;^const _TCGETS_val&#39; ${OUT} &amp;gt;/dev/null 2&amp;gt;&amp;1; then
+    echo &#39;const TCGETS = _TCGETS_val&#39; &amp;gt;&amp;gt; ${OUT}
+  fi
+fi
+if ! grep &#39;^const TCSETS&#39; ${OUT} &amp;gt;/dev/null 2&amp;gt;&amp;1; then
+  if grep &#39;^const _TCSETS_val&#39; ${OUT} &amp;gt;/dev/null 2&amp;gt;&amp;1; then
+    echo &#39;const TCSETS = _TCSETS_val&#39; &amp;gt;&amp;gt; ${OUT}
+  fi
+fi
 if ! grep &#39;^const TIOCNOTTY&#39; ${OUT} &amp;gt;/dev/null 2&amp;gt;&amp;1; then
   if grep &#39;^const _TIOCNOTTY_val&#39; ${OUT} &amp;gt;/dev/null 2&amp;gt;&amp;1; then
     echo &#39;const TIOCNOTTY = _TIOCNOTTY_val&#39; &amp;gt;&amp;gt; ${OUT}
&lt;/pre&gt;

&lt;p&gt;If you&amp;rsquo;re planning on making a break, just wait one more minute. We&amp;rsquo;ll launch GCC&amp;rsquo;s build&amp;#8230;&lt;/p&gt;

&lt;h3 id=&#34;5-build-gcc:843e6337e67e60d94f17e70050c565c9&#34;&gt;5. Build GCC&lt;/h3&gt;

&lt;p&gt;As usual, except that we built it out of tree.&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;cd
mkdir build-gcc
cd build-gcc
sudo apt-get install -y libgmp-dev libmpfr-dev libmpc-dev flex bison
../gcc/configure --enable-languages=go --disable-multilib --prefix=/opt/gcc-trunk
make -j200 # if using the big instance
sudo make install
&lt;/pre&gt;

&lt;p&gt;Be patient, read a book, watch a movie, go visit friends&amp;#8230; It takes a while. On the &amp;#8216;S&amp;rsquo; instance, it took me around 98 minutes.&lt;/p&gt;

&lt;p&gt;Once done, we have some additional setup:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;export PATH=/opt/gcc-trunk/bin:$PATH
echo &#34;/opt/gcc-trunk/lib64&#34; | sudo tee /etc/ld.so.conf.d/gcc-trunk.conf
sudo ldconfig
&lt;/pre&gt;

&lt;h3 id=&#34;6-build-and-install-cgo:843e6337e67e60d94f17e70050c565c9&#34;&gt;6. Build (and install) CGO&lt;/h3&gt;

&lt;p&gt;Cgo is the component bridging the gap between Go and C world. It is notably required to build the devmapper driver of Docker.&lt;/p&gt;

&lt;p&gt;As we won&amp;rsquo;t attempt to build the full go toolchain (it does&amp;rsquo;nt work yet), we&amp;rsquo;ll need to patch &amp;#8220;gcc.go&amp;#8220; to insert `const defaultCC = &amp;#8220;gcc&amp;#8221;` near the top of the file.&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;cd go/src/cmd/cgo
go build
&lt;/pre&gt;

&lt;p&gt;You can now install it. It&amp;rsquo;s hackish but it does the job. But I still can&amp;rsquo;t figure out why I needed to copy the source files to `/usr/src/cmd/cgo`. Anyway, it&amp;rsquo;s working.&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;sudo mkdir -p /usr/pkg/tool/linux_ppc64
sudo mkdir -p /usr/src/cmd/cgo
sudo cp cgo /usr/pkg/tool/linux_ppc64/cgo
sudo cp * /usr/src/cmd/cgo
&lt;/pre&gt;

&lt;p&gt;One more thing: to let `go build` know we prepared to using cgo, we need to switch `CGO_ENABLED` environment variable on.&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;export CGO_ENABLED=1
&lt;/pre&gt;

&lt;h3 id=&#34;7-grab-docker-1-3-0:843e6337e67e60d94f17e70050c565c9&#34;&gt;7. Grab Docker 1.3.0&lt;/h3&gt;

&lt;p&gt;This is the last stable release at the time of writing. Let&amp;rsquo;s use it.&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;cd
git clone https://github.com/docker/docker.git
cd docker
git checkout v1.3.1
&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;ll also need to prepare a little the build environment:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;sudo mkdir -p /go/src/github.com/docker/
sudo ln -s $HOME/docker /go/src/github.com/docker/docker
export PATH=/opt/gcc-trunk/bin/:$PATH
export GOPATH=/go:/go/src/github.com/docker/docker/vendor
&lt;/pre&gt;

&lt;h3 id=&#34;8-build-docker:843e6337e67e60d94f17e70050c565c9&#34;&gt;8. Build Docker&lt;/h3&gt;

&lt;p&gt;Just issue &amp;#8216;docker build&amp;rsquo;. I&amp;rsquo;m kidding.&lt;/p&gt;

&lt;p&gt;This is the trickiest part of the job as all the full build systems assumes a working docker environment. So we&amp;rsquo;ll mostly emulate it.&lt;/p&gt;

&lt;p&gt;First, let&amp;rsquo;s apply a couple of patches.&lt;/p&gt;

&lt;p&gt;Remove a runtime (?!) check preventing Docker to run on non amd64 platforms:&lt;/p&gt;

&lt;pre class=&#34;brush: diff; title: ; notranslate&#34; title=&#34;&#34;&gt;diff --git a/daemon/daemon.go b/daemon/daemon.go
index 235788c..b75a94e 100644
--- a/daemon/daemon.go
+++ b/daemon/daemon.go
@@ -1104,9 +1104,9 @@ func (daemon *Daemon) ImageGetCached(imgID string, config *runconfig.Config) (*i
 
 func checkKernelAndArch() error {
    // Check for unsupported architectures
-   if runtime.GOARCH != &#34;amd64&#34; {
-       return fmt.Errorf(&#34;The Docker runtime currently only supports amd64 (not %s). This will change in the future. Aborting.&#34;, runtime.GOARCH)
-   }
+   //if runtime.GOARCH != &#34;amd64&#34; {
+   //  return fmt.Errorf(&#34;The Docker runtime currently only supports amd64 (not %s). This will change in the future. Aborting.&#34;, runtime.GOARCH)
+   //}
    // Check for unsupported kernel versions
    // FIXME: it would be cleaner to not test for specific versions, but rather
    // test for specific functionalities.
&lt;/pre&gt;

&lt;p&gt;Next, we need to workaround hard-coded references to official go compiler:&lt;/p&gt;

&lt;pre class=&#34;brush: diff; title: ; notranslate&#34; title=&#34;&#34;&gt;diff --git a/vendor/src/github.com/kr/pty/pty_linux.go b/vendor/src/github.com/kr/pty/pty_linux.go
index 6e5a042..8525f80 100644
--- a/vendor/src/github.com/kr/pty/pty_linux.go
+++ b/vendor/src/github.com/kr/pty/pty_linux.go
@@ -7,6 +7,11 @@ import (
    &#34;unsafe&#34;
 )
 
+type (
+        _C_int  int32
+        _C_uint uint32
+)
+
 var (
    ioctl_TIOCGPTN   = _IOR(&#39;T&#39;, 0x30, unsafe.Sizeof(_C_uint(0))) /* Get Pty Number (of pty-mux device) */
    ioctl_TIOCSPTLCK = _IOW(&#39;T&#39;, 0x31, unsafe.Sizeof(_C_int(0)))  /* Lock/unlock Pty */
&lt;/pre&gt;

&lt;p&gt;And, finally, change the link flags. Note that for some reason `-static` breaks network communication. It seems to be related to name resolution but I did not investigate further as dynamic linking works just fine.&lt;/p&gt;

&lt;pre class=&#34;brush: diff; title: ; notranslate&#34; title=&#34;&#34;&gt;diff --git a/hack/make/binary b/hack/make/binary
index b97069a..f5398ae 100755
--- a/hack/make/binary
+++ b/hack/make/binary
@@ -6,9 +6,8 @@ DEST=$1
 go build \
    -o &#34;$DEST/docker-$VERSION&#34; \
    &#34;${BUILDFLAGS[@]}&#34; \
-   -ldflags &#34;
-       $LDFLAGS
-       $LDFLAGS_STATIC_DOCKER
+   -gccgoflags &#34;
+       -static-libgo -static-libgcc
    &#34; \
    ./docker
 echo &#34;Created binary: $DEST/docker-$VERSION&#34;
&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s start to build. Most of the following steps are normally handled by the Dockerfile but&amp;#8230; we don&amp;rsquo;t have a working Docker yet.&lt;/p&gt;

&lt;p&gt;Grab the dependencies:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;sudo apt-get install -y \
        aufs-tools \
        automake \
        btrfs-tools \
        build-essential \
        curl \
        dpkg-sig \
        git \
        iptables \
        libapparmor-dev \
        libcap-dev \
        libsqlite3-dev \
        lxc=1.0* \
        mercurial \
        parallel \
        reprepro \
        ruby1.9.1 \
        ruby1.9.1-dev \
        s3cmd=1.1.0* \
        --no-install-recommends
&lt;/pre&gt;

&lt;p&gt;Docker needs a pretty recent devmapper build to run. Get it.&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;cd
git clone --no-checkout https://git.fedorahosted.org/git/lvm2.git
cd lvm2
git checkout -q v2_02_103
&lt;/pre&gt;

&lt;p&gt;Now we&amp;rsquo;ll hit an outdated file `config.guess`, overload it.&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;mkdir -p autoconf
wget &#39;http://git.savannah.gnu.org/gitweb/?p=config.git;a=blob_plain;f=config.guess;hb=HEAD&#39; -O autoconf/config.guess
&lt;/pre&gt;

&lt;p&gt;Build it:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;./configure --enable-static_link
make device-mapper
sudo make install_device-mapper
&lt;/pre&gt;

&lt;p&gt;Make sure you have the the ldconfig, PATH and CGO_ENABLED tricks then:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;cd
cd docker
./hack/make.sh binary
sudo cp /home/admin/docker/bundles/1.3.1/binary/docker-1.3.1 /usr/bin/docker
&lt;/pre&gt;

&lt;p&gt;And we&amp;rsquo;re done !&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using Docker to triage Nasty-Bugs(tm)</title>
      <link>http://blog.yadutaf.fr/2014/09/27/using-docker-to-triage-nasty-bugs/</link>
      <pubDate>Sat, 27 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://blog.yadutaf.fr/2014/09/27/using-docker-to-triage-nasty-bugs/</guid>
      <description>&lt;p&gt;Docker is the container system for reproducible builds. This is precisely what you want when dealing with bugs, especially the nastiest one: an environment where to reproduce it in a fully deterministic way.&lt;/p&gt;

&lt;p&gt;Not long ago, I had to troubleshoot the install process of a new cool piece of software. The weird and really uncool thing with this bug is that it only occurred on the first install install attempt. Even with a full (well, in theory) wipe, there still remained some kind of side effect on the system causing the subsequents install attempts to succeed. Anyone who has ever dealt with Q/A will know what I mean when I say this is pretty damn frustrating. (1)&lt;/p&gt;

&lt;p&gt;Traditional approach: use a smart combination of script and snapshots.&lt;/p&gt;

&lt;p&gt;Wait, isn&amp;rsquo;t it exactly what Docker those ? Sure it is !&lt;/p&gt;

&lt;p&gt;Even better than that: Docker saves one snapshot for each step. This is awesome&lt;/p&gt;

&lt;p&gt;when iterating.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s build a &lt;code&gt;Dockerfile&lt;/code&gt; for a Python project (Whoops, did I just name the perpetrator?):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;dockerfile&#34;&gt;# start from clean, minimalist system
FROM debian:stable

# step 1: make it less minimalist
RUN apt-get update &amp;&amp; apt-get install -y git vim python-pip

# step 2: grab code from GIT repo + switch to dev branch
RUN mkdir -p /usr/src &amp;&amp; git clone http://some-server/my-project /usr/src/my-project --branch fix-nastybugtm

# step 3: change workdir so it spares me one &#39;cd&#39; one each attempt
WORKDIR /usr/src/my-project
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As recommended by &lt;a href=&#34;https://docs.docker.com/articles/dockerfile_best-practices/&#34;&gt;Docker&amp;rsquo;s best practices&lt;/a&gt;, each logical step is grouped on its own dedicated line so that we keep the number of intermediate snapshots reasonable.&lt;/p&gt;

&lt;p&gt;Speaking of snapshots, let&amp;rsquo;s build our lab environment:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;docker build -t my-project-lab .
&lt;/pre&gt;

&lt;p&gt;And work on it!&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;docker run -t -i –rm my-project-lab /bin/bash
&lt;/pre&gt;

&lt;p&gt;This is where all the magic happens. We tell Docker to fire our &lt;code&gt;my-project-lab&lt;/code&gt; env from a clean copy in interactive mode (&lt;code&gt;-i&lt;/code&gt;) and do not attempt to retain data for later use, we won&amp;rsquo;t need it (&lt;code&gt;--rm&lt;/code&gt;). As we&amp;rsquo;re interactive, we&amp;rsquo;ll need a shell. I use &lt;code&gt;/bin/bash&lt;/code&gt; but given recent security context, I may want to be a better hipster and user &lt;code&gt;/bin/zsh&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;See how easy it is to industrialize bug fighting with Docker!&lt;/p&gt;

&lt;p&gt;Any time you&amp;rsquo;ve come closer to you bug, feel free to update your &lt;code&gt;Dockerfile&lt;/code&gt; and rebuild the image. That&amp;rsquo;s one less step to do manually.&lt;/p&gt;

&lt;p&gt;(1) actually, it was even more fun: the bug only occurred when installing from&lt;/p&gt;

&lt;p&gt;release website. Installing from GIT was always successful.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>